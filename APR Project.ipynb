{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"/content/Data_Assignment4.zip\"\n",
        "\n",
        "extract_path = \"/content/assignment4_data\"\n",
        "\n",
        "for root, dirs, files in os.walk(extract_path):\n",
        "    level = root.replace(extract_path, '').count(os.sep)\n",
        "    indent = ' ' * 4 * level\n",
        "    print(f\"{indent}{os.path.basename(root)}/\")\n",
        "    subindent = ' ' * 4 * (level + 1)\n",
        "    for f in files[:10]:  # show first 10 files per folder\n",
        "        print(f\"{subindent}{f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pL0slwI06vjg",
        "outputId": "ef679e00-a09e-491d-da60-c48fce5c5fdd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assignment4_data/\n",
            "    Data_Assignment4/\n",
            "        .DS_Store\n",
            "        Data_Assignment4/\n",
            "            .DS_Store\n",
            "            Hand_writing_Data/\n",
            "                a/\n",
            "                    a/\n",
            "                        train/\n",
            "                            67.txt\n",
            "                            11.txt\n",
            "                            25.txt\n",
            "                            10.txt\n",
            "                            31.txt\n",
            "                            8.txt\n",
            "                            2.txt\n",
            "                            9.txt\n",
            "                            15.txt\n",
            "                            61.txt\n",
            "                        dev/\n",
            "                            86.txt\n",
            "                            85.txt\n",
            "                            82.txt\n",
            "                            75.txt\n",
            "                            70.txt\n",
            "                            77.txt\n",
            "                            73.txt\n",
            "                            81.txt\n",
            "                            84.txt\n",
            "                            89.txt\n",
            "                chA/\n",
            "                    chA/\n",
            "                        train/\n",
            "                            67.txt\n",
            "                            11.txt\n",
            "                            25.txt\n",
            "                            10.txt\n",
            "                            31.txt\n",
            "                            8.txt\n",
            "                            2.txt\n",
            "                            9.txt\n",
            "                            15.txt\n",
            "                            61.txt\n",
            "                        dev/\n",
            "                            86.txt\n",
            "                            85.txt\n",
            "                            82.txt\n",
            "                            75.txt\n",
            "                            70.txt\n",
            "                            77.txt\n",
            "                            73.txt\n",
            "                            81.txt\n",
            "                            84.txt\n",
            "                            89.txt\n",
            "                dA/\n",
            "                    dA/\n",
            "                        train/\n",
            "                            67.txt\n",
            "                            11.txt\n",
            "                            25.txt\n",
            "                            10.txt\n",
            "                            31.txt\n",
            "                            8.txt\n",
            "                            2.txt\n",
            "                            9.txt\n",
            "                            15.txt\n",
            "                            61.txt\n",
            "                        dev/\n",
            "                            86.txt\n",
            "                            85.txt\n",
            "                            82.txt\n",
            "                            75.txt\n",
            "                            70.txt\n",
            "                            77.txt\n",
            "                            73.txt\n",
            "                            81.txt\n",
            "                            84.txt\n",
            "                            72.txt\n",
            "                ai/\n",
            "                    ai/\n",
            "                        train/\n",
            "                            67.txt\n",
            "                            11.txt\n",
            "                            25.txt\n",
            "                            10.txt\n",
            "                            31.txt\n",
            "                            8.txt\n",
            "                            2.txt\n",
            "                            9.txt\n",
            "                            15.txt\n",
            "                            61.txt\n",
            "                        dev/\n",
            "                            86.txt\n",
            "                            85.txt\n",
            "                            82.txt\n",
            "                            75.txt\n",
            "                            70.txt\n",
            "                            77.txt\n",
            "                            73.txt\n",
            "                            81.txt\n",
            "                            84.txt\n",
            "                            89.txt\n",
            "                bA/\n",
            "                    bA/\n",
            "                        train/\n",
            "                            11.txt\n",
            "                            25.txt\n",
            "                            10.txt\n",
            "                            31.txt\n",
            "                            8.txt\n",
            "                            2.txt\n",
            "                            9.txt\n",
            "                            15.txt\n",
            "                            61.txt\n",
            "                            65.txt\n",
            "                        dev/\n",
            "                            67.txt\n",
            "                            86.txt\n",
            "                            85.txt\n",
            "                            82.txt\n",
            "                            75.txt\n",
            "                            70.txt\n",
            "                            77.txt\n",
            "                            68.txt\n",
            "                            73.txt\n",
            "                            81.txt\n",
            "                _Sample-Image-for-each-class/\n",
            "                    a.png\n",
            "                    chA.png\n",
            "                    ai.png\n",
            "                    dA.png\n",
            "                    bA.png\n",
            "            CV_DATA/\n",
            "                tA/\n",
            "                    tA/\n",
            "                        Test/\n",
            "                            05_tA_147_7.mfcc\n",
            "                            04_tA_6_7.mfcc\n",
            "                            08_tA_144_6.mfcc\n",
            "                            05_tA_125_3.mfcc\n",
            "                            16_tA_136_8.mfcc\n",
            "                            16_tA_24_6.mfcc\n",
            "                            19_tA_156_11.mfcc\n",
            "                            02_tA_54_12.mfcc\n",
            "                            05_tA_74_10.mfcc\n",
            "                            15_tA_126_8.mfcc\n",
            "                        Train/\n",
            "                            16_tA_129_6.mfcc\n",
            "                            08_tA_92_9.mfcc\n",
            "                            13_tA_134_4.mfcc\n",
            "                            19_tA_157_5.mfcc\n",
            "                            06_tA_54_6.mfcc\n",
            "                            09_tA_62_5.mfcc\n",
            "                            09_tA_17_4.mfcc\n",
            "                            10_tA_68_6.mfcc\n",
            "                            09_tA_8_10.mfcc\n",
            "                            06_tA_54_2.mfcc\n",
            "                ni/\n",
            "                    ni/\n",
            "                        Test/\n",
            "                            08_ni_3_5.mfcc\n",
            "                            13_ni_56_3.mfcc\n",
            "                            15_ni_242_6.mfcc\n",
            "                            07_ni_52_2.mfcc\n",
            "                            11_ni_168_16.mfcc\n",
            "                            16_ni_246_11.mfcc\n",
            "                            16_ni_47_9.mfcc\n",
            "                            16_ni_341_10.mfcc\n",
            "                            01_ni_22_9.mfcc\n",
            "                            13_ni_19_5.mfcc\n",
            "                        Train/\n",
            "                            16_ni_176_16.mfcc\n",
            "                            02_ni_44_11.mfcc\n",
            "                            05_ni_51_6.mfcc\n",
            "                            16_ni_182_8.mfcc\n",
            "                            07_ni_166_6.mfcc\n",
            "                            18_ni_139_16.mfcc\n",
            "                            09_ni_147_3.mfcc\n",
            "                            18_ni_146_2.mfcc\n",
            "                            05_ni_4_2.mfcc\n",
            "                            19_ni_102_3.mfcc\n",
            "                ka/\n",
            "                    ka/\n",
            "                        Test/\n",
            "                            10_ka_81_11.mfcc\n",
            "                            05_ka_19_8.mfcc\n",
            "                            18_ka_21_17.mfcc\n",
            "                            17_ka_105_7.mfcc\n",
            "                            09_ka_157_11.mfcc\n",
            "                            10_ka_80_12.mfcc\n",
            "                            18_ka_131_14.mfcc\n",
            "                            05_ka_190_2.mfcc\n",
            "                            07_ka_50_5.mfcc\n",
            "                            19_ka_116_7.mfcc\n",
            "                        Train/\n",
            "                            18_ka_98_15.mfcc\n",
            "                            01_ka_54_5.mfcc\n",
            "                            08_ka_116_6.mfcc\n",
            "                            03_ka_132_10.mfcc\n",
            "                            14_ka_270_5.mfcc\n",
            "                            10_ka_40_17.mfcc\n",
            "                            02_ka_188_10.mfcc\n",
            "                            01_ka_103_5.mfcc\n",
            "                            10_ka_132_7.mfcc\n",
            "                            14_ka_138_9.mfcc\n",
            "                ba/\n",
            "                    ba/\n",
            "                        Test/\n",
            "                            02_ba_234_13.mfcc\n",
            "                            15_ba_58_5.mfcc\n",
            "                            16_ba_262_16.mfcc\n",
            "                            02_ba_280_2.mfcc\n",
            "                            10_ba_70_3.mfcc\n",
            "                            02_ba_191_7.mfcc\n",
            "                            02_ba_9_15.mfcc\n",
            "                            14_ba_189_8.mfcc\n",
            "                            19_ba_221_5.mfcc\n",
            "                            18_ba_173_5.mfcc\n",
            "                        Train/\n",
            "                            16_ba_305_4.mfcc\n",
            "                            14_ba_37_2.mfcc\n",
            "                            19_ba_35_13.mfcc\n",
            "                            12_ba_117_10.mfcc\n",
            "                            08_ba_55_14.mfcc\n",
            "                            04_ba_64_12.mfcc\n",
            "                            12_ba_130_4.mfcc\n",
            "                            09_ba_98_16.mfcc\n",
            "                            16_ba_326_4.mfcc\n",
            "                            18_ba_138_11.mfcc\n",
            "                bhA/\n",
            "                    bhA/\n",
            "                        Test/\n",
            "                            10_bhA_7_2.mfcc\n",
            "                            18_bhA_1_2.mfcc\n",
            "                            14_bhA_84_3.mfcc\n",
            "                            15_bhA_10_6.mfcc\n",
            "                            18_bhA_84_14.mfcc\n",
            "                            09_bhA_16_3.mfcc\n",
            "                            18_bhA_61_3.mfcc\n",
            "                            02_bhA_144_2.mfcc\n",
            "                            17_bhA_44_2.mfcc\n",
            "                            06_bhA_45_3.mfcc\n",
            "                        Train/\n",
            "                            13_bhA_115_15.mfcc\n",
            "                            16_bhA_12_8.mfcc\n",
            "                            14_bhA_177_2.mfcc\n",
            "                            08_bhA_165_8.mfcc\n",
            "                            14_bhA_11_7.mfcc\n",
            "                            09_bhA_33_3.mfcc\n",
            "                            07_bhA_30_2.mfcc\n",
            "                            19_bhA_44_2.mfcc\n",
            "                            19_bhA_70_2.mfcc\n",
            "                            16_bhA_106_18.mfcc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lWo1BiEdxsZ",
        "outputId": "b645226e-02f3-44c7-fbd0-e8358a0f688b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n",
            "Extracted /content/Data_Assignment4.zip -> /content/assignment4_data\n",
            "Detected handwritten_root: /content/assignment4_data/Data_Assignment4/Data_Assignment4/Hand_writing_Data\n",
            "Detected mfcc_root: /content/assignment4_data/Data_Assignment4/Data_Assignment4/CV_DATA\n",
            "\n",
            "Processing Handwritten dataset...\n",
            "Loaded 69 train and 20 test samples for class 'a' (0 bad files).\n",
            "Loaded 70 train and 20 test samples for class 'ai' (0 bad files).\n",
            "Loaded 67 train and 20 test samples for class 'bA' (0 bad files).\n",
            "Loaded 70 train and 20 test samples for class 'chA' (0 bad files).\n",
            "Loaded 69 train and 20 test samples for class 'dA' (0 bad files).\n",
            "Total handwriting samples: train=345 test=100 classes=5\n",
            "\n",
            "=== Running on dataset: handwritten ===\n",
            "\n",
            "--- handwritten | RNN | layers=1 | hidden=64 ---\n",
            "Epoch 000: train_loss=1.588515 train_acc=0.2605 val_loss=1.572271 val_acc=0.3235\n",
            "Epoch 001: train_loss=1.476933 train_acc=0.3987 val_loss=1.301555 val_acc=0.4412\n",
            "Epoch 002: train_loss=1.349703 train_acc=0.3601 val_loss=1.141958 val_acc=0.4118\n",
            "Epoch 003: train_loss=1.224288 train_acc=0.3955 val_loss=1.030968 val_acc=0.4118\n",
            "Epoch 004: train_loss=1.154102 train_acc=0.3633 val_loss=0.906968 val_acc=0.4118\n",
            "Epoch 005: train_loss=1.052422 train_acc=0.4244 val_loss=0.896868 val_acc=0.4706\n",
            "Epoch 006: train_loss=1.010646 train_acc=0.3955 val_loss=0.939984 val_acc=0.4706\n",
            "Epoch 007: train_loss=0.999457 train_acc=0.4502 val_loss=0.923953 val_acc=0.4706\n",
            "Epoch 008: train_loss=0.951579 train_acc=0.4920 val_loss=0.822781 val_acc=0.5882\n",
            "Epoch 009: train_loss=0.930592 train_acc=0.5048 val_loss=0.783715 val_acc=0.5588\n",
            "Epoch 010: train_loss=0.916027 train_acc=0.5177 val_loss=0.765807 val_acc=0.4706\n",
            "Epoch 011: train_loss=0.879925 train_acc=0.5338 val_loss=0.778821 val_acc=0.4706\n",
            "Epoch 012: train_loss=0.878379 train_acc=0.5145 val_loss=0.746874 val_acc=0.5294\n",
            "Epoch 013: train_loss=0.868227 train_acc=0.5080 val_loss=0.726209 val_acc=0.5294\n",
            "Epoch 014: train_loss=0.833155 train_acc=0.5305 val_loss=0.699015 val_acc=0.5294\n",
            "Epoch 015: train_loss=0.843614 train_acc=0.5113 val_loss=0.814398 val_acc=0.5588\n",
            "Epoch 016: train_loss=0.823277 train_acc=0.5434 val_loss=0.670892 val_acc=0.5588\n",
            "Epoch 017: train_loss=0.816842 train_acc=0.5305 val_loss=0.722695 val_acc=0.5294\n",
            "Epoch 018: train_loss=0.808100 train_acc=0.5563 val_loss=0.674927 val_acc=0.5588\n",
            "Epoch 019: train_loss=0.784186 train_acc=0.5627 val_loss=0.648700 val_acc=0.6176\n",
            "Epoch 020: train_loss=0.787187 train_acc=0.5852 val_loss=0.655583 val_acc=0.5882\n",
            "Epoch 021: train_loss=0.773157 train_acc=0.5949 val_loss=0.621655 val_acc=0.5882\n",
            "Epoch 022: train_loss=0.779979 train_acc=0.5723 val_loss=0.628346 val_acc=0.5588\n",
            "Epoch 023: train_loss=0.770857 train_acc=0.6013 val_loss=0.615750 val_acc=0.6176\n",
            "Converged at epoch 24 (train_loss change 7.64e-06 < 0.0001)\n",
            "Train acc: 0.5981 | Test acc: 0.6200\n",
            "\n",
            "--- handwritten | RNN | layers=1 | hidden=128 ---\n",
            "Epoch 000: train_loss=1.574052 train_acc=0.2637 val_loss=1.507479 val_acc=0.3235\n",
            "Epoch 001: train_loss=1.341222 train_acc=0.3151 val_loss=1.158436 val_acc=0.3824\n",
            "Epoch 002: train_loss=1.119524 train_acc=0.4502 val_loss=0.846475 val_acc=0.4706\n",
            "Epoch 003: train_loss=1.313402 train_acc=0.3891 val_loss=1.151639 val_acc=0.5294\n",
            "Epoch 004: train_loss=1.137394 train_acc=0.4984 val_loss=1.072524 val_acc=0.4412\n",
            "Epoch 005: train_loss=1.035826 train_acc=0.4469 val_loss=0.908331 val_acc=0.5000\n",
            "Epoch 006: train_loss=0.963712 train_acc=0.4952 val_loss=0.858851 val_acc=0.5882\n",
            "Epoch 007: train_loss=0.935174 train_acc=0.5080 val_loss=0.800502 val_acc=0.5588\n",
            "Epoch 008: train_loss=0.896839 train_acc=0.4984 val_loss=0.786274 val_acc=0.5882\n",
            "Epoch 009: train_loss=0.872494 train_acc=0.4984 val_loss=0.763143 val_acc=0.5588\n",
            "Epoch 010: train_loss=0.892221 train_acc=0.5177 val_loss=0.813789 val_acc=0.5000\n",
            "Epoch 011: train_loss=0.874788 train_acc=0.5434 val_loss=0.708791 val_acc=0.5588\n",
            "Epoch 012: train_loss=0.830280 train_acc=0.5595 val_loss=0.696170 val_acc=0.5588\n",
            "Epoch 013: train_loss=0.829091 train_acc=0.6013 val_loss=0.701254 val_acc=0.5294\n",
            "Epoch 014: train_loss=0.779907 train_acc=0.5595 val_loss=0.697081 val_acc=0.6471\n",
            "Epoch 015: train_loss=0.766976 train_acc=0.6367 val_loss=0.639467 val_acc=0.5294\n",
            "Epoch 016: train_loss=0.749967 train_acc=0.6141 val_loss=0.648099 val_acc=0.6176\n",
            "Epoch 017: train_loss=0.776504 train_acc=0.6013 val_loss=0.639503 val_acc=0.5882\n",
            "Epoch 018: train_loss=0.774768 train_acc=0.6109 val_loss=0.592134 val_acc=0.6765\n",
            "Epoch 019: train_loss=0.769333 train_acc=0.6077 val_loss=0.594386 val_acc=0.5882\n",
            "Epoch 020: train_loss=0.740010 train_acc=0.6206 val_loss=0.710482 val_acc=0.7059\n",
            "Epoch 021: train_loss=0.732677 train_acc=0.6302 val_loss=0.604339 val_acc=0.7059\n",
            "Epoch 022: train_loss=0.789487 train_acc=0.5659 val_loss=0.640875 val_acc=0.5588\n",
            "Epoch 023: train_loss=0.785291 train_acc=0.6141 val_loss=0.600235 val_acc=0.5882\n",
            "Epoch 024: train_loss=0.803028 train_acc=0.5659 val_loss=0.889353 val_acc=0.5588\n",
            "Epoch 025: train_loss=0.822190 train_acc=0.5659 val_loss=0.780347 val_acc=0.5882\n",
            "Epoch 026: train_loss=0.769702 train_acc=0.6141 val_loss=0.606674 val_acc=0.6471\n",
            "Epoch 027: train_loss=0.745731 train_acc=0.6174 val_loss=0.616376 val_acc=0.7059\n",
            "Epoch 028: train_loss=0.737284 train_acc=0.6238 val_loss=0.599144 val_acc=0.5882\n",
            "Epoch 029: train_loss=0.745793 train_acc=0.6077 val_loss=0.577208 val_acc=0.6471\n",
            "Epoch 030: train_loss=0.729235 train_acc=0.6174 val_loss=0.612466 val_acc=0.6471\n",
            "Epoch 031: train_loss=0.742364 train_acc=0.6045 val_loss=0.591664 val_acc=0.7059\n",
            "Epoch 032: train_loss=0.713451 train_acc=0.6206 val_loss=0.585132 val_acc=0.5882\n",
            "Epoch 033: train_loss=0.730130 train_acc=0.6077 val_loss=0.625290 val_acc=0.6471\n",
            "Epoch 034: train_loss=0.737321 train_acc=0.6045 val_loss=0.666289 val_acc=0.7059\n",
            "Epoch 035: train_loss=0.737590 train_acc=0.6109 val_loss=0.568066 val_acc=0.6471\n",
            "Epoch 036: train_loss=0.752928 train_acc=0.5884 val_loss=0.574677 val_acc=0.6176\n",
            "Epoch 037: train_loss=0.720844 train_acc=0.6270 val_loss=0.640049 val_acc=0.7353\n",
            "Epoch 038: train_loss=0.698284 train_acc=0.6141 val_loss=0.576798 val_acc=0.6471\n",
            "Epoch 039: train_loss=0.699586 train_acc=0.6302 val_loss=0.607136 val_acc=0.6176\n",
            "Epoch 040: train_loss=0.743512 train_acc=0.6045 val_loss=0.617693 val_acc=0.5882\n",
            "Epoch 041: train_loss=0.757696 train_acc=0.6109 val_loss=0.858475 val_acc=0.4706\n",
            "Epoch 042: train_loss=0.825082 train_acc=0.5241 val_loss=0.626846 val_acc=0.5588\n",
            "Epoch 043: train_loss=0.739272 train_acc=0.5884 val_loss=0.565710 val_acc=0.6471\n",
            "Epoch 044: train_loss=0.766337 train_acc=0.5949 val_loss=0.632943 val_acc=0.6176\n",
            "Epoch 045: train_loss=0.728891 train_acc=0.6206 val_loss=0.634575 val_acc=0.6176\n",
            "Epoch 046: train_loss=0.720285 train_acc=0.6238 val_loss=0.594041 val_acc=0.6471\n",
            "Epoch 047: train_loss=0.709189 train_acc=0.6302 val_loss=0.581388 val_acc=0.6471\n",
            "Epoch 048: train_loss=0.700184 train_acc=0.6463 val_loss=0.636478 val_acc=0.7059\n",
            "Epoch 049: train_loss=0.692491 train_acc=0.6302 val_loss=0.630596 val_acc=0.7059\n",
            "Epoch 050: train_loss=0.688308 train_acc=0.6367 val_loss=0.565188 val_acc=0.6765\n",
            "Epoch 051: train_loss=0.681131 train_acc=0.6399 val_loss=0.621554 val_acc=0.7059\n",
            "Epoch 052: train_loss=0.701172 train_acc=0.6077 val_loss=0.597130 val_acc=0.7059\n",
            "Epoch 053: train_loss=0.697494 train_acc=0.6559 val_loss=1.545918 val_acc=0.6471\n",
            "Epoch 054: train_loss=0.696298 train_acc=0.6399 val_loss=0.654720 val_acc=0.7059\n",
            "Epoch 055: train_loss=0.716430 train_acc=0.6174 val_loss=0.608336 val_acc=0.7059\n",
            "Epoch 056: train_loss=0.867520 train_acc=0.5595 val_loss=0.558735 val_acc=0.6176\n",
            "Epoch 057: train_loss=0.746916 train_acc=0.5723 val_loss=0.711194 val_acc=0.6471\n",
            "Epoch 058: train_loss=0.791141 train_acc=0.5563 val_loss=0.911651 val_acc=0.5882\n",
            "Epoch 059: train_loss=0.874759 train_acc=0.5466 val_loss=0.856854 val_acc=0.5588\n",
            "Epoch 060: train_loss=0.750476 train_acc=0.5949 val_loss=0.608863 val_acc=0.6471\n",
            "Epoch 061: train_loss=0.713465 train_acc=0.6270 val_loss=0.610431 val_acc=0.5882\n",
            "Epoch 062: train_loss=0.712130 train_acc=0.6399 val_loss=0.620562 val_acc=0.5882\n",
            "Epoch 063: train_loss=0.694529 train_acc=0.6495 val_loss=0.616692 val_acc=0.6176\n",
            "Epoch 064: train_loss=0.685387 train_acc=0.6206 val_loss=0.570171 val_acc=0.6176\n",
            "Epoch 065: train_loss=0.681367 train_acc=0.6399 val_loss=0.601923 val_acc=0.6765\n",
            "Epoch 066: train_loss=0.675580 train_acc=0.6656 val_loss=0.615850 val_acc=0.7353\n",
            "Epoch 067: train_loss=0.697161 train_acc=0.6270 val_loss=0.594080 val_acc=0.6471\n",
            "Epoch 068: train_loss=0.688575 train_acc=0.6174 val_loss=0.609531 val_acc=0.5882\n",
            "Epoch 069: train_loss=0.678323 train_acc=0.6688 val_loss=0.585275 val_acc=0.6471\n",
            "Epoch 070: train_loss=0.682000 train_acc=0.6431 val_loss=0.637527 val_acc=0.6765\n",
            "Epoch 071: train_loss=0.690560 train_acc=0.6270 val_loss=0.628477 val_acc=0.6471\n",
            "Epoch 072: train_loss=0.671272 train_acc=0.6624 val_loss=0.570695 val_acc=0.6471\n",
            "Epoch 073: train_loss=0.672055 train_acc=0.6817 val_loss=0.536542 val_acc=0.6471\n",
            "Epoch 074: train_loss=0.712802 train_acc=0.6399 val_loss=0.614160 val_acc=0.7353\n",
            "Epoch 075: train_loss=0.709819 train_acc=0.6013 val_loss=0.605420 val_acc=0.6471\n",
            "Epoch 076: train_loss=0.721098 train_acc=0.6174 val_loss=0.563408 val_acc=0.6471\n",
            "Epoch 077: train_loss=0.694287 train_acc=0.6688 val_loss=0.696906 val_acc=0.6765\n",
            "Epoch 078: train_loss=0.697520 train_acc=0.6688 val_loss=0.612526 val_acc=0.5882\n",
            "Epoch 079: train_loss=0.677694 train_acc=0.6752 val_loss=0.579107 val_acc=0.6176\n",
            "Epoch 080: train_loss=0.693220 train_acc=0.6270 val_loss=0.580080 val_acc=0.6765\n",
            "Epoch 081: train_loss=0.685291 train_acc=0.6431 val_loss=0.632069 val_acc=0.6471\n",
            "Epoch 082: train_loss=0.684218 train_acc=0.6270 val_loss=0.609986 val_acc=0.7059\n",
            "Epoch 083: train_loss=0.765290 train_acc=0.5820 val_loss=0.721465 val_acc=0.5000\n",
            "Epoch 084: train_loss=0.800535 train_acc=0.5466 val_loss=0.650721 val_acc=0.5294\n",
            "Epoch 085: train_loss=0.752766 train_acc=0.5820 val_loss=0.608926 val_acc=0.5882\n",
            "Epoch 086: train_loss=0.727208 train_acc=0.6077 val_loss=0.600654 val_acc=0.5882\n",
            "Epoch 087: train_loss=0.695305 train_acc=0.6431 val_loss=0.604628 val_acc=0.6471\n",
            "Epoch 088: train_loss=0.696554 train_acc=0.6431 val_loss=0.663312 val_acc=0.6471\n",
            "Epoch 089: train_loss=0.718641 train_acc=0.6238 val_loss=0.705242 val_acc=0.5882\n",
            "Epoch 090: train_loss=0.695896 train_acc=0.6463 val_loss=0.582334 val_acc=0.5882\n",
            "Epoch 091: train_loss=0.677956 train_acc=0.6559 val_loss=0.691996 val_acc=0.6765\n",
            "Epoch 092: train_loss=0.705234 train_acc=0.6399 val_loss=0.586202 val_acc=0.6176\n",
            "Epoch 093: train_loss=0.681778 train_acc=0.6367 val_loss=0.558226 val_acc=0.6176\n",
            "Epoch 094: train_loss=0.674274 train_acc=0.6495 val_loss=0.705286 val_acc=0.6471\n",
            "Epoch 095: train_loss=0.816515 train_acc=0.5981 val_loss=0.991095 val_acc=0.4412\n",
            "Epoch 096: train_loss=0.862945 train_acc=0.5434 val_loss=0.679025 val_acc=0.5294\n",
            "Epoch 097: train_loss=0.783395 train_acc=0.5788 val_loss=0.606600 val_acc=0.6471\n",
            "Epoch 098: train_loss=0.727530 train_acc=0.6206 val_loss=0.612590 val_acc=0.6471\n",
            "Epoch 099: train_loss=0.705463 train_acc=0.6141 val_loss=0.591021 val_acc=0.6471\n",
            "Epoch 100: train_loss=0.688798 train_acc=0.6431 val_loss=0.581187 val_acc=0.6765\n",
            "Epoch 101: train_loss=0.676838 train_acc=0.6656 val_loss=0.600561 val_acc=0.6176\n",
            "Epoch 102: train_loss=0.684980 train_acc=0.6559 val_loss=0.685998 val_acc=0.6765\n",
            "Epoch 103: train_loss=0.670829 train_acc=0.6592 val_loss=0.563742 val_acc=0.6176\n",
            "Epoch 104: train_loss=0.669951 train_acc=0.6559 val_loss=0.730377 val_acc=0.6471\n",
            "Epoch 105: train_loss=0.684958 train_acc=0.6463 val_loss=0.601234 val_acc=0.6176\n",
            "Epoch 106: train_loss=0.669435 train_acc=0.6559 val_loss=0.637546 val_acc=0.6471\n",
            "Epoch 107: train_loss=0.666068 train_acc=0.6431 val_loss=0.610908 val_acc=0.7059\n",
            "Epoch 108: train_loss=0.658573 train_acc=0.6752 val_loss=0.557070 val_acc=0.6471\n",
            "Epoch 109: train_loss=0.663513 train_acc=0.6527 val_loss=0.638061 val_acc=0.6471\n",
            "Epoch 110: train_loss=0.656996 train_acc=0.6881 val_loss=0.560380 val_acc=0.6176\n",
            "Epoch 111: train_loss=0.657468 train_acc=0.6752 val_loss=0.546826 val_acc=0.6471\n",
            "Epoch 112: train_loss=0.673134 train_acc=0.6495 val_loss=0.603268 val_acc=0.6176\n",
            "Epoch 113: train_loss=0.683603 train_acc=0.6367 val_loss=0.619017 val_acc=0.6471\n",
            "Epoch 114: train_loss=0.669424 train_acc=0.6624 val_loss=0.579148 val_acc=0.6176\n",
            "Epoch 115: train_loss=0.653610 train_acc=0.6592 val_loss=0.659415 val_acc=0.7059\n",
            "Epoch 116: train_loss=0.655415 train_acc=0.6656 val_loss=0.616570 val_acc=0.6471\n",
            "Epoch 117: train_loss=0.647891 train_acc=0.6592 val_loss=0.584431 val_acc=0.7059\n",
            "Epoch 118: train_loss=0.667692 train_acc=0.6592 val_loss=0.551690 val_acc=0.6176\n",
            "Epoch 119: train_loss=0.669516 train_acc=0.6592 val_loss=0.608576 val_acc=0.7059\n",
            "Epoch 120: train_loss=0.660740 train_acc=0.6849 val_loss=0.579422 val_acc=0.6765\n",
            "Epoch 121: train_loss=0.674124 train_acc=0.6656 val_loss=0.535371 val_acc=0.6765\n",
            "Epoch 122: train_loss=0.636740 train_acc=0.6752 val_loss=0.528188 val_acc=0.6471\n",
            "Epoch 123: train_loss=0.674018 train_acc=0.6592 val_loss=0.506032 val_acc=0.6471\n",
            "Epoch 124: train_loss=0.742427 train_acc=0.5788 val_loss=0.696130 val_acc=0.6176\n",
            "Epoch 125: train_loss=0.678053 train_acc=0.6463 val_loss=0.538004 val_acc=0.6471\n",
            "Epoch 126: train_loss=0.673874 train_acc=0.6399 val_loss=0.618875 val_acc=0.6471\n",
            "Epoch 127: train_loss=0.657426 train_acc=0.6592 val_loss=0.597242 val_acc=0.6765\n",
            "Epoch 128: train_loss=0.642919 train_acc=0.7042 val_loss=0.582484 val_acc=0.6765\n",
            "Epoch 129: train_loss=0.641801 train_acc=0.6785 val_loss=0.566252 val_acc=0.6176\n",
            "Epoch 130: train_loss=0.670465 train_acc=0.6559 val_loss=0.583594 val_acc=0.7353\n",
            "Epoch 131: train_loss=0.652355 train_acc=0.6849 val_loss=0.561617 val_acc=0.7059\n",
            "Epoch 132: train_loss=0.662336 train_acc=0.6656 val_loss=0.565586 val_acc=0.6176\n",
            "Epoch 133: train_loss=0.634536 train_acc=0.6849 val_loss=0.611036 val_acc=0.7353\n",
            "Epoch 134: train_loss=0.626609 train_acc=0.7170 val_loss=0.683663 val_acc=0.7059\n",
            "Epoch 135: train_loss=0.638761 train_acc=0.6688 val_loss=0.489159 val_acc=0.6471\n",
            "Epoch 136: train_loss=0.631835 train_acc=0.6945 val_loss=0.687722 val_acc=0.6765\n",
            "Epoch 137: train_loss=0.646978 train_acc=0.6785 val_loss=0.768185 val_acc=0.5882\n",
            "Epoch 138: train_loss=0.661622 train_acc=0.6302 val_loss=0.719812 val_acc=0.6765\n",
            "Epoch 139: train_loss=0.708718 train_acc=0.6077 val_loss=0.555106 val_acc=0.6765\n",
            "Epoch 140: train_loss=0.657449 train_acc=0.6817 val_loss=0.605941 val_acc=0.6176\n",
            "Epoch 141: train_loss=0.653623 train_acc=0.6720 val_loss=0.591894 val_acc=0.7941\n",
            "Epoch 142: train_loss=0.656864 train_acc=0.6559 val_loss=0.574067 val_acc=0.7647\n",
            "Epoch 143: train_loss=0.650802 train_acc=0.6656 val_loss=0.550689 val_acc=0.7059\n",
            "Epoch 144: train_loss=0.648173 train_acc=0.6977 val_loss=0.536330 val_acc=0.6471\n",
            "Epoch 145: train_loss=0.609676 train_acc=0.6977 val_loss=0.477336 val_acc=0.7059\n",
            "Epoch 146: train_loss=0.556545 train_acc=0.7299 val_loss=0.579226 val_acc=0.7059\n",
            "Epoch 147: train_loss=0.539094 train_acc=0.7588 val_loss=0.476875 val_acc=0.7059\n",
            "Epoch 148: train_loss=0.527616 train_acc=0.7492 val_loss=0.389653 val_acc=0.7941\n",
            "Epoch 149: train_loss=0.543448 train_acc=0.7428 val_loss=0.632389 val_acc=0.6765\n",
            "Epoch 150: train_loss=0.623543 train_acc=0.6977 val_loss=0.495802 val_acc=0.6471\n",
            "Epoch 151: train_loss=0.681275 train_acc=0.6785 val_loss=0.535665 val_acc=0.6176\n",
            "Epoch 152: train_loss=0.618508 train_acc=0.7074 val_loss=0.502548 val_acc=0.7353\n",
            "Epoch 153: train_loss=0.587641 train_acc=0.7299 val_loss=0.454277 val_acc=0.7353\n",
            "Epoch 154: train_loss=0.492416 train_acc=0.7653 val_loss=0.726629 val_acc=0.7059\n",
            "Epoch 155: train_loss=0.542083 train_acc=0.7524 val_loss=0.504568 val_acc=0.7059\n",
            "Epoch 156: train_loss=0.732475 train_acc=0.6495 val_loss=0.732694 val_acc=0.5294\n",
            "Epoch 157: train_loss=0.687705 train_acc=0.6849 val_loss=0.749803 val_acc=0.7059\n",
            "Epoch 158: train_loss=0.637184 train_acc=0.7074 val_loss=0.439923 val_acc=0.7059\n",
            "Epoch 159: train_loss=0.566813 train_acc=0.7267 val_loss=0.562803 val_acc=0.7353\n",
            "Epoch 160: train_loss=0.567841 train_acc=0.7363 val_loss=0.417835 val_acc=0.7647\n",
            "Epoch 161: train_loss=0.543236 train_acc=0.7106 val_loss=0.507623 val_acc=0.7059\n",
            "Epoch 162: train_loss=0.529570 train_acc=0.7588 val_loss=0.532286 val_acc=0.7059\n",
            "Epoch 163: train_loss=0.535355 train_acc=0.7588 val_loss=0.413866 val_acc=0.7647\n",
            "Epoch 164: train_loss=0.532713 train_acc=0.7556 val_loss=0.624380 val_acc=0.6176\n",
            "Epoch 165: train_loss=0.854994 train_acc=0.5756 val_loss=0.661804 val_acc=0.6176\n",
            "Epoch 166: train_loss=0.650658 train_acc=0.6849 val_loss=0.523241 val_acc=0.6471\n",
            "Epoch 167: train_loss=0.569423 train_acc=0.7428 val_loss=0.512628 val_acc=0.7353\n",
            "Epoch 168: train_loss=0.476589 train_acc=0.7814 val_loss=0.384590 val_acc=0.7647\n",
            "Epoch 169: train_loss=0.423467 train_acc=0.7781 val_loss=0.354354 val_acc=0.8235\n",
            "Epoch 170: train_loss=0.416644 train_acc=0.7846 val_loss=0.942196 val_acc=0.5000\n",
            "Epoch 171: train_loss=2.023399 train_acc=0.3569 val_loss=3.827700 val_acc=0.2647\n",
            "Epoch 172: train_loss=2.041375 train_acc=0.3923 val_loss=2.347395 val_acc=0.3235\n",
            "Epoch 173: train_loss=1.431597 train_acc=0.3987 val_loss=1.148174 val_acc=0.3824\n",
            "Epoch 174: train_loss=1.141845 train_acc=0.4469 val_loss=1.324481 val_acc=0.3529\n",
            "Epoch 175: train_loss=1.066264 train_acc=0.4373 val_loss=1.041387 val_acc=0.4412\n",
            "Epoch 176: train_loss=0.981388 train_acc=0.4952 val_loss=0.948834 val_acc=0.4412\n",
            "Epoch 177: train_loss=0.919900 train_acc=0.5273 val_loss=0.845169 val_acc=0.4706\n",
            "Epoch 178: train_loss=0.866391 train_acc=0.5273 val_loss=0.744660 val_acc=0.5000\n",
            "Epoch 179: train_loss=0.835202 train_acc=0.5305 val_loss=0.738358 val_acc=0.5882\n",
            "Epoch 180: train_loss=0.822920 train_acc=0.5563 val_loss=0.692423 val_acc=0.5588\n",
            "Epoch 181: train_loss=0.846065 train_acc=0.5563 val_loss=0.731652 val_acc=0.6471\n",
            "Epoch 182: train_loss=0.794459 train_acc=0.5723 val_loss=0.708331 val_acc=0.5588\n",
            "Epoch 183: train_loss=0.784308 train_acc=0.5981 val_loss=0.674947 val_acc=0.5882\n",
            "Epoch 184: train_loss=0.759752 train_acc=0.6013 val_loss=0.695486 val_acc=0.5588\n",
            "Epoch 185: train_loss=0.752415 train_acc=0.5531 val_loss=0.631635 val_acc=0.5882\n",
            "Epoch 186: train_loss=0.844065 train_acc=0.5788 val_loss=0.717191 val_acc=0.4412\n",
            "Epoch 187: train_loss=0.749987 train_acc=0.6174 val_loss=0.636160 val_acc=0.5882\n",
            "Epoch 188: train_loss=0.725402 train_acc=0.6109 val_loss=0.654725 val_acc=0.5882\n",
            "Epoch 189: train_loss=0.710078 train_acc=0.6013 val_loss=0.646402 val_acc=0.5588\n",
            "Epoch 190: train_loss=0.835460 train_acc=0.5852 val_loss=0.663784 val_acc=0.5294\n",
            "Epoch 191: train_loss=0.941765 train_acc=0.5209 val_loss=0.780609 val_acc=0.5000\n",
            "Epoch 192: train_loss=0.847792 train_acc=0.5434 val_loss=0.794702 val_acc=0.6176\n",
            "Epoch 193: train_loss=1.020709 train_acc=0.5048 val_loss=0.786878 val_acc=0.5294\n",
            "Epoch 194: train_loss=0.875481 train_acc=0.5177 val_loss=0.815287 val_acc=0.5000\n",
            "Epoch 195: train_loss=0.813284 train_acc=0.6013 val_loss=0.653520 val_acc=0.5882\n",
            "Epoch 196: train_loss=0.750605 train_acc=0.6527 val_loss=0.650718 val_acc=0.5588\n",
            "Epoch 197: train_loss=0.722339 train_acc=0.6559 val_loss=0.637067 val_acc=0.5588\n",
            "Epoch 198: train_loss=0.709833 train_acc=0.6367 val_loss=0.633428 val_acc=0.5294\n",
            "Epoch 199: train_loss=0.717653 train_acc=0.6013 val_loss=0.640013 val_acc=0.5882\n",
            "Train acc: 0.6559 | Test acc: 0.6500\n",
            "\n",
            "--- handwritten | RNN | layers=2 | hidden=64 ---\n",
            "Epoch 000: train_loss=1.585670 train_acc=0.2669 val_loss=1.562608 val_acc=0.3529\n",
            "Epoch 001: train_loss=1.432842 train_acc=0.3023 val_loss=1.200671 val_acc=0.3235\n",
            "Epoch 002: train_loss=1.145903 train_acc=0.4405 val_loss=0.999298 val_acc=0.4412\n",
            "Epoch 003: train_loss=1.028921 train_acc=0.4309 val_loss=0.910593 val_acc=0.4412\n",
            "Epoch 004: train_loss=0.972627 train_acc=0.4373 val_loss=0.927234 val_acc=0.5294\n",
            "Epoch 005: train_loss=0.940803 train_acc=0.4823 val_loss=0.859027 val_acc=0.5588\n",
            "Epoch 006: train_loss=0.911526 train_acc=0.4920 val_loss=0.851960 val_acc=0.6176\n",
            "Epoch 007: train_loss=0.885081 train_acc=0.5595 val_loss=0.819846 val_acc=0.5588\n",
            "Epoch 008: train_loss=0.877756 train_acc=0.5048 val_loss=0.739836 val_acc=0.5294\n",
            "Epoch 009: train_loss=0.810945 train_acc=0.5627 val_loss=0.765215 val_acc=0.6176\n",
            "Epoch 010: train_loss=0.778879 train_acc=0.5659 val_loss=0.675782 val_acc=0.5588\n",
            "Epoch 011: train_loss=0.806298 train_acc=0.5466 val_loss=0.701089 val_acc=0.5588\n",
            "Epoch 012: train_loss=0.767420 train_acc=0.5756 val_loss=0.706881 val_acc=0.5588\n",
            "Epoch 013: train_loss=0.772715 train_acc=0.6045 val_loss=0.681564 val_acc=0.5000\n",
            "Epoch 014: train_loss=0.760733 train_acc=0.6367 val_loss=0.651427 val_acc=0.5588\n",
            "Epoch 015: train_loss=0.735393 train_acc=0.6367 val_loss=0.653864 val_acc=0.6765\n",
            "Epoch 016: train_loss=0.721178 train_acc=0.6141 val_loss=0.673508 val_acc=0.5882\n",
            "Epoch 017: train_loss=0.752449 train_acc=0.6238 val_loss=0.710803 val_acc=0.5588\n",
            "Epoch 018: train_loss=0.822089 train_acc=0.5756 val_loss=0.751376 val_acc=0.5588\n",
            "Epoch 019: train_loss=0.755371 train_acc=0.5820 val_loss=0.653081 val_acc=0.4706\n",
            "Epoch 020: train_loss=0.772055 train_acc=0.5370 val_loss=0.657986 val_acc=0.5882\n",
            "Epoch 021: train_loss=0.738627 train_acc=0.6141 val_loss=0.691521 val_acc=0.5882\n",
            "Epoch 022: train_loss=0.728615 train_acc=0.6077 val_loss=0.675702 val_acc=0.5294\n",
            "Epoch 023: train_loss=0.710961 train_acc=0.6592 val_loss=0.658030 val_acc=0.5000\n",
            "Epoch 024: train_loss=0.704936 train_acc=0.6045 val_loss=0.658901 val_acc=0.5882\n",
            "Epoch 025: train_loss=0.694892 train_acc=0.6592 val_loss=0.664035 val_acc=0.5882\n",
            "Epoch 026: train_loss=0.683357 train_acc=0.6174 val_loss=0.630858 val_acc=0.6471\n",
            "Epoch 027: train_loss=0.697516 train_acc=0.6109 val_loss=0.665051 val_acc=0.5588\n",
            "Epoch 028: train_loss=0.699245 train_acc=0.6334 val_loss=0.631998 val_acc=0.5882\n",
            "Epoch 029: train_loss=0.697024 train_acc=0.6688 val_loss=0.649310 val_acc=0.5294\n",
            "Epoch 030: train_loss=0.684078 train_acc=0.6367 val_loss=0.626346 val_acc=0.4706\n",
            "Epoch 031: train_loss=0.737521 train_acc=0.6141 val_loss=0.733448 val_acc=0.6176\n",
            "Epoch 032: train_loss=0.737250 train_acc=0.6174 val_loss=0.663959 val_acc=0.5882\n",
            "Epoch 033: train_loss=0.720562 train_acc=0.5659 val_loss=0.709987 val_acc=0.5588\n",
            "Epoch 034: train_loss=0.734626 train_acc=0.6174 val_loss=0.740158 val_acc=0.4706\n",
            "Epoch 035: train_loss=0.771743 train_acc=0.5691 val_loss=0.637179 val_acc=0.5294\n",
            "Epoch 036: train_loss=0.741954 train_acc=0.5691 val_loss=0.722503 val_acc=0.5882\n",
            "Epoch 037: train_loss=0.722993 train_acc=0.5756 val_loss=0.694783 val_acc=0.4706\n",
            "Epoch 038: train_loss=0.713074 train_acc=0.6077 val_loss=0.642764 val_acc=0.5294\n",
            "Epoch 039: train_loss=0.694957 train_acc=0.6141 val_loss=0.664176 val_acc=0.5588\n",
            "Epoch 040: train_loss=0.689874 train_acc=0.6367 val_loss=0.632241 val_acc=0.5000\n",
            "Epoch 041: train_loss=0.700796 train_acc=0.6495 val_loss=0.658792 val_acc=0.5882\n",
            "Epoch 042: train_loss=0.687236 train_acc=0.6334 val_loss=0.639294 val_acc=0.5588\n",
            "Epoch 043: train_loss=0.693515 train_acc=0.6270 val_loss=0.678038 val_acc=0.5882\n",
            "Epoch 044: train_loss=0.692231 train_acc=0.6077 val_loss=0.623666 val_acc=0.5294\n",
            "Epoch 045: train_loss=0.678188 train_acc=0.6431 val_loss=0.668318 val_acc=0.5882\n",
            "Epoch 046: train_loss=0.691927 train_acc=0.6399 val_loss=0.693226 val_acc=0.6176\n",
            "Epoch 047: train_loss=0.672193 train_acc=0.6206 val_loss=0.651138 val_acc=0.6176\n",
            "Epoch 048: train_loss=0.685884 train_acc=0.6077 val_loss=0.653225 val_acc=0.6471\n",
            "Epoch 049: train_loss=0.668934 train_acc=0.6399 val_loss=0.700180 val_acc=0.6176\n",
            "Epoch 050: train_loss=0.701757 train_acc=0.6206 val_loss=0.680180 val_acc=0.5294\n",
            "Epoch 051: train_loss=0.757201 train_acc=0.5981 val_loss=0.642223 val_acc=0.5882\n",
            "Epoch 052: train_loss=0.739559 train_acc=0.5949 val_loss=0.647647 val_acc=0.5294\n",
            "Epoch 053: train_loss=0.721742 train_acc=0.6045 val_loss=0.662405 val_acc=0.6176\n",
            "Epoch 054: train_loss=0.716067 train_acc=0.5723 val_loss=0.699062 val_acc=0.5000\n",
            "Epoch 055: train_loss=0.699673 train_acc=0.6174 val_loss=0.703466 val_acc=0.5294\n",
            "Epoch 056: train_loss=0.694350 train_acc=0.6174 val_loss=0.611389 val_acc=0.5882\n",
            "Epoch 057: train_loss=0.682074 train_acc=0.6463 val_loss=0.696983 val_acc=0.5882\n",
            "Epoch 058: train_loss=0.681094 train_acc=0.6077 val_loss=0.632090 val_acc=0.6176\n",
            "Epoch 059: train_loss=0.668106 train_acc=0.6495 val_loss=0.694592 val_acc=0.5294\n",
            "Epoch 060: train_loss=0.660009 train_acc=0.6141 val_loss=0.627236 val_acc=0.5588\n",
            "Epoch 061: train_loss=0.710622 train_acc=0.6399 val_loss=0.674462 val_acc=0.6176\n",
            "Epoch 062: train_loss=0.782824 train_acc=0.5595 val_loss=0.769964 val_acc=0.5000\n",
            "Epoch 063: train_loss=0.758144 train_acc=0.5884 val_loss=0.656825 val_acc=0.5294\n",
            "Epoch 064: train_loss=0.721036 train_acc=0.6431 val_loss=0.697803 val_acc=0.5294\n",
            "Epoch 065: train_loss=0.706886 train_acc=0.5723 val_loss=0.637298 val_acc=0.6176\n",
            "Epoch 066: train_loss=0.683398 train_acc=0.6238 val_loss=0.688557 val_acc=0.6176\n",
            "Epoch 067: train_loss=0.682339 train_acc=0.6174 val_loss=0.618960 val_acc=0.5588\n",
            "Epoch 068: train_loss=0.688063 train_acc=0.6174 val_loss=0.730258 val_acc=0.6176\n",
            "Epoch 069: train_loss=0.669674 train_acc=0.6270 val_loss=0.659070 val_acc=0.5882\n",
            "Epoch 070: train_loss=0.668252 train_acc=0.6141 val_loss=0.656743 val_acc=0.6471\n",
            "Epoch 071: train_loss=0.666373 train_acc=0.6399 val_loss=0.625634 val_acc=0.5882\n",
            "Epoch 072: train_loss=0.655692 train_acc=0.6559 val_loss=0.669971 val_acc=0.5588\n",
            "Epoch 073: train_loss=0.638352 train_acc=0.6527 val_loss=0.657629 val_acc=0.5294\n",
            "Epoch 074: train_loss=0.647404 train_acc=0.6495 val_loss=0.601398 val_acc=0.5882\n",
            "Epoch 075: train_loss=0.652370 train_acc=0.6785 val_loss=0.724747 val_acc=0.5882\n",
            "Epoch 076: train_loss=0.650701 train_acc=0.6527 val_loss=0.622512 val_acc=0.5882\n",
            "Epoch 077: train_loss=0.641553 train_acc=0.6624 val_loss=0.746111 val_acc=0.6765\n",
            "Epoch 078: train_loss=0.635366 train_acc=0.6656 val_loss=0.582166 val_acc=0.6176\n",
            "Epoch 079: train_loss=0.643288 train_acc=0.6624 val_loss=0.777301 val_acc=0.6471\n",
            "Epoch 080: train_loss=0.658503 train_acc=0.6431 val_loss=0.580214 val_acc=0.5588\n",
            "Epoch 081: train_loss=0.642379 train_acc=0.6495 val_loss=0.667663 val_acc=0.6765\n",
            "Epoch 082: train_loss=0.632926 train_acc=0.6656 val_loss=0.650648 val_acc=0.5882\n",
            "Epoch 083: train_loss=0.632380 train_acc=0.6592 val_loss=0.701280 val_acc=0.5882\n",
            "Epoch 084: train_loss=0.631955 train_acc=0.6817 val_loss=0.630962 val_acc=0.6176\n",
            "Epoch 085: train_loss=0.632159 train_acc=0.6688 val_loss=0.699981 val_acc=0.6176\n",
            "Epoch 086: train_loss=0.631599 train_acc=0.6624 val_loss=0.624518 val_acc=0.5882\n",
            "Epoch 087: train_loss=0.638491 train_acc=0.6559 val_loss=0.604020 val_acc=0.6471\n",
            "Epoch 088: train_loss=0.631471 train_acc=0.6849 val_loss=0.630769 val_acc=0.6471\n",
            "Epoch 089: train_loss=0.627235 train_acc=0.6688 val_loss=0.653495 val_acc=0.6471\n",
            "Epoch 090: train_loss=0.631191 train_acc=0.6785 val_loss=0.739958 val_acc=0.6471\n",
            "Epoch 091: train_loss=0.630661 train_acc=0.6817 val_loss=0.589543 val_acc=0.5882\n",
            "Epoch 092: train_loss=0.619307 train_acc=0.6624 val_loss=0.598732 val_acc=0.6176\n",
            "Epoch 093: train_loss=0.637035 train_acc=0.6752 val_loss=0.598332 val_acc=0.6176\n",
            "Epoch 094: train_loss=0.634256 train_acc=0.6559 val_loss=0.585143 val_acc=0.5882\n",
            "Epoch 095: train_loss=0.628283 train_acc=0.6592 val_loss=0.705468 val_acc=0.6471\n",
            "Epoch 096: train_loss=0.624277 train_acc=0.6849 val_loss=0.727963 val_acc=0.7059\n",
            "Epoch 097: train_loss=0.631335 train_acc=0.6656 val_loss=0.671302 val_acc=0.6471\n",
            "Epoch 098: train_loss=0.616927 train_acc=0.6849 val_loss=0.615831 val_acc=0.5882\n",
            "Epoch 099: train_loss=0.624201 train_acc=0.6720 val_loss=0.678675 val_acc=0.6765\n",
            "Epoch 100: train_loss=0.610329 train_acc=0.6849 val_loss=0.597194 val_acc=0.6176\n",
            "Epoch 101: train_loss=0.620738 train_acc=0.6881 val_loss=0.548725 val_acc=0.5882\n",
            "Epoch 102: train_loss=0.637998 train_acc=0.6624 val_loss=0.628135 val_acc=0.5882\n",
            "Epoch 103: train_loss=0.614260 train_acc=0.6913 val_loss=0.754037 val_acc=0.6765\n",
            "Epoch 104: train_loss=0.631633 train_acc=0.6592 val_loss=0.751502 val_acc=0.6765\n",
            "Epoch 105: train_loss=0.645282 train_acc=0.6656 val_loss=0.545354 val_acc=0.6176\n",
            "Epoch 106: train_loss=0.673312 train_acc=0.6270 val_loss=0.576751 val_acc=0.5588\n",
            "Epoch 107: train_loss=0.714979 train_acc=0.6624 val_loss=0.634586 val_acc=0.6471\n",
            "Epoch 108: train_loss=0.756549 train_acc=0.6592 val_loss=0.713166 val_acc=0.6471\n",
            "Epoch 109: train_loss=0.729088 train_acc=0.6238 val_loss=0.622812 val_acc=0.5588\n",
            "Epoch 110: train_loss=0.702815 train_acc=0.6174 val_loss=0.822931 val_acc=0.5588\n",
            "Epoch 111: train_loss=0.689683 train_acc=0.5916 val_loss=0.576379 val_acc=0.6176\n",
            "Epoch 112: train_loss=0.662903 train_acc=0.6495 val_loss=0.756736 val_acc=0.6176\n",
            "Epoch 113: train_loss=0.647554 train_acc=0.6559 val_loss=0.648375 val_acc=0.6471\n",
            "Epoch 114: train_loss=0.659862 train_acc=0.6720 val_loss=0.659010 val_acc=0.5588\n",
            "Epoch 115: train_loss=0.642687 train_acc=0.6720 val_loss=0.630392 val_acc=0.6176\n",
            "Epoch 116: train_loss=0.716251 train_acc=0.6431 val_loss=0.584620 val_acc=0.5588\n",
            "Epoch 117: train_loss=0.759285 train_acc=0.5788 val_loss=0.626694 val_acc=0.5588\n",
            "Epoch 118: train_loss=0.747440 train_acc=0.5627 val_loss=0.683521 val_acc=0.5882\n",
            "Epoch 119: train_loss=0.711431 train_acc=0.5788 val_loss=0.601376 val_acc=0.6765\n",
            "Epoch 120: train_loss=0.685776 train_acc=0.6174 val_loss=0.641388 val_acc=0.6471\n",
            "Epoch 121: train_loss=0.670466 train_acc=0.6527 val_loss=0.776455 val_acc=0.5882\n",
            "Epoch 122: train_loss=0.661611 train_acc=0.6624 val_loss=0.651739 val_acc=0.5882\n",
            "Epoch 123: train_loss=0.644307 train_acc=0.6656 val_loss=0.914965 val_acc=0.6471\n",
            "Epoch 124: train_loss=0.665336 train_acc=0.6399 val_loss=0.610253 val_acc=0.5588\n",
            "Epoch 125: train_loss=0.630533 train_acc=0.6881 val_loss=0.627744 val_acc=0.6176\n",
            "Epoch 126: train_loss=0.615231 train_acc=0.7010 val_loss=0.784630 val_acc=0.6765\n",
            "Epoch 127: train_loss=0.621523 train_acc=0.6849 val_loss=0.610950 val_acc=0.5882\n",
            "Epoch 128: train_loss=0.620677 train_acc=0.7010 val_loss=0.822949 val_acc=0.6765\n",
            "Epoch 129: train_loss=0.651867 train_acc=0.6559 val_loss=0.737134 val_acc=0.6176\n",
            "Epoch 130: train_loss=0.706364 train_acc=0.6559 val_loss=1.861509 val_acc=0.5294\n",
            "Epoch 131: train_loss=1.040690 train_acc=0.5048 val_loss=0.694279 val_acc=0.6176\n",
            "Epoch 132: train_loss=0.804089 train_acc=0.5884 val_loss=1.805947 val_acc=0.5588\n",
            "Epoch 133: train_loss=0.840738 train_acc=0.5595 val_loss=0.709853 val_acc=0.5588\n",
            "Epoch 134: train_loss=0.716702 train_acc=0.6270 val_loss=0.639591 val_acc=0.5000\n",
            "Epoch 135: train_loss=0.733718 train_acc=0.6270 val_loss=0.635122 val_acc=0.5882\n",
            "Epoch 136: train_loss=0.669076 train_acc=0.6656 val_loss=0.670668 val_acc=0.5882\n",
            "Epoch 137: train_loss=0.655473 train_acc=0.6559 val_loss=0.703924 val_acc=0.5882\n",
            "Epoch 138: train_loss=0.656485 train_acc=0.6495 val_loss=0.673687 val_acc=0.5882\n",
            "Epoch 139: train_loss=0.676774 train_acc=0.6206 val_loss=0.651035 val_acc=0.5588\n",
            "Epoch 140: train_loss=0.668518 train_acc=0.6431 val_loss=0.701548 val_acc=0.6471\n",
            "Epoch 141: train_loss=0.667648 train_acc=0.6399 val_loss=0.586987 val_acc=0.5588\n",
            "Epoch 142: train_loss=0.654975 train_acc=0.6367 val_loss=0.639183 val_acc=0.6176\n",
            "Epoch 143: train_loss=0.638928 train_acc=0.6881 val_loss=0.700506 val_acc=0.6176\n",
            "Epoch 144: train_loss=0.634105 train_acc=0.6688 val_loss=0.571348 val_acc=0.6176\n",
            "Epoch 145: train_loss=0.654831 train_acc=0.6592 val_loss=0.747495 val_acc=0.6471\n",
            "Epoch 146: train_loss=0.647421 train_acc=0.6559 val_loss=0.589457 val_acc=0.5882\n",
            "Epoch 147: train_loss=0.660130 train_acc=0.6367 val_loss=0.578013 val_acc=0.6471\n",
            "Epoch 148: train_loss=0.664634 train_acc=0.6367 val_loss=0.633160 val_acc=0.6176\n",
            "Epoch 149: train_loss=0.638284 train_acc=0.6656 val_loss=0.659868 val_acc=0.6765\n",
            "Epoch 150: train_loss=0.627210 train_acc=0.6688 val_loss=0.708283 val_acc=0.6471\n",
            "Epoch 151: train_loss=0.620370 train_acc=0.6849 val_loss=0.673094 val_acc=0.5588\n",
            "Epoch 152: train_loss=0.612985 train_acc=0.6817 val_loss=0.633479 val_acc=0.6176\n",
            "Epoch 153: train_loss=0.610862 train_acc=0.6881 val_loss=0.665053 val_acc=0.6176\n",
            "Epoch 154: train_loss=0.611721 train_acc=0.6945 val_loss=0.636885 val_acc=0.5588\n",
            "Epoch 155: train_loss=0.620091 train_acc=0.6977 val_loss=0.574381 val_acc=0.6176\n",
            "Epoch 156: train_loss=0.606679 train_acc=0.6977 val_loss=0.695727 val_acc=0.6765\n",
            "Converged at epoch 157 (train_loss change 8.26e-05 < 0.0001)\n",
            "Train acc: 0.7138 | Test acc: 0.6300\n",
            "\n",
            "--- handwritten | RNN | layers=2 | hidden=128 ---\n",
            "Epoch 000: train_loss=1.601069 train_acc=0.1897 val_loss=1.494285 val_acc=0.4706\n",
            "Epoch 001: train_loss=1.249168 train_acc=0.4662 val_loss=0.935636 val_acc=0.5588\n",
            "Epoch 002: train_loss=0.977773 train_acc=0.4534 val_loss=0.896786 val_acc=0.4706\n",
            "Epoch 003: train_loss=0.920561 train_acc=0.4534 val_loss=0.852870 val_acc=0.5882\n",
            "Epoch 004: train_loss=0.882664 train_acc=0.5113 val_loss=0.705501 val_acc=0.4706\n",
            "Epoch 005: train_loss=0.791419 train_acc=0.5691 val_loss=0.718086 val_acc=0.5588\n",
            "Epoch 006: train_loss=0.782581 train_acc=0.5209 val_loss=0.659381 val_acc=0.5882\n",
            "Epoch 007: train_loss=0.768162 train_acc=0.5659 val_loss=0.678684 val_acc=0.5294\n",
            "Epoch 008: train_loss=0.737497 train_acc=0.5498 val_loss=0.704783 val_acc=0.6176\n",
            "Epoch 009: train_loss=0.723224 train_acc=0.5916 val_loss=0.658736 val_acc=0.5294\n",
            "Epoch 010: train_loss=0.985451 train_acc=0.4630 val_loss=0.827705 val_acc=0.4412\n",
            "Epoch 011: train_loss=0.843116 train_acc=0.4887 val_loss=0.640399 val_acc=0.5882\n",
            "Epoch 012: train_loss=0.747133 train_acc=0.5691 val_loss=0.638858 val_acc=0.5588\n",
            "Epoch 013: train_loss=0.755158 train_acc=0.6013 val_loss=1.050060 val_acc=0.5882\n",
            "Epoch 014: train_loss=0.821241 train_acc=0.5723 val_loss=0.669655 val_acc=0.5882\n",
            "Epoch 015: train_loss=0.764126 train_acc=0.5788 val_loss=0.646643 val_acc=0.5588\n",
            "Epoch 016: train_loss=0.718954 train_acc=0.5852 val_loss=0.616221 val_acc=0.5882\n",
            "Epoch 017: train_loss=0.710349 train_acc=0.5659 val_loss=0.677108 val_acc=0.5294\n",
            "Epoch 018: train_loss=0.717635 train_acc=0.5981 val_loss=0.652728 val_acc=0.5588\n",
            "Epoch 019: train_loss=0.696853 train_acc=0.6045 val_loss=0.653505 val_acc=0.6176\n",
            "Epoch 020: train_loss=0.689746 train_acc=0.6302 val_loss=0.648944 val_acc=0.5294\n",
            "Epoch 021: train_loss=0.706134 train_acc=0.6270 val_loss=0.818207 val_acc=0.5588\n",
            "Epoch 022: train_loss=1.136370 train_acc=0.4598 val_loss=0.664374 val_acc=0.5588\n",
            "Epoch 023: train_loss=0.933500 train_acc=0.4277 val_loss=0.821280 val_acc=0.4706\n",
            "Epoch 024: train_loss=0.915229 train_acc=0.4920 val_loss=0.789280 val_acc=0.4412\n",
            "Epoch 025: train_loss=0.847170 train_acc=0.5080 val_loss=0.680707 val_acc=0.6176\n",
            "Epoch 026: train_loss=0.805880 train_acc=0.5595 val_loss=0.697587 val_acc=0.5588\n",
            "Epoch 027: train_loss=0.772843 train_acc=0.5402 val_loss=0.672774 val_acc=0.6176\n",
            "Epoch 028: train_loss=0.776041 train_acc=0.5820 val_loss=0.653031 val_acc=0.5588\n",
            "Epoch 029: train_loss=0.733956 train_acc=0.5981 val_loss=0.674824 val_acc=0.5294\n",
            "Epoch 030: train_loss=0.721421 train_acc=0.6045 val_loss=0.653562 val_acc=0.5588\n",
            "Epoch 031: train_loss=0.706146 train_acc=0.6013 val_loss=0.678519 val_acc=0.6471\n",
            "Epoch 032: train_loss=0.694986 train_acc=0.6109 val_loss=0.646112 val_acc=0.5588\n",
            "Epoch 033: train_loss=0.682164 train_acc=0.6206 val_loss=0.685578 val_acc=0.5588\n",
            "Epoch 034: train_loss=0.689913 train_acc=0.6174 val_loss=0.674727 val_acc=0.5588\n",
            "Epoch 035: train_loss=0.687676 train_acc=0.6302 val_loss=0.650246 val_acc=0.5588\n",
            "Epoch 036: train_loss=0.685537 train_acc=0.6141 val_loss=0.693373 val_acc=0.5588\n",
            "Epoch 037: train_loss=0.698988 train_acc=0.6174 val_loss=0.676336 val_acc=0.5588\n",
            "Epoch 038: train_loss=0.676916 train_acc=0.6206 val_loss=0.674786 val_acc=0.5588\n",
            "Epoch 039: train_loss=0.712992 train_acc=0.6495 val_loss=0.698201 val_acc=0.5588\n",
            "Epoch 040: train_loss=0.689442 train_acc=0.6141 val_loss=0.678701 val_acc=0.5294\n",
            "Epoch 041: train_loss=0.688741 train_acc=0.6174 val_loss=0.688741 val_acc=0.5588\n",
            "Epoch 042: train_loss=0.695186 train_acc=0.5756 val_loss=0.641425 val_acc=0.6176\n",
            "Epoch 043: train_loss=0.737399 train_acc=0.5820 val_loss=0.674991 val_acc=0.5588\n",
            "Epoch 044: train_loss=0.711427 train_acc=0.6077 val_loss=0.621131 val_acc=0.5882\n",
            "Epoch 045: train_loss=0.700732 train_acc=0.6141 val_loss=0.650590 val_acc=0.5882\n",
            "Epoch 046: train_loss=0.680765 train_acc=0.6527 val_loss=0.631135 val_acc=0.5294\n",
            "Epoch 047: train_loss=0.679915 train_acc=0.6334 val_loss=0.687533 val_acc=0.5588\n",
            "Epoch 048: train_loss=0.677247 train_acc=0.6592 val_loss=0.697525 val_acc=0.5588\n",
            "Epoch 049: train_loss=0.661130 train_acc=0.6334 val_loss=0.654193 val_acc=0.6176\n",
            "Epoch 050: train_loss=0.680230 train_acc=0.6238 val_loss=0.773116 val_acc=0.6176\n",
            "Epoch 051: train_loss=0.666331 train_acc=0.6527 val_loss=0.645572 val_acc=0.5588\n",
            "Epoch 052: train_loss=0.660370 train_acc=0.6495 val_loss=0.787030 val_acc=0.6176\n",
            "Epoch 053: train_loss=0.663957 train_acc=0.6141 val_loss=0.705618 val_acc=0.5588\n",
            "Epoch 054: train_loss=0.651497 train_acc=0.6688 val_loss=0.642465 val_acc=0.5588\n",
            "Epoch 055: train_loss=0.644004 train_acc=0.6752 val_loss=0.732771 val_acc=0.5882\n",
            "Epoch 056: train_loss=0.684615 train_acc=0.6302 val_loss=0.672174 val_acc=0.5882\n",
            "Epoch 057: train_loss=0.674136 train_acc=0.6174 val_loss=0.588325 val_acc=0.5882\n",
            "Epoch 058: train_loss=0.663735 train_acc=0.6463 val_loss=0.600059 val_acc=0.5882\n",
            "Epoch 059: train_loss=0.666759 train_acc=0.6688 val_loss=0.654108 val_acc=0.5000\n",
            "Epoch 060: train_loss=0.681385 train_acc=0.6174 val_loss=0.619223 val_acc=0.5882\n",
            "Epoch 061: train_loss=0.659679 train_acc=0.6399 val_loss=0.694187 val_acc=0.5588\n",
            "Epoch 062: train_loss=0.653798 train_acc=0.6559 val_loss=0.719467 val_acc=0.6176\n",
            "Epoch 063: train_loss=0.661177 train_acc=0.6431 val_loss=0.619306 val_acc=0.5588\n",
            "Epoch 064: train_loss=0.638839 train_acc=0.6785 val_loss=0.713557 val_acc=0.6176\n",
            "Epoch 065: train_loss=0.699855 train_acc=0.6334 val_loss=1.169959 val_acc=0.4706\n",
            "Epoch 066: train_loss=0.857979 train_acc=0.5756 val_loss=0.873741 val_acc=0.5294\n",
            "Epoch 067: train_loss=0.754525 train_acc=0.5756 val_loss=0.669122 val_acc=0.5294\n",
            "Epoch 068: train_loss=0.824749 train_acc=0.5949 val_loss=0.794420 val_acc=0.5588\n",
            "Epoch 069: train_loss=0.722835 train_acc=0.6174 val_loss=0.705550 val_acc=0.5588\n",
            "Epoch 070: train_loss=0.691121 train_acc=0.6174 val_loss=0.681903 val_acc=0.5588\n",
            "Epoch 071: train_loss=0.677804 train_acc=0.6109 val_loss=0.689654 val_acc=0.6765\n",
            "Epoch 072: train_loss=0.663360 train_acc=0.6367 val_loss=0.707953 val_acc=0.5294\n",
            "Epoch 073: train_loss=0.654875 train_acc=0.6656 val_loss=0.685867 val_acc=0.5588\n",
            "Epoch 074: train_loss=0.649618 train_acc=0.6624 val_loss=0.865561 val_acc=0.6176\n",
            "Epoch 075: train_loss=0.705631 train_acc=0.6399 val_loss=0.613633 val_acc=0.5588\n",
            "Epoch 076: train_loss=0.847054 train_acc=0.5659 val_loss=0.695391 val_acc=0.5294\n",
            "Epoch 077: train_loss=0.731044 train_acc=0.5820 val_loss=0.749305 val_acc=0.5882\n",
            "Epoch 078: train_loss=0.689102 train_acc=0.6109 val_loss=0.622326 val_acc=0.5588\n",
            "Epoch 079: train_loss=0.685639 train_acc=0.6559 val_loss=0.685408 val_acc=0.5588\n",
            "Epoch 080: train_loss=0.677853 train_acc=0.6174 val_loss=0.676536 val_acc=0.7059\n",
            "Epoch 081: train_loss=0.669134 train_acc=0.6431 val_loss=0.773196 val_acc=0.6471\n",
            "Epoch 082: train_loss=0.670604 train_acc=0.6302 val_loss=0.658018 val_acc=0.6176\n",
            "Epoch 083: train_loss=0.672213 train_acc=0.6624 val_loss=0.763512 val_acc=0.6471\n",
            "Epoch 084: train_loss=0.753269 train_acc=0.6367 val_loss=0.668534 val_acc=0.5882\n",
            "Epoch 085: train_loss=0.661848 train_acc=0.6624 val_loss=0.831782 val_acc=0.6471\n",
            "Epoch 086: train_loss=0.723348 train_acc=0.5916 val_loss=0.619083 val_acc=0.5588\n",
            "Epoch 087: train_loss=0.693141 train_acc=0.5788 val_loss=0.645380 val_acc=0.6765\n",
            "Epoch 088: train_loss=0.705256 train_acc=0.6495 val_loss=0.936399 val_acc=0.5882\n",
            "Epoch 089: train_loss=0.896055 train_acc=0.5048 val_loss=0.838369 val_acc=0.5588\n",
            "Epoch 090: train_loss=0.835779 train_acc=0.5209 val_loss=0.715320 val_acc=0.5000\n",
            "Epoch 091: train_loss=0.744303 train_acc=0.5563 val_loss=0.580731 val_acc=0.6176\n",
            "Epoch 092: train_loss=0.712605 train_acc=0.5916 val_loss=0.629500 val_acc=0.5588\n",
            "Epoch 093: train_loss=0.693215 train_acc=0.5852 val_loss=0.757729 val_acc=0.5882\n",
            "Epoch 094: train_loss=0.704885 train_acc=0.6141 val_loss=0.591905 val_acc=0.6176\n",
            "Epoch 095: train_loss=0.704708 train_acc=0.5466 val_loss=0.677785 val_acc=0.5882\n",
            "Epoch 096: train_loss=0.699568 train_acc=0.5691 val_loss=0.626868 val_acc=0.5882\n",
            "Epoch 097: train_loss=0.694195 train_acc=0.6334 val_loss=0.619606 val_acc=0.6176\n",
            "Epoch 098: train_loss=0.677936 train_acc=0.6238 val_loss=0.671475 val_acc=0.6471\n",
            "Epoch 099: train_loss=0.676399 train_acc=0.5916 val_loss=0.681692 val_acc=0.5588\n",
            "Epoch 100: train_loss=0.663070 train_acc=0.6559 val_loss=0.692460 val_acc=0.5882\n",
            "Epoch 101: train_loss=0.654697 train_acc=0.6592 val_loss=0.705961 val_acc=0.5588\n",
            "Epoch 102: train_loss=0.645640 train_acc=0.6752 val_loss=0.743055 val_acc=0.6176\n",
            "Epoch 103: train_loss=0.645419 train_acc=0.6592 val_loss=0.723366 val_acc=0.6471\n",
            "Epoch 104: train_loss=0.647888 train_acc=0.6688 val_loss=0.642155 val_acc=0.5882\n",
            "Epoch 105: train_loss=0.660733 train_acc=0.6334 val_loss=0.713714 val_acc=0.5882\n",
            "Epoch 106: train_loss=0.679547 train_acc=0.6367 val_loss=0.684533 val_acc=0.5588\n",
            "Epoch 107: train_loss=0.753576 train_acc=0.6463 val_loss=1.053440 val_acc=0.5882\n",
            "Epoch 108: train_loss=0.942087 train_acc=0.5338 val_loss=1.023536 val_acc=0.5588\n",
            "Epoch 109: train_loss=0.797547 train_acc=0.5177 val_loss=0.632716 val_acc=0.5588\n",
            "Epoch 110: train_loss=0.738076 train_acc=0.5852 val_loss=0.696333 val_acc=0.6176\n",
            "Epoch 111: train_loss=0.746485 train_acc=0.5820 val_loss=0.687229 val_acc=0.6176\n",
            "Epoch 112: train_loss=0.743059 train_acc=0.5723 val_loss=0.746453 val_acc=0.5882\n",
            "Epoch 113: train_loss=0.710972 train_acc=0.6302 val_loss=0.614535 val_acc=0.6176\n",
            "Epoch 114: train_loss=0.667380 train_acc=0.6463 val_loss=0.786931 val_acc=0.5588\n",
            "Epoch 115: train_loss=0.675456 train_acc=0.6527 val_loss=0.807128 val_acc=0.5294\n",
            "Epoch 116: train_loss=0.687387 train_acc=0.6367 val_loss=0.771964 val_acc=0.6176\n",
            "Epoch 117: train_loss=0.742978 train_acc=0.6045 val_loss=0.754034 val_acc=0.5000\n",
            "Epoch 118: train_loss=0.840174 train_acc=0.5659 val_loss=0.831201 val_acc=0.5588\n",
            "Epoch 119: train_loss=0.713052 train_acc=0.6045 val_loss=0.638390 val_acc=0.6176\n",
            "Epoch 120: train_loss=0.714085 train_acc=0.5691 val_loss=0.680120 val_acc=0.6176\n",
            "Epoch 121: train_loss=0.676530 train_acc=0.6399 val_loss=0.691533 val_acc=0.6176\n",
            "Epoch 122: train_loss=0.655800 train_acc=0.6399 val_loss=0.685973 val_acc=0.5882\n",
            "Epoch 123: train_loss=0.656585 train_acc=0.6656 val_loss=0.722128 val_acc=0.6176\n",
            "Epoch 124: train_loss=0.651066 train_acc=0.6559 val_loss=0.627487 val_acc=0.6176\n",
            "Epoch 125: train_loss=0.640837 train_acc=0.6785 val_loss=0.621910 val_acc=0.6176\n",
            "Epoch 126: train_loss=0.655412 train_acc=0.6720 val_loss=0.579680 val_acc=0.5588\n",
            "Epoch 127: train_loss=0.665536 train_acc=0.6270 val_loss=0.604129 val_acc=0.6176\n",
            "Epoch 128: train_loss=0.631981 train_acc=0.6752 val_loss=0.747217 val_acc=0.6471\n",
            "Epoch 129: train_loss=0.662163 train_acc=0.6817 val_loss=0.775415 val_acc=0.6176\n",
            "Epoch 130: train_loss=0.652157 train_acc=0.6527 val_loss=0.663402 val_acc=0.6765\n",
            "Epoch 131: train_loss=0.620058 train_acc=0.6881 val_loss=0.686373 val_acc=0.7059\n",
            "Epoch 132: train_loss=0.615980 train_acc=0.6881 val_loss=0.741114 val_acc=0.7059\n",
            "Epoch 133: train_loss=0.946839 train_acc=0.6206 val_loss=0.604527 val_acc=0.5294\n",
            "Epoch 134: train_loss=0.905278 train_acc=0.5531 val_loss=0.594514 val_acc=0.5588\n",
            "Epoch 135: train_loss=0.796303 train_acc=0.5756 val_loss=0.608176 val_acc=0.5294\n",
            "Epoch 136: train_loss=0.782552 train_acc=0.5820 val_loss=3.010145 val_acc=0.5588\n",
            "Epoch 137: train_loss=0.756506 train_acc=0.6174 val_loss=0.571861 val_acc=0.6176\n",
            "Epoch 138: train_loss=0.732931 train_acc=0.5595 val_loss=0.744540 val_acc=0.6176\n",
            "Epoch 139: train_loss=0.681175 train_acc=0.6302 val_loss=0.587893 val_acc=0.6176\n",
            "Epoch 140: train_loss=0.736446 train_acc=0.6141 val_loss=0.591728 val_acc=0.6471\n",
            "Epoch 141: train_loss=0.745446 train_acc=0.6045 val_loss=0.701986 val_acc=0.5588\n",
            "Epoch 142: train_loss=0.712359 train_acc=0.6238 val_loss=0.706514 val_acc=0.5882\n",
            "Epoch 143: train_loss=0.755156 train_acc=0.6013 val_loss=0.552327 val_acc=0.6176\n",
            "Epoch 144: train_loss=0.750452 train_acc=0.5691 val_loss=0.738017 val_acc=0.5882\n",
            "Epoch 145: train_loss=0.726379 train_acc=0.5756 val_loss=0.711415 val_acc=0.6176\n",
            "Epoch 146: train_loss=0.694108 train_acc=0.6495 val_loss=0.709854 val_acc=0.5588\n",
            "Epoch 147: train_loss=0.715451 train_acc=0.6013 val_loss=0.652499 val_acc=0.5588\n",
            "Epoch 148: train_loss=0.675910 train_acc=0.6367 val_loss=0.810721 val_acc=0.6176\n",
            "Epoch 149: train_loss=0.676424 train_acc=0.6367 val_loss=0.598841 val_acc=0.5294\n",
            "Epoch 150: train_loss=0.682721 train_acc=0.6688 val_loss=0.713854 val_acc=0.6176\n",
            "Epoch 151: train_loss=0.677690 train_acc=0.6399 val_loss=0.612812 val_acc=0.5882\n",
            "Epoch 152: train_loss=0.681105 train_acc=0.6527 val_loss=0.733740 val_acc=0.6176\n",
            "Epoch 153: train_loss=0.669257 train_acc=0.6656 val_loss=0.584836 val_acc=0.6176\n",
            "Epoch 154: train_loss=0.683783 train_acc=0.6399 val_loss=0.640929 val_acc=0.5588\n",
            "Epoch 155: train_loss=0.694202 train_acc=0.6302 val_loss=2.611547 val_acc=0.6176\n",
            "Epoch 156: train_loss=0.675666 train_acc=0.6270 val_loss=2.440499 val_acc=0.5294\n",
            "Epoch 157: train_loss=0.707032 train_acc=0.6270 val_loss=2.505673 val_acc=0.5588\n",
            "Epoch 158: train_loss=0.962952 train_acc=0.5691 val_loss=0.821650 val_acc=0.4706\n",
            "Epoch 159: train_loss=1.187236 train_acc=0.3923 val_loss=0.984562 val_acc=0.5000\n",
            "Epoch 160: train_loss=1.076304 train_acc=0.4598 val_loss=0.772752 val_acc=0.4412\n",
            "Epoch 161: train_loss=0.864399 train_acc=0.5113 val_loss=0.701985 val_acc=0.5588\n",
            "Epoch 162: train_loss=0.782926 train_acc=0.6013 val_loss=0.661194 val_acc=0.5882\n",
            "Epoch 163: train_loss=0.731845 train_acc=0.5691 val_loss=0.621332 val_acc=0.6176\n",
            "Epoch 164: train_loss=0.723930 train_acc=0.5884 val_loss=0.622349 val_acc=0.6765\n",
            "Epoch 165: train_loss=0.692229 train_acc=0.5884 val_loss=0.631574 val_acc=0.5882\n",
            "Epoch 166: train_loss=0.698175 train_acc=0.6238 val_loss=0.623801 val_acc=0.6471\n",
            "Epoch 167: train_loss=0.681333 train_acc=0.6431 val_loss=0.606369 val_acc=0.6176\n",
            "Epoch 168: train_loss=0.671799 train_acc=0.6109 val_loss=0.615139 val_acc=0.6176\n",
            "Epoch 169: train_loss=0.678757 train_acc=0.6302 val_loss=0.657019 val_acc=0.5882\n",
            "Epoch 170: train_loss=0.672771 train_acc=0.6495 val_loss=0.586789 val_acc=0.6471\n",
            "Epoch 171: train_loss=0.647243 train_acc=0.6624 val_loss=0.653736 val_acc=0.6471\n",
            "Epoch 172: train_loss=0.652164 train_acc=0.6624 val_loss=0.614053 val_acc=0.5588\n",
            "Epoch 173: train_loss=0.648472 train_acc=0.6527 val_loss=0.786140 val_acc=0.5882\n",
            "Epoch 174: train_loss=0.679050 train_acc=0.6206 val_loss=0.507288 val_acc=0.6176\n",
            "Epoch 175: train_loss=0.802164 train_acc=0.5949 val_loss=0.608951 val_acc=0.5882\n",
            "Epoch 176: train_loss=0.782648 train_acc=0.5402 val_loss=0.802060 val_acc=0.5588\n",
            "Epoch 177: train_loss=0.733414 train_acc=0.5691 val_loss=0.639830 val_acc=0.5882\n",
            "Epoch 178: train_loss=0.716237 train_acc=0.6109 val_loss=0.679294 val_acc=0.5588\n",
            "Epoch 179: train_loss=0.712454 train_acc=0.5916 val_loss=0.613352 val_acc=0.6176\n",
            "Epoch 180: train_loss=0.682719 train_acc=0.6334 val_loss=0.701227 val_acc=0.5294\n",
            "Epoch 181: train_loss=0.683803 train_acc=0.5884 val_loss=0.672885 val_acc=0.5588\n",
            "Epoch 182: train_loss=0.675293 train_acc=0.6367 val_loss=0.680437 val_acc=0.6176\n",
            "Epoch 183: train_loss=0.665311 train_acc=0.6367 val_loss=0.697617 val_acc=0.5588\n",
            "Epoch 184: train_loss=0.655314 train_acc=0.6431 val_loss=0.717922 val_acc=0.5882\n",
            "Epoch 185: train_loss=0.658078 train_acc=0.6495 val_loss=0.706364 val_acc=0.6471\n",
            "Epoch 186: train_loss=0.636182 train_acc=0.6785 val_loss=0.729479 val_acc=0.5588\n",
            "Epoch 187: train_loss=0.645538 train_acc=0.6624 val_loss=0.744375 val_acc=0.6176\n",
            "Epoch 188: train_loss=0.657633 train_acc=0.6141 val_loss=0.650999 val_acc=0.6471\n",
            "Epoch 189: train_loss=0.639017 train_acc=0.6913 val_loss=0.725781 val_acc=0.5882\n",
            "Epoch 190: train_loss=0.633430 train_acc=0.6431 val_loss=0.662168 val_acc=0.6471\n",
            "Epoch 191: train_loss=0.625862 train_acc=0.6785 val_loss=0.800637 val_acc=0.6765\n",
            "Epoch 192: train_loss=0.650687 train_acc=0.6270 val_loss=0.558031 val_acc=0.6176\n",
            "Epoch 193: train_loss=0.670577 train_acc=0.6141 val_loss=0.654459 val_acc=0.6471\n",
            "Epoch 194: train_loss=0.665823 train_acc=0.6463 val_loss=0.646640 val_acc=0.6176\n",
            "Epoch 195: train_loss=0.664738 train_acc=0.6463 val_loss=0.658850 val_acc=0.5882\n",
            "Epoch 196: train_loss=0.652385 train_acc=0.6045 val_loss=0.691166 val_acc=0.6471\n",
            "Epoch 197: train_loss=0.629976 train_acc=0.6399 val_loss=0.634946 val_acc=0.5294\n",
            "Epoch 198: train_loss=0.632975 train_acc=0.6785 val_loss=0.715218 val_acc=0.6176\n",
            "Epoch 199: train_loss=0.624447 train_acc=0.6817 val_loss=0.698890 val_acc=0.7059\n",
            "Train acc: 0.6913 | Test acc: 0.6400\n",
            "\n",
            "--- handwritten | LSTM | layers=1 | hidden=64 ---\n",
            "Epoch 000: train_loss=1.605331 train_acc=0.2058 val_loss=1.622894 val_acc=0.1471\n",
            "Epoch 001: train_loss=1.590341 train_acc=0.2862 val_loss=1.601117 val_acc=0.3529\n",
            "Epoch 002: train_loss=1.564509 train_acc=0.3826 val_loss=1.568901 val_acc=0.3235\n",
            "Epoch 003: train_loss=1.501861 train_acc=0.2540 val_loss=1.428272 val_acc=0.5294\n",
            "Epoch 004: train_loss=1.400729 train_acc=0.4437 val_loss=1.277529 val_acc=0.5294\n",
            "Epoch 005: train_loss=1.321621 train_acc=0.4695 val_loss=1.162373 val_acc=0.4412\n",
            "Epoch 006: train_loss=1.189237 train_acc=0.4148 val_loss=0.930619 val_acc=0.4412\n",
            "Epoch 007: train_loss=1.097238 train_acc=0.4180 val_loss=0.861877 val_acc=0.4706\n",
            "Epoch 008: train_loss=1.061529 train_acc=0.4148 val_loss=0.921750 val_acc=0.5588\n",
            "Epoch 009: train_loss=1.030130 train_acc=0.4469 val_loss=0.901181 val_acc=0.5000\n",
            "Epoch 010: train_loss=0.989408 train_acc=0.5177 val_loss=0.825491 val_acc=0.5588\n",
            "Epoch 011: train_loss=0.955020 train_acc=0.5145 val_loss=0.805895 val_acc=0.5588\n",
            "Epoch 012: train_loss=0.920656 train_acc=0.5466 val_loss=0.793755 val_acc=0.6176\n",
            "Epoch 013: train_loss=0.917395 train_acc=0.5209 val_loss=0.774478 val_acc=0.5588\n",
            "Epoch 014: train_loss=0.909844 train_acc=0.5595 val_loss=0.721689 val_acc=0.5588\n",
            "Epoch 015: train_loss=0.883446 train_acc=0.5756 val_loss=0.727922 val_acc=0.5588\n",
            "Epoch 016: train_loss=0.873194 train_acc=0.5498 val_loss=0.728127 val_acc=0.5588\n",
            "Epoch 017: train_loss=0.841520 train_acc=0.5916 val_loss=0.720385 val_acc=0.5882\n",
            "Epoch 018: train_loss=0.816977 train_acc=0.6334 val_loss=0.728891 val_acc=0.5882\n",
            "Epoch 019: train_loss=0.807500 train_acc=0.5820 val_loss=0.671566 val_acc=0.5294\n",
            "Epoch 020: train_loss=0.786866 train_acc=0.6270 val_loss=0.655933 val_acc=0.5588\n",
            "Epoch 021: train_loss=0.797613 train_acc=0.6367 val_loss=0.698548 val_acc=0.5588\n",
            "Epoch 022: train_loss=0.834609 train_acc=0.5916 val_loss=0.816660 val_acc=0.4706\n",
            "Epoch 023: train_loss=0.831374 train_acc=0.5852 val_loss=0.680705 val_acc=0.5882\n",
            "Epoch 024: train_loss=0.813359 train_acc=0.5916 val_loss=0.704151 val_acc=0.4706\n",
            "Epoch 025: train_loss=0.787173 train_acc=0.6399 val_loss=0.680075 val_acc=0.5588\n",
            "Epoch 026: train_loss=0.767242 train_acc=0.6463 val_loss=0.650357 val_acc=0.7353\n",
            "Epoch 027: train_loss=0.748639 train_acc=0.6752 val_loss=0.634383 val_acc=0.5588\n",
            "Epoch 028: train_loss=0.738861 train_acc=0.6913 val_loss=0.628646 val_acc=0.7941\n",
            "Epoch 029: train_loss=0.701132 train_acc=0.7074 val_loss=0.678683 val_acc=0.6471\n",
            "Epoch 030: train_loss=0.661803 train_acc=0.7299 val_loss=0.503810 val_acc=0.7353\n",
            "Epoch 031: train_loss=0.660448 train_acc=0.7299 val_loss=0.629055 val_acc=0.5588\n",
            "Epoch 032: train_loss=0.776608 train_acc=0.6592 val_loss=0.548146 val_acc=0.7353\n",
            "Epoch 033: train_loss=0.692075 train_acc=0.7010 val_loss=0.487757 val_acc=0.7353\n",
            "Epoch 034: train_loss=0.622947 train_acc=0.7395 val_loss=0.771900 val_acc=0.6765\n",
            "Epoch 035: train_loss=0.690294 train_acc=0.7010 val_loss=0.505348 val_acc=0.7059\n",
            "Epoch 036: train_loss=0.599288 train_acc=0.7331 val_loss=0.499425 val_acc=0.7059\n",
            "Epoch 037: train_loss=0.582096 train_acc=0.7331 val_loss=0.518394 val_acc=0.7059\n",
            "Epoch 038: train_loss=0.685802 train_acc=0.7010 val_loss=0.521210 val_acc=0.6471\n",
            "Epoch 039: train_loss=0.838763 train_acc=0.6077 val_loss=0.664436 val_acc=0.6176\n",
            "Epoch 040: train_loss=0.681990 train_acc=0.7138 val_loss=0.670372 val_acc=0.6176\n",
            "Epoch 041: train_loss=0.692057 train_acc=0.7106 val_loss=0.524164 val_acc=0.7353\n",
            "Epoch 042: train_loss=0.688622 train_acc=0.6913 val_loss=0.597476 val_acc=0.6765\n",
            "Epoch 043: train_loss=0.572278 train_acc=0.7685 val_loss=0.444571 val_acc=0.7059\n",
            "Epoch 044: train_loss=0.539809 train_acc=0.7363 val_loss=0.398770 val_acc=0.7353\n",
            "Epoch 045: train_loss=0.474602 train_acc=0.7846 val_loss=0.439912 val_acc=0.7353\n",
            "Epoch 046: train_loss=0.463205 train_acc=0.7781 val_loss=0.386809 val_acc=0.7353\n",
            "Epoch 047: train_loss=0.433472 train_acc=0.7974 val_loss=0.415590 val_acc=0.8235\n",
            "Epoch 048: train_loss=0.389964 train_acc=0.8264 val_loss=0.432699 val_acc=0.7647\n",
            "Epoch 049: train_loss=0.386644 train_acc=0.8103 val_loss=0.368825 val_acc=0.7941\n",
            "Epoch 050: train_loss=0.371419 train_acc=0.8167 val_loss=0.390108 val_acc=0.7353\n",
            "Epoch 051: train_loss=0.367740 train_acc=0.8457 val_loss=0.351609 val_acc=0.8235\n",
            "Epoch 052: train_loss=0.339344 train_acc=0.8489 val_loss=0.362914 val_acc=0.8529\n",
            "Epoch 053: train_loss=0.325646 train_acc=0.8457 val_loss=0.399112 val_acc=0.8235\n",
            "Epoch 054: train_loss=0.322306 train_acc=0.8521 val_loss=0.368316 val_acc=0.7941\n",
            "Epoch 055: train_loss=0.326465 train_acc=0.8457 val_loss=0.458560 val_acc=0.8529\n",
            "Epoch 056: train_loss=0.341792 train_acc=0.8232 val_loss=0.427703 val_acc=0.8529\n",
            "Epoch 057: train_loss=0.437403 train_acc=0.7717 val_loss=0.375772 val_acc=0.7941\n",
            "Epoch 058: train_loss=0.965697 train_acc=0.6077 val_loss=0.542692 val_acc=0.6765\n",
            "Epoch 059: train_loss=0.759288 train_acc=0.7106 val_loss=0.531195 val_acc=0.6471\n",
            "Epoch 060: train_loss=0.724912 train_acc=0.6913 val_loss=0.499688 val_acc=0.7647\n",
            "Epoch 061: train_loss=0.629748 train_acc=0.7267 val_loss=1.160800 val_acc=0.7059\n",
            "Epoch 062: train_loss=0.602501 train_acc=0.7331 val_loss=0.433168 val_acc=0.7059\n",
            "Epoch 063: train_loss=0.583759 train_acc=0.7685 val_loss=0.446328 val_acc=0.8235\n",
            "Epoch 064: train_loss=0.499518 train_acc=0.7781 val_loss=0.406121 val_acc=0.7353\n",
            "Epoch 065: train_loss=0.454823 train_acc=0.7910 val_loss=0.412403 val_acc=0.7353\n",
            "Epoch 066: train_loss=0.433437 train_acc=0.8039 val_loss=0.388756 val_acc=0.7941\n",
            "Epoch 067: train_loss=0.419380 train_acc=0.7846 val_loss=0.367965 val_acc=0.8235\n",
            "Epoch 068: train_loss=0.404220 train_acc=0.8264 val_loss=0.372742 val_acc=0.8235\n",
            "Epoch 069: train_loss=0.395307 train_acc=0.8199 val_loss=0.346529 val_acc=0.8529\n",
            "Epoch 070: train_loss=0.376860 train_acc=0.8489 val_loss=0.348033 val_acc=0.8529\n",
            "Epoch 071: train_loss=0.371875 train_acc=0.8553 val_loss=0.341421 val_acc=0.8529\n",
            "Epoch 072: train_loss=0.364889 train_acc=0.8328 val_loss=0.381689 val_acc=0.8824\n",
            "Epoch 073: train_loss=0.351097 train_acc=0.8617 val_loss=0.364799 val_acc=0.8824\n",
            "Epoch 074: train_loss=0.338270 train_acc=0.8746 val_loss=0.358262 val_acc=0.8824\n",
            "Epoch 075: train_loss=0.331677 train_acc=0.8746 val_loss=0.360857 val_acc=0.8824\n",
            "Epoch 076: train_loss=0.328577 train_acc=0.8810 val_loss=0.410076 val_acc=0.8529\n",
            "Epoch 077: train_loss=0.333515 train_acc=0.8585 val_loss=0.487283 val_acc=0.8529\n",
            "Epoch 078: train_loss=0.343394 train_acc=0.8585 val_loss=0.336270 val_acc=0.7941\n",
            "Epoch 079: train_loss=0.322462 train_acc=0.8521 val_loss=0.519134 val_acc=0.8529\n",
            "Epoch 080: train_loss=0.311064 train_acc=0.8650 val_loss=0.346090 val_acc=0.7941\n",
            "Epoch 081: train_loss=0.381023 train_acc=0.8328 val_loss=0.300499 val_acc=0.7941\n",
            "Epoch 082: train_loss=0.337988 train_acc=0.8746 val_loss=0.485811 val_acc=0.8824\n",
            "Epoch 083: train_loss=0.307287 train_acc=0.8650 val_loss=0.352000 val_acc=0.8824\n",
            "Epoch 084: train_loss=0.303910 train_acc=0.8842 val_loss=0.397360 val_acc=0.8235\n",
            "Epoch 085: train_loss=0.304732 train_acc=0.8810 val_loss=0.483090 val_acc=0.8529\n",
            "Epoch 086: train_loss=0.291722 train_acc=0.8778 val_loss=0.523981 val_acc=0.8529\n",
            "Epoch 087: train_loss=0.376974 train_acc=0.8424 val_loss=0.597071 val_acc=0.7059\n",
            "Epoch 088: train_loss=1.431552 train_acc=0.5048 val_loss=1.132914 val_acc=0.6471\n",
            "Epoch 089: train_loss=0.743012 train_acc=0.6720 val_loss=0.378161 val_acc=0.7059\n",
            "Epoch 090: train_loss=0.506242 train_acc=0.7524 val_loss=0.419611 val_acc=0.7059\n",
            "Epoch 091: train_loss=0.453827 train_acc=0.7717 val_loss=0.372838 val_acc=0.7353\n",
            "Epoch 092: train_loss=0.432445 train_acc=0.8006 val_loss=0.382049 val_acc=0.7941\n",
            "Epoch 093: train_loss=0.384534 train_acc=0.8199 val_loss=0.343131 val_acc=0.7647\n",
            "Epoch 094: train_loss=0.348300 train_acc=0.8264 val_loss=0.346383 val_acc=0.7941\n",
            "Epoch 095: train_loss=0.335908 train_acc=0.8457 val_loss=0.333555 val_acc=0.7647\n",
            "Epoch 096: train_loss=0.335608 train_acc=0.8296 val_loss=0.367189 val_acc=0.8529\n",
            "Epoch 097: train_loss=0.327301 train_acc=0.8585 val_loss=0.309164 val_acc=0.7941\n",
            "Epoch 098: train_loss=0.313605 train_acc=0.8328 val_loss=0.318275 val_acc=0.7941\n",
            "Epoch 099: train_loss=0.319121 train_acc=0.8746 val_loss=0.318615 val_acc=0.8824\n",
            "Epoch 100: train_loss=0.310491 train_acc=0.8457 val_loss=0.327554 val_acc=0.7647\n",
            "Epoch 101: train_loss=0.286845 train_acc=0.8714 val_loss=0.333054 val_acc=0.8529\n",
            "Epoch 102: train_loss=0.288680 train_acc=0.8521 val_loss=0.282619 val_acc=0.8529\n",
            "Epoch 103: train_loss=0.282772 train_acc=0.8650 val_loss=0.300476 val_acc=0.8235\n",
            "Epoch 104: train_loss=0.268940 train_acc=0.8778 val_loss=0.287318 val_acc=0.8824\n",
            "Epoch 105: train_loss=0.260663 train_acc=0.8746 val_loss=0.289834 val_acc=0.8824\n",
            "Epoch 106: train_loss=0.257148 train_acc=0.8875 val_loss=0.312489 val_acc=0.9118\n",
            "Epoch 107: train_loss=0.255429 train_acc=0.8875 val_loss=0.278083 val_acc=0.7941\n",
            "Epoch 108: train_loss=0.272978 train_acc=0.8682 val_loss=0.393828 val_acc=0.8235\n",
            "Epoch 109: train_loss=0.282414 train_acc=0.8553 val_loss=0.301964 val_acc=0.9118\n",
            "Epoch 110: train_loss=0.261051 train_acc=0.8617 val_loss=0.266838 val_acc=0.8824\n",
            "Epoch 111: train_loss=0.251520 train_acc=0.8939 val_loss=0.323342 val_acc=0.8529\n",
            "Epoch 112: train_loss=0.254246 train_acc=0.8810 val_loss=0.290745 val_acc=0.9118\n",
            "Epoch 113: train_loss=0.262469 train_acc=0.9035 val_loss=0.395796 val_acc=0.8529\n",
            "Epoch 114: train_loss=0.257508 train_acc=0.8810 val_loss=0.218198 val_acc=0.8235\n",
            "Epoch 115: train_loss=0.275044 train_acc=0.8489 val_loss=0.276854 val_acc=0.9118\n",
            "Epoch 116: train_loss=0.249124 train_acc=0.8939 val_loss=0.341145 val_acc=0.8529\n",
            "Epoch 117: train_loss=0.235045 train_acc=0.9035 val_loss=0.263175 val_acc=0.9118\n",
            "Epoch 118: train_loss=0.234730 train_acc=0.8907 val_loss=0.238412 val_acc=0.9118\n",
            "Epoch 119: train_loss=0.237827 train_acc=0.8778 val_loss=0.251444 val_acc=0.7941\n",
            "Epoch 120: train_loss=0.244726 train_acc=0.8939 val_loss=0.293162 val_acc=0.8824\n",
            "Epoch 121: train_loss=0.223992 train_acc=0.9100 val_loss=0.309724 val_acc=0.8529\n",
            "Epoch 122: train_loss=0.261339 train_acc=0.8778 val_loss=0.286249 val_acc=0.8235\n",
            "Epoch 123: train_loss=0.252493 train_acc=0.8842 val_loss=0.255269 val_acc=0.7647\n",
            "Epoch 124: train_loss=0.250190 train_acc=0.8875 val_loss=0.263336 val_acc=0.9118\n",
            "Epoch 125: train_loss=0.501934 train_acc=0.8103 val_loss=2.677723 val_acc=0.4118\n",
            "Epoch 126: train_loss=1.714525 train_acc=0.5402 val_loss=2.022822 val_acc=0.4706\n",
            "Epoch 127: train_loss=1.007275 train_acc=0.6559 val_loss=1.546850 val_acc=0.6471\n",
            "Epoch 128: train_loss=0.744177 train_acc=0.7267 val_loss=0.944653 val_acc=0.7059\n",
            "Epoch 129: train_loss=0.599592 train_acc=0.7749 val_loss=0.590076 val_acc=0.7353\n",
            "Epoch 130: train_loss=0.544913 train_acc=0.7878 val_loss=0.527419 val_acc=0.7647\n",
            "Epoch 131: train_loss=0.491240 train_acc=0.8103 val_loss=0.485805 val_acc=0.8824\n",
            "Epoch 132: train_loss=0.430650 train_acc=0.8521 val_loss=0.640969 val_acc=0.7941\n",
            "Epoch 133: train_loss=0.437979 train_acc=0.8167 val_loss=0.514613 val_acc=0.8529\n",
            "Epoch 134: train_loss=0.379590 train_acc=0.8746 val_loss=0.430435 val_acc=0.8824\n",
            "Epoch 135: train_loss=0.330543 train_acc=0.8714 val_loss=0.506269 val_acc=0.8824\n",
            "Epoch 136: train_loss=0.335120 train_acc=0.8714 val_loss=0.327719 val_acc=0.8235\n",
            "Epoch 137: train_loss=0.319571 train_acc=0.8650 val_loss=0.558161 val_acc=0.8235\n",
            "Epoch 138: train_loss=0.304985 train_acc=0.8778 val_loss=0.448172 val_acc=0.8529\n",
            "Epoch 139: train_loss=0.301801 train_acc=0.8939 val_loss=0.408051 val_acc=0.8529\n",
            "Epoch 140: train_loss=0.299891 train_acc=0.9068 val_loss=0.357812 val_acc=0.8824\n",
            "Epoch 141: train_loss=0.285811 train_acc=0.8907 val_loss=0.405219 val_acc=0.8529\n",
            "Epoch 142: train_loss=0.275543 train_acc=0.9068 val_loss=0.347877 val_acc=0.8529\n",
            "Epoch 143: train_loss=0.290072 train_acc=0.8778 val_loss=0.401213 val_acc=0.8824\n",
            "Epoch 144: train_loss=0.259255 train_acc=0.9068 val_loss=0.388429 val_acc=0.8529\n",
            "Epoch 145: train_loss=0.247893 train_acc=0.9035 val_loss=0.418786 val_acc=0.8529\n",
            "Epoch 146: train_loss=0.241393 train_acc=0.9068 val_loss=0.613451 val_acc=0.8529\n",
            "Epoch 147: train_loss=0.272380 train_acc=0.8875 val_loss=0.475821 val_acc=0.8529\n",
            "Epoch 148: train_loss=0.243471 train_acc=0.9068 val_loss=0.360201 val_acc=0.8824\n",
            "Epoch 149: train_loss=0.253563 train_acc=0.9003 val_loss=0.322424 val_acc=0.9118\n",
            "Epoch 150: train_loss=0.254152 train_acc=0.8746 val_loss=0.758394 val_acc=0.8529\n",
            "Epoch 151: train_loss=0.250103 train_acc=0.8971 val_loss=0.335243 val_acc=0.8824\n",
            "Epoch 152: train_loss=0.240989 train_acc=0.9003 val_loss=0.324216 val_acc=0.9118\n",
            "Epoch 153: train_loss=0.222709 train_acc=0.9068 val_loss=0.612335 val_acc=0.8529\n",
            "Epoch 154: train_loss=0.264946 train_acc=0.8778 val_loss=0.274214 val_acc=0.8529\n",
            "Epoch 155: train_loss=0.258529 train_acc=0.9003 val_loss=0.304358 val_acc=0.8529\n",
            "Epoch 156: train_loss=0.261426 train_acc=0.8778 val_loss=0.684937 val_acc=0.8529\n",
            "Converged at epoch 157 (train_loss change 6.89e-06 < 0.0001)\n",
            "Train acc: 0.8585 | Test acc: 0.8600\n",
            "\n",
            "--- handwritten | LSTM | layers=1 | hidden=128 ---\n",
            "Epoch 000: train_loss=1.601699 train_acc=0.1929 val_loss=1.588578 val_acc=0.2059\n",
            "Epoch 001: train_loss=1.571941 train_acc=0.2058 val_loss=1.530692 val_acc=0.2647\n",
            "Epoch 002: train_loss=1.479266 train_acc=0.3505 val_loss=1.441530 val_acc=0.4412\n",
            "Epoch 003: train_loss=1.415883 train_acc=0.4502 val_loss=1.287049 val_acc=0.4706\n",
            "Epoch 004: train_loss=1.301175 train_acc=0.4566 val_loss=1.078173 val_acc=0.4412\n",
            "Epoch 005: train_loss=1.150292 train_acc=0.4405 val_loss=0.899807 val_acc=0.5294\n",
            "Epoch 006: train_loss=1.135261 train_acc=0.4373 val_loss=0.885776 val_acc=0.5882\n",
            "Epoch 007: train_loss=1.047241 train_acc=0.4855 val_loss=0.867403 val_acc=0.5294\n",
            "Epoch 008: train_loss=1.093682 train_acc=0.4630 val_loss=1.268092 val_acc=0.2353\n",
            "Epoch 009: train_loss=1.241908 train_acc=0.3955 val_loss=1.442380 val_acc=0.3529\n",
            "Epoch 010: train_loss=1.188790 train_acc=0.4469 val_loss=1.287130 val_acc=0.3529\n",
            "Epoch 011: train_loss=1.096759 train_acc=0.4469 val_loss=1.195288 val_acc=0.3824\n",
            "Epoch 012: train_loss=1.032086 train_acc=0.4920 val_loss=1.101150 val_acc=0.4412\n",
            "Epoch 013: train_loss=1.016021 train_acc=0.5659 val_loss=0.990121 val_acc=0.4706\n",
            "Epoch 014: train_loss=0.986341 train_acc=0.5595 val_loss=1.002583 val_acc=0.4706\n",
            "Epoch 015: train_loss=0.930035 train_acc=0.5531 val_loss=0.824756 val_acc=0.5882\n",
            "Epoch 016: train_loss=0.873044 train_acc=0.6141 val_loss=0.745099 val_acc=0.6471\n",
            "Epoch 017: train_loss=0.854112 train_acc=0.6045 val_loss=0.675019 val_acc=0.5588\n",
            "Epoch 018: train_loss=0.811545 train_acc=0.6367 val_loss=0.705101 val_acc=0.6765\n",
            "Epoch 019: train_loss=0.787745 train_acc=0.6624 val_loss=0.684237 val_acc=0.6471\n",
            "Epoch 020: train_loss=0.842043 train_acc=0.5981 val_loss=0.778676 val_acc=0.5588\n",
            "Epoch 021: train_loss=0.805712 train_acc=0.6431 val_loss=0.737722 val_acc=0.6176\n",
            "Epoch 022: train_loss=0.749549 train_acc=0.6688 val_loss=0.630384 val_acc=0.6765\n",
            "Epoch 023: train_loss=0.709449 train_acc=0.6945 val_loss=0.572685 val_acc=0.7059\n",
            "Epoch 024: train_loss=0.689389 train_acc=0.6945 val_loss=0.585181 val_acc=0.6765\n",
            "Epoch 025: train_loss=0.622613 train_acc=0.7042 val_loss=0.460423 val_acc=0.7941\n",
            "Epoch 026: train_loss=0.629695 train_acc=0.7203 val_loss=0.522377 val_acc=0.7647\n",
            "Epoch 027: train_loss=0.681218 train_acc=0.7074 val_loss=0.534507 val_acc=0.7353\n",
            "Epoch 028: train_loss=0.662258 train_acc=0.6849 val_loss=0.562657 val_acc=0.7941\n",
            "Epoch 029: train_loss=0.552600 train_acc=0.7524 val_loss=0.450939 val_acc=0.7353\n",
            "Epoch 030: train_loss=0.467170 train_acc=0.7910 val_loss=0.402034 val_acc=0.7647\n",
            "Epoch 031: train_loss=0.447436 train_acc=0.8071 val_loss=0.383622 val_acc=0.7647\n",
            "Epoch 032: train_loss=0.496567 train_acc=0.7878 val_loss=0.487906 val_acc=0.7647\n",
            "Epoch 033: train_loss=0.398525 train_acc=0.8232 val_loss=0.368421 val_acc=0.8235\n",
            "Epoch 034: train_loss=0.406854 train_acc=0.8296 val_loss=0.452377 val_acc=0.8235\n",
            "Epoch 035: train_loss=0.398267 train_acc=0.8039 val_loss=0.445580 val_acc=0.7941\n",
            "Epoch 036: train_loss=0.708748 train_acc=0.6849 val_loss=0.669754 val_acc=0.5588\n",
            "Epoch 037: train_loss=0.852767 train_acc=0.6270 val_loss=0.574472 val_acc=0.7059\n",
            "Epoch 038: train_loss=0.638018 train_acc=0.7138 val_loss=0.546849 val_acc=0.7353\n",
            "Epoch 039: train_loss=0.583745 train_acc=0.7363 val_loss=0.991233 val_acc=0.7647\n",
            "Epoch 040: train_loss=0.515571 train_acc=0.7974 val_loss=0.399856 val_acc=0.7941\n",
            "Epoch 041: train_loss=0.464989 train_acc=0.7685 val_loss=0.384540 val_acc=0.7353\n",
            "Epoch 042: train_loss=0.559012 train_acc=0.7363 val_loss=0.642911 val_acc=0.7941\n",
            "Epoch 043: train_loss=0.649052 train_acc=0.7331 val_loss=0.455399 val_acc=0.7941\n",
            "Epoch 044: train_loss=0.695181 train_acc=0.6913 val_loss=0.725559 val_acc=0.6176\n",
            "Epoch 045: train_loss=0.544397 train_acc=0.7203 val_loss=0.407144 val_acc=0.7059\n",
            "Epoch 046: train_loss=0.460748 train_acc=0.7942 val_loss=0.516694 val_acc=0.7941\n",
            "Epoch 047: train_loss=0.422824 train_acc=0.8039 val_loss=0.518160 val_acc=0.7647\n",
            "Epoch 048: train_loss=0.400168 train_acc=0.8039 val_loss=0.387624 val_acc=0.7941\n",
            "Epoch 049: train_loss=0.416587 train_acc=0.7878 val_loss=0.379223 val_acc=0.8235\n",
            "Epoch 050: train_loss=0.379084 train_acc=0.8135 val_loss=0.381743 val_acc=0.8235\n",
            "Epoch 051: train_loss=0.349690 train_acc=0.8167 val_loss=0.341085 val_acc=0.7941\n",
            "Epoch 052: train_loss=0.325766 train_acc=0.8328 val_loss=0.339215 val_acc=0.7941\n",
            "Epoch 053: train_loss=0.329840 train_acc=0.8457 val_loss=0.359078 val_acc=0.8235\n",
            "Epoch 054: train_loss=0.323413 train_acc=0.8232 val_loss=0.379391 val_acc=0.8235\n",
            "Epoch 055: train_loss=0.338160 train_acc=0.8199 val_loss=0.298374 val_acc=0.7941\n",
            "Epoch 056: train_loss=0.330751 train_acc=0.8328 val_loss=0.348059 val_acc=0.7647\n",
            "Epoch 057: train_loss=0.322783 train_acc=0.8521 val_loss=0.334212 val_acc=0.8824\n",
            "Epoch 058: train_loss=0.452069 train_acc=0.7942 val_loss=0.333711 val_acc=0.8235\n",
            "Epoch 059: train_loss=0.426173 train_acc=0.8071 val_loss=0.314025 val_acc=0.8529\n",
            "Epoch 060: train_loss=0.348320 train_acc=0.8392 val_loss=0.339828 val_acc=0.8824\n",
            "Epoch 061: train_loss=0.334320 train_acc=0.8360 val_loss=0.303032 val_acc=0.7941\n",
            "Epoch 062: train_loss=0.306444 train_acc=0.8650 val_loss=0.355596 val_acc=0.8235\n",
            "Epoch 063: train_loss=0.297115 train_acc=0.8521 val_loss=0.273771 val_acc=0.7941\n",
            "Epoch 064: train_loss=0.300525 train_acc=0.8521 val_loss=0.367130 val_acc=0.8235\n",
            "Epoch 065: train_loss=0.285268 train_acc=0.8553 val_loss=0.272351 val_acc=0.8235\n",
            "Epoch 066: train_loss=0.292024 train_acc=0.8585 val_loss=0.266475 val_acc=0.8235\n",
            "Epoch 067: train_loss=0.306017 train_acc=0.8617 val_loss=0.297303 val_acc=0.8235\n",
            "Epoch 068: train_loss=0.311659 train_acc=0.8489 val_loss=0.315376 val_acc=0.8529\n",
            "Epoch 069: train_loss=0.299625 train_acc=0.8424 val_loss=0.257557 val_acc=0.8235\n",
            "Epoch 070: train_loss=0.291465 train_acc=0.8714 val_loss=0.360044 val_acc=0.8235\n",
            "Epoch 071: train_loss=0.292017 train_acc=0.8424 val_loss=0.273105 val_acc=0.8235\n",
            "Epoch 072: train_loss=0.285946 train_acc=0.8553 val_loss=0.311769 val_acc=0.7941\n",
            "Epoch 073: train_loss=0.365850 train_acc=0.8360 val_loss=0.256459 val_acc=0.8529\n",
            "Epoch 074: train_loss=0.367867 train_acc=0.8199 val_loss=0.298016 val_acc=0.7941\n",
            "Epoch 075: train_loss=0.326629 train_acc=0.8135 val_loss=0.360282 val_acc=0.8235\n",
            "Epoch 076: train_loss=0.307266 train_acc=0.8650 val_loss=0.273587 val_acc=0.8235\n",
            "Epoch 077: train_loss=0.309864 train_acc=0.8585 val_loss=1.573971 val_acc=0.7647\n",
            "Epoch 078: train_loss=0.623726 train_acc=0.7492 val_loss=0.970281 val_acc=0.7647\n",
            "Epoch 079: train_loss=0.716968 train_acc=0.7363 val_loss=1.114059 val_acc=0.5294\n",
            "Epoch 080: train_loss=0.790161 train_acc=0.6688 val_loss=0.605501 val_acc=0.6471\n",
            "Epoch 081: train_loss=0.523124 train_acc=0.7814 val_loss=0.358953 val_acc=0.8235\n",
            "Epoch 082: train_loss=0.440648 train_acc=0.8006 val_loss=0.281619 val_acc=0.8529\n",
            "Epoch 083: train_loss=0.374752 train_acc=0.8360 val_loss=0.284811 val_acc=0.8529\n",
            "Epoch 084: train_loss=0.358922 train_acc=0.8553 val_loss=0.388473 val_acc=0.8235\n",
            "Epoch 085: train_loss=0.330872 train_acc=0.8521 val_loss=0.332173 val_acc=0.8529\n",
            "Epoch 086: train_loss=0.326395 train_acc=0.8489 val_loss=0.304326 val_acc=0.8235\n",
            "Epoch 087: train_loss=0.322971 train_acc=0.8489 val_loss=0.342874 val_acc=0.8529\n",
            "Epoch 088: train_loss=0.279096 train_acc=0.8714 val_loss=0.256721 val_acc=0.8235\n",
            "Epoch 089: train_loss=0.316341 train_acc=0.8489 val_loss=0.276637 val_acc=0.8235\n",
            "Epoch 090: train_loss=0.315454 train_acc=0.8457 val_loss=0.241180 val_acc=0.8529\n",
            "Epoch 091: train_loss=0.288758 train_acc=0.8392 val_loss=0.294879 val_acc=0.8529\n",
            "Epoch 092: train_loss=0.283544 train_acc=0.8617 val_loss=0.270134 val_acc=0.8529\n",
            "Epoch 093: train_loss=0.284957 train_acc=0.8489 val_loss=0.274360 val_acc=0.8529\n",
            "Epoch 094: train_loss=0.271179 train_acc=0.8489 val_loss=0.234756 val_acc=0.8529\n",
            "Epoch 095: train_loss=0.264811 train_acc=0.8746 val_loss=0.233661 val_acc=0.8529\n",
            "Epoch 096: train_loss=0.274524 train_acc=0.8457 val_loss=0.275843 val_acc=0.8529\n",
            "Epoch 097: train_loss=0.652857 train_acc=0.7717 val_loss=0.541972 val_acc=0.8235\n",
            "Epoch 098: train_loss=0.513497 train_acc=0.7878 val_loss=0.424367 val_acc=0.7941\n",
            "Epoch 099: train_loss=0.357722 train_acc=0.8328 val_loss=0.298056 val_acc=0.8235\n",
            "Epoch 100: train_loss=0.319516 train_acc=0.8553 val_loss=0.315284 val_acc=0.8824\n",
            "Epoch 101: train_loss=0.315266 train_acc=0.8521 val_loss=0.257109 val_acc=0.8529\n",
            "Epoch 102: train_loss=0.272832 train_acc=0.8778 val_loss=0.273010 val_acc=0.9118\n",
            "Epoch 103: train_loss=0.263643 train_acc=0.8585 val_loss=0.220648 val_acc=0.8529\n",
            "Epoch 104: train_loss=0.275687 train_acc=0.8553 val_loss=0.224694 val_acc=0.8529\n",
            "Epoch 105: train_loss=0.250722 train_acc=0.8810 val_loss=0.269704 val_acc=0.9118\n",
            "Epoch 106: train_loss=0.261913 train_acc=0.8682 val_loss=0.259325 val_acc=0.9118\n",
            "Epoch 107: train_loss=0.267836 train_acc=0.8875 val_loss=0.216076 val_acc=0.8529\n",
            "Epoch 108: train_loss=0.249659 train_acc=0.8714 val_loss=0.256124 val_acc=0.9118\n",
            "Epoch 109: train_loss=0.247246 train_acc=0.8746 val_loss=0.235860 val_acc=0.8824\n",
            "Epoch 110: train_loss=0.242438 train_acc=0.8746 val_loss=0.212292 val_acc=0.8529\n",
            "Epoch 111: train_loss=0.271477 train_acc=0.8553 val_loss=0.362690 val_acc=0.8824\n",
            "Epoch 112: train_loss=0.266457 train_acc=0.8778 val_loss=0.275790 val_acc=0.8824\n",
            "Epoch 113: train_loss=0.281345 train_acc=0.8714 val_loss=0.353691 val_acc=0.8235\n",
            "Epoch 114: train_loss=0.332762 train_acc=0.8617 val_loss=0.290274 val_acc=0.9118\n",
            "Epoch 115: train_loss=0.306713 train_acc=0.8553 val_loss=0.217066 val_acc=0.8824\n",
            "Epoch 116: train_loss=0.292993 train_acc=0.8585 val_loss=0.226186 val_acc=0.8824\n",
            "Epoch 117: train_loss=0.267050 train_acc=0.8617 val_loss=0.236696 val_acc=0.8235\n",
            "Epoch 118: train_loss=0.242433 train_acc=0.8746 val_loss=0.322507 val_acc=0.8824\n",
            "Epoch 119: train_loss=0.244948 train_acc=0.8714 val_loss=0.199863 val_acc=0.8529\n",
            "Epoch 120: train_loss=0.256504 train_acc=0.8650 val_loss=0.240052 val_acc=0.8235\n",
            "Epoch 121: train_loss=0.297571 train_acc=0.8585 val_loss=0.306404 val_acc=0.8529\n",
            "Epoch 122: train_loss=0.272278 train_acc=0.8714 val_loss=0.208641 val_acc=0.8824\n",
            "Epoch 123: train_loss=0.254086 train_acc=0.8714 val_loss=0.207033 val_acc=0.8529\n",
            "Epoch 124: train_loss=0.242434 train_acc=0.8971 val_loss=0.259444 val_acc=0.9118\n",
            "Epoch 125: train_loss=0.242040 train_acc=0.8810 val_loss=0.186190 val_acc=0.8824\n",
            "Epoch 126: train_loss=0.224311 train_acc=0.8842 val_loss=0.248408 val_acc=0.9118\n",
            "Epoch 127: train_loss=0.228554 train_acc=0.8875 val_loss=0.200034 val_acc=0.8824\n",
            "Epoch 128: train_loss=0.212539 train_acc=0.9068 val_loss=0.185254 val_acc=0.9118\n",
            "Epoch 129: train_loss=0.218511 train_acc=0.9003 val_loss=0.255240 val_acc=0.8824\n",
            "Epoch 130: train_loss=0.242915 train_acc=0.8875 val_loss=0.195582 val_acc=0.8529\n",
            "Epoch 131: train_loss=0.252338 train_acc=0.8875 val_loss=0.325100 val_acc=0.7941\n",
            "Epoch 132: train_loss=0.274624 train_acc=0.8617 val_loss=0.201559 val_acc=0.8824\n",
            "Epoch 133: train_loss=0.265048 train_acc=0.8424 val_loss=0.293902 val_acc=0.9118\n",
            "Epoch 134: train_loss=0.248358 train_acc=0.8650 val_loss=0.336964 val_acc=0.8824\n",
            "Epoch 135: train_loss=0.234325 train_acc=0.8842 val_loss=0.244892 val_acc=0.9118\n",
            "Epoch 136: train_loss=0.225547 train_acc=0.8875 val_loss=0.176579 val_acc=0.8824\n",
            "Epoch 137: train_loss=0.234441 train_acc=0.8746 val_loss=0.243302 val_acc=0.9118\n",
            "Epoch 138: train_loss=0.227583 train_acc=0.8778 val_loss=0.386696 val_acc=0.8235\n",
            "Epoch 139: train_loss=0.289149 train_acc=0.8553 val_loss=0.223038 val_acc=0.8529\n",
            "Epoch 140: train_loss=0.251561 train_acc=0.8746 val_loss=0.251065 val_acc=0.9118\n",
            "Epoch 141: train_loss=0.222422 train_acc=0.8842 val_loss=0.469112 val_acc=0.8235\n",
            "Epoch 142: train_loss=0.253003 train_acc=0.8778 val_loss=0.188777 val_acc=0.8824\n",
            "Epoch 143: train_loss=0.230611 train_acc=0.8907 val_loss=0.165281 val_acc=0.9412\n",
            "Epoch 144: train_loss=0.203737 train_acc=0.9035 val_loss=0.202686 val_acc=0.9118\n",
            "Epoch 145: train_loss=0.177144 train_acc=0.9293 val_loss=0.263129 val_acc=0.8824\n",
            "Epoch 146: train_loss=0.158658 train_acc=0.9293 val_loss=0.847791 val_acc=0.9412\n",
            "Epoch 147: train_loss=0.309479 train_acc=0.8553 val_loss=0.252230 val_acc=0.9118\n",
            "Epoch 148: train_loss=0.270741 train_acc=0.8778 val_loss=0.267303 val_acc=0.9118\n",
            "Epoch 149: train_loss=0.227762 train_acc=0.8939 val_loss=0.181683 val_acc=0.8529\n",
            "Epoch 150: train_loss=0.230867 train_acc=0.8907 val_loss=0.211669 val_acc=0.8824\n",
            "Epoch 151: train_loss=0.254953 train_acc=0.8907 val_loss=0.309186 val_acc=0.8824\n",
            "Epoch 152: train_loss=0.249008 train_acc=0.8810 val_loss=0.294205 val_acc=0.9412\n",
            "Epoch 153: train_loss=0.264047 train_acc=0.8682 val_loss=0.233718 val_acc=0.9118\n",
            "Epoch 154: train_loss=0.234951 train_acc=0.8842 val_loss=0.168163 val_acc=0.9118\n",
            "Epoch 155: train_loss=0.245721 train_acc=0.8778 val_loss=0.213394 val_acc=0.8529\n",
            "Epoch 156: train_loss=0.244219 train_acc=0.8714 val_loss=0.307205 val_acc=0.8824\n",
            "Epoch 157: train_loss=0.227654 train_acc=0.8778 val_loss=0.247246 val_acc=0.9118\n",
            "Epoch 158: train_loss=0.230423 train_acc=0.8714 val_loss=0.174363 val_acc=0.8529\n",
            "Epoch 159: train_loss=0.212800 train_acc=0.9035 val_loss=0.186262 val_acc=0.9118\n",
            "Epoch 160: train_loss=0.213811 train_acc=0.8907 val_loss=0.194276 val_acc=0.9118\n",
            "Epoch 161: train_loss=0.198297 train_acc=0.9132 val_loss=0.186943 val_acc=0.9118\n",
            "Epoch 162: train_loss=0.194675 train_acc=0.9035 val_loss=0.504462 val_acc=0.9118\n",
            "Epoch 163: train_loss=0.228491 train_acc=0.8907 val_loss=0.301815 val_acc=0.9118\n",
            "Epoch 164: train_loss=0.237475 train_acc=0.8810 val_loss=0.201609 val_acc=0.8529\n",
            "Epoch 165: train_loss=0.193611 train_acc=0.9132 val_loss=0.364889 val_acc=0.9118\n",
            "Epoch 166: train_loss=0.161963 train_acc=0.9325 val_loss=0.461016 val_acc=0.9118\n",
            "Epoch 167: train_loss=0.171663 train_acc=0.9293 val_loss=0.177938 val_acc=0.9118\n",
            "Epoch 168: train_loss=0.191370 train_acc=0.9196 val_loss=0.129339 val_acc=0.9118\n",
            "Epoch 169: train_loss=0.219620 train_acc=0.9035 val_loss=1.019719 val_acc=0.8235\n",
            "Epoch 170: train_loss=0.301583 train_acc=0.8489 val_loss=0.245903 val_acc=0.7941\n",
            "Epoch 171: train_loss=0.359180 train_acc=0.7749 val_loss=0.597197 val_acc=0.7647\n",
            "Epoch 172: train_loss=0.411404 train_acc=0.7974 val_loss=0.475270 val_acc=0.7647\n",
            "Epoch 173: train_loss=0.387327 train_acc=0.7910 val_loss=0.264213 val_acc=0.8235\n",
            "Epoch 174: train_loss=0.386371 train_acc=0.7653 val_loss=0.318578 val_acc=0.8529\n",
            "Epoch 175: train_loss=0.345304 train_acc=0.8232 val_loss=0.263937 val_acc=0.7941\n",
            "Epoch 176: train_loss=0.336251 train_acc=0.8489 val_loss=2.397489 val_acc=0.6471\n",
            "Epoch 177: train_loss=0.402643 train_acc=0.8424 val_loss=0.297434 val_acc=0.8824\n",
            "Epoch 178: train_loss=0.289467 train_acc=0.8585 val_loss=0.274117 val_acc=0.9118\n",
            "Epoch 179: train_loss=0.277995 train_acc=0.8875 val_loss=0.229678 val_acc=0.7941\n",
            "Epoch 180: train_loss=0.280867 train_acc=0.8553 val_loss=0.324793 val_acc=0.8824\n",
            "Epoch 181: train_loss=0.259303 train_acc=0.8778 val_loss=0.319639 val_acc=0.8824\n",
            "Epoch 182: train_loss=0.234677 train_acc=0.8682 val_loss=0.193298 val_acc=0.9118\n",
            "Epoch 183: train_loss=0.223240 train_acc=0.8810 val_loss=0.238497 val_acc=0.9118\n",
            "Epoch 184: train_loss=0.217328 train_acc=0.8810 val_loss=0.191942 val_acc=0.9118\n",
            "Epoch 185: train_loss=0.209925 train_acc=0.8810 val_loss=0.212649 val_acc=0.9118\n",
            "Epoch 186: train_loss=0.204376 train_acc=0.9068 val_loss=0.271920 val_acc=0.9118\n",
            "Epoch 187: train_loss=0.205454 train_acc=0.9003 val_loss=0.179094 val_acc=0.9118\n",
            "Epoch 188: train_loss=0.221389 train_acc=0.8971 val_loss=0.165377 val_acc=0.8529\n",
            "Epoch 189: train_loss=0.485655 train_acc=0.8039 val_loss=0.541446 val_acc=0.7647\n",
            "Epoch 190: train_loss=0.442793 train_acc=0.8103 val_loss=0.418288 val_acc=0.7059\n",
            "Epoch 191: train_loss=0.310737 train_acc=0.8521 val_loss=0.245850 val_acc=0.9118\n",
            "Epoch 192: train_loss=0.352262 train_acc=0.8135 val_loss=0.469249 val_acc=0.7647\n",
            "Epoch 193: train_loss=0.556817 train_acc=0.7781 val_loss=0.335959 val_acc=0.8235\n",
            "Epoch 194: train_loss=0.305973 train_acc=0.8714 val_loss=0.225377 val_acc=0.8529\n",
            "Epoch 195: train_loss=0.302771 train_acc=0.8617 val_loss=0.208469 val_acc=0.8529\n",
            "Epoch 196: train_loss=0.250336 train_acc=0.8778 val_loss=0.331478 val_acc=0.8824\n",
            "Epoch 197: train_loss=0.243146 train_acc=0.8682 val_loss=0.311778 val_acc=0.8824\n",
            "Epoch 198: train_loss=0.237234 train_acc=0.8714 val_loss=0.313858 val_acc=0.8824\n",
            "Epoch 199: train_loss=0.241365 train_acc=0.8617 val_loss=0.199590 val_acc=0.8824\n",
            "Train acc: 0.8971 | Test acc: 0.8800\n",
            "\n",
            "--- handwritten | LSTM | layers=2 | hidden=64 ---\n",
            "Epoch 000: train_loss=1.607656 train_acc=0.2058 val_loss=1.597825 val_acc=0.2059\n",
            "Epoch 001: train_loss=1.591830 train_acc=0.3569 val_loss=1.582965 val_acc=0.2353\n",
            "Epoch 002: train_loss=1.528466 train_acc=0.2219 val_loss=1.443894 val_acc=0.1765\n",
            "Epoch 003: train_loss=1.348807 train_acc=0.3537 val_loss=1.189062 val_acc=0.3529\n",
            "Epoch 004: train_loss=1.167491 train_acc=0.4695 val_loss=0.924061 val_acc=0.5588\n",
            "Epoch 005: train_loss=1.084965 train_acc=0.4566 val_loss=0.885615 val_acc=0.5000\n",
            "Epoch 006: train_loss=0.949209 train_acc=0.5080 val_loss=0.843303 val_acc=0.5294\n",
            "Epoch 007: train_loss=0.867141 train_acc=0.5531 val_loss=0.725093 val_acc=0.5294\n",
            "Epoch 008: train_loss=0.857302 train_acc=0.5531 val_loss=0.712470 val_acc=0.5882\n",
            "Epoch 009: train_loss=0.820815 train_acc=0.5756 val_loss=0.696475 val_acc=0.5294\n",
            "Epoch 010: train_loss=0.795976 train_acc=0.6013 val_loss=0.654707 val_acc=0.5882\n",
            "Epoch 011: train_loss=0.775829 train_acc=0.6141 val_loss=0.709213 val_acc=0.5882\n",
            "Epoch 012: train_loss=0.762304 train_acc=0.6013 val_loss=0.601923 val_acc=0.6176\n",
            "Epoch 013: train_loss=0.759760 train_acc=0.6141 val_loss=0.694092 val_acc=0.6765\n",
            "Epoch 014: train_loss=0.715313 train_acc=0.7042 val_loss=0.567023 val_acc=0.7059\n",
            "Epoch 015: train_loss=0.710553 train_acc=0.6913 val_loss=0.895659 val_acc=0.6176\n",
            "Epoch 016: train_loss=0.742547 train_acc=0.7074 val_loss=0.542275 val_acc=0.7059\n",
            "Epoch 017: train_loss=0.661041 train_acc=0.7106 val_loss=0.586667 val_acc=0.7941\n",
            "Epoch 018: train_loss=0.633668 train_acc=0.7556 val_loss=0.680717 val_acc=0.7059\n",
            "Epoch 019: train_loss=0.713212 train_acc=0.6881 val_loss=0.594766 val_acc=0.7353\n",
            "Epoch 020: train_loss=0.598973 train_acc=0.7653 val_loss=0.474904 val_acc=0.8235\n",
            "Epoch 021: train_loss=0.529381 train_acc=0.8006 val_loss=0.520635 val_acc=0.7647\n",
            "Epoch 022: train_loss=0.482037 train_acc=0.7781 val_loss=0.404321 val_acc=0.8824\n",
            "Epoch 023: train_loss=0.594202 train_acc=0.7653 val_loss=0.808977 val_acc=0.7059\n",
            "Epoch 024: train_loss=0.723159 train_acc=0.6849 val_loss=0.443283 val_acc=0.7647\n",
            "Epoch 025: train_loss=0.537220 train_acc=0.7363 val_loss=0.514306 val_acc=0.7941\n",
            "Epoch 026: train_loss=0.547849 train_acc=0.7492 val_loss=0.443977 val_acc=0.7647\n",
            "Epoch 027: train_loss=0.746017 train_acc=0.6656 val_loss=1.330058 val_acc=0.6176\n",
            "Epoch 028: train_loss=0.609539 train_acc=0.6945 val_loss=0.420270 val_acc=0.7941\n",
            "Epoch 029: train_loss=0.451781 train_acc=0.7717 val_loss=0.387144 val_acc=0.7647\n",
            "Epoch 030: train_loss=0.413458 train_acc=0.8167 val_loss=0.380866 val_acc=0.8529\n",
            "Epoch 031: train_loss=0.389626 train_acc=0.8264 val_loss=0.390802 val_acc=0.8529\n",
            "Epoch 032: train_loss=0.384127 train_acc=0.8199 val_loss=0.404444 val_acc=0.8529\n",
            "Epoch 033: train_loss=0.362883 train_acc=0.8296 val_loss=0.370622 val_acc=0.8529\n",
            "Epoch 034: train_loss=0.357695 train_acc=0.8360 val_loss=0.355333 val_acc=0.8235\n",
            "Epoch 035: train_loss=0.355551 train_acc=0.8199 val_loss=0.376010 val_acc=0.8529\n",
            "Epoch 036: train_loss=0.343112 train_acc=0.8360 val_loss=0.356362 val_acc=0.8529\n",
            "Epoch 037: train_loss=0.327957 train_acc=0.8457 val_loss=0.349234 val_acc=0.8529\n",
            "Epoch 038: train_loss=0.324215 train_acc=0.8360 val_loss=0.284233 val_acc=0.8529\n",
            "Epoch 039: train_loss=0.366924 train_acc=0.8328 val_loss=0.355682 val_acc=0.8235\n",
            "Epoch 040: train_loss=0.454027 train_acc=0.7942 val_loss=0.320846 val_acc=0.7647\n",
            "Epoch 041: train_loss=0.381136 train_acc=0.8264 val_loss=0.505715 val_acc=0.8235\n",
            "Epoch 042: train_loss=0.356853 train_acc=0.8232 val_loss=0.334681 val_acc=0.8529\n",
            "Epoch 043: train_loss=0.370753 train_acc=0.8167 val_loss=0.325504 val_acc=0.8235\n",
            "Epoch 044: train_loss=0.358028 train_acc=0.8232 val_loss=0.392266 val_acc=0.8235\n",
            "Epoch 045: train_loss=0.374970 train_acc=0.7974 val_loss=0.356041 val_acc=0.8235\n",
            "Epoch 046: train_loss=0.338231 train_acc=0.8521 val_loss=0.441990 val_acc=0.7647\n",
            "Epoch 047: train_loss=0.332589 train_acc=0.8457 val_loss=0.455852 val_acc=0.8529\n",
            "Epoch 048: train_loss=0.589185 train_acc=0.7331 val_loss=1.125228 val_acc=0.6765\n",
            "Epoch 049: train_loss=0.417776 train_acc=0.8232 val_loss=0.329859 val_acc=0.8824\n",
            "Epoch 050: train_loss=0.397677 train_acc=0.8071 val_loss=0.426465 val_acc=0.8235\n",
            "Epoch 051: train_loss=0.348026 train_acc=0.8489 val_loss=0.342687 val_acc=0.8529\n",
            "Epoch 052: train_loss=0.342411 train_acc=0.8392 val_loss=0.302836 val_acc=0.8824\n",
            "Epoch 053: train_loss=0.324323 train_acc=0.8489 val_loss=0.344801 val_acc=0.8529\n",
            "Epoch 054: train_loss=0.319290 train_acc=0.8457 val_loss=0.260418 val_acc=0.9118\n",
            "Epoch 055: train_loss=0.312224 train_acc=0.8553 val_loss=0.267598 val_acc=0.9118\n",
            "Epoch 056: train_loss=0.296660 train_acc=0.8682 val_loss=0.252276 val_acc=0.8529\n",
            "Epoch 057: train_loss=0.324346 train_acc=0.8296 val_loss=0.471411 val_acc=0.8529\n",
            "Epoch 058: train_loss=0.326907 train_acc=0.8167 val_loss=0.327775 val_acc=0.7941\n",
            "Epoch 059: train_loss=0.300407 train_acc=0.8585 val_loss=0.376136 val_acc=0.8235\n",
            "Epoch 060: train_loss=0.296647 train_acc=0.8553 val_loss=0.369398 val_acc=0.8529\n",
            "Epoch 061: train_loss=0.295265 train_acc=0.8650 val_loss=0.299412 val_acc=0.8529\n",
            "Epoch 062: train_loss=0.296143 train_acc=0.8521 val_loss=0.267556 val_acc=0.9118\n",
            "Epoch 063: train_loss=0.283722 train_acc=0.8650 val_loss=0.360290 val_acc=0.7941\n",
            "Epoch 064: train_loss=0.295771 train_acc=0.8585 val_loss=0.337823 val_acc=0.7941\n",
            "Epoch 065: train_loss=0.280316 train_acc=0.8682 val_loss=0.372981 val_acc=0.7941\n",
            "Epoch 066: train_loss=0.288885 train_acc=0.8617 val_loss=0.323613 val_acc=0.8529\n",
            "Epoch 067: train_loss=0.267458 train_acc=0.8714 val_loss=0.270766 val_acc=0.8529\n",
            "Epoch 068: train_loss=0.281133 train_acc=0.8617 val_loss=0.419951 val_acc=0.8235\n",
            "Epoch 069: train_loss=0.278565 train_acc=0.8746 val_loss=0.390276 val_acc=0.8235\n",
            "Epoch 070: train_loss=0.305511 train_acc=0.8521 val_loss=0.246968 val_acc=0.8235\n",
            "Epoch 071: train_loss=0.372024 train_acc=0.8489 val_loss=0.463184 val_acc=0.7059\n",
            "Epoch 072: train_loss=0.750606 train_acc=0.6527 val_loss=0.680537 val_acc=0.6765\n",
            "Epoch 073: train_loss=0.632860 train_acc=0.7074 val_loss=0.431846 val_acc=0.6765\n",
            "Epoch 074: train_loss=0.506080 train_acc=0.7363 val_loss=0.394107 val_acc=0.7941\n",
            "Epoch 075: train_loss=0.443763 train_acc=0.7492 val_loss=0.397789 val_acc=0.7647\n",
            "Epoch 076: train_loss=0.398922 train_acc=0.7363 val_loss=0.372824 val_acc=0.7941\n",
            "Epoch 077: train_loss=0.396120 train_acc=0.7621 val_loss=0.329219 val_acc=0.7941\n",
            "Epoch 078: train_loss=0.366684 train_acc=0.7781 val_loss=0.343112 val_acc=0.7941\n",
            "Epoch 079: train_loss=0.358844 train_acc=0.7942 val_loss=0.348102 val_acc=0.8235\n",
            "Epoch 080: train_loss=0.346200 train_acc=0.7781 val_loss=0.336998 val_acc=0.8529\n",
            "Epoch 081: train_loss=0.337942 train_acc=0.8392 val_loss=0.316135 val_acc=0.8235\n",
            "Epoch 082: train_loss=0.327147 train_acc=0.8232 val_loss=0.317839 val_acc=0.8529\n",
            "Epoch 083: train_loss=0.314203 train_acc=0.8424 val_loss=0.324757 val_acc=0.8529\n",
            "Epoch 084: train_loss=0.303546 train_acc=0.8457 val_loss=0.318932 val_acc=0.8529\n",
            "Epoch 085: train_loss=0.343646 train_acc=0.8264 val_loss=0.323131 val_acc=0.7353\n",
            "Epoch 086: train_loss=0.293004 train_acc=0.8617 val_loss=0.444263 val_acc=0.8529\n",
            "Epoch 087: train_loss=0.311131 train_acc=0.8264 val_loss=0.263986 val_acc=0.7941\n",
            "Epoch 088: train_loss=0.299309 train_acc=0.8424 val_loss=0.458468 val_acc=0.8529\n",
            "Epoch 089: train_loss=0.298808 train_acc=0.8457 val_loss=0.290799 val_acc=0.9412\n",
            "Epoch 090: train_loss=0.318649 train_acc=0.8617 val_loss=0.307492 val_acc=0.8824\n",
            "Epoch 091: train_loss=0.288273 train_acc=0.8810 val_loss=0.266808 val_acc=0.9118\n",
            "Epoch 092: train_loss=0.277066 train_acc=0.8714 val_loss=0.286462 val_acc=0.8824\n",
            "Epoch 093: train_loss=0.263487 train_acc=0.8682 val_loss=0.364511 val_acc=0.8529\n",
            "Epoch 094: train_loss=0.254824 train_acc=0.8875 val_loss=0.378946 val_acc=0.8235\n",
            "Epoch 095: train_loss=0.279169 train_acc=0.8650 val_loss=0.511857 val_acc=0.8529\n",
            "Epoch 096: train_loss=0.288073 train_acc=0.8553 val_loss=0.246185 val_acc=0.7941\n",
            "Epoch 097: train_loss=0.299641 train_acc=0.8617 val_loss=0.350801 val_acc=0.8529\n",
            "Epoch 098: train_loss=0.268990 train_acc=0.8489 val_loss=0.289210 val_acc=0.9118\n",
            "Epoch 099: train_loss=0.277645 train_acc=0.8650 val_loss=0.233810 val_acc=0.9118\n",
            "Epoch 100: train_loss=0.286869 train_acc=0.8842 val_loss=0.305049 val_acc=0.8529\n",
            "Epoch 101: train_loss=0.339568 train_acc=0.8392 val_loss=0.283249 val_acc=0.7941\n",
            "Epoch 102: train_loss=0.345077 train_acc=0.8199 val_loss=0.391519 val_acc=0.8529\n",
            "Epoch 103: train_loss=0.275347 train_acc=0.8682 val_loss=0.237965 val_acc=0.9118\n",
            "Epoch 104: train_loss=0.282038 train_acc=0.8521 val_loss=0.230645 val_acc=0.8235\n",
            "Epoch 105: train_loss=0.298337 train_acc=0.8521 val_loss=0.318551 val_acc=0.8235\n",
            "Epoch 106: train_loss=0.287313 train_acc=0.8617 val_loss=0.244667 val_acc=0.9412\n",
            "Epoch 107: train_loss=0.268888 train_acc=0.8875 val_loss=0.340072 val_acc=0.8235\n",
            "Epoch 108: train_loss=0.328052 train_acc=0.8360 val_loss=0.293943 val_acc=0.9118\n",
            "Epoch 109: train_loss=0.275909 train_acc=0.8650 val_loss=0.229818 val_acc=0.8235\n",
            "Epoch 110: train_loss=0.250620 train_acc=0.8778 val_loss=0.286244 val_acc=0.8529\n",
            "Epoch 111: train_loss=0.241376 train_acc=0.8907 val_loss=0.217433 val_acc=0.9412\n",
            "Epoch 112: train_loss=0.237197 train_acc=0.8907 val_loss=0.191206 val_acc=0.8529\n",
            "Epoch 113: train_loss=0.258071 train_acc=0.8682 val_loss=0.208710 val_acc=0.8824\n",
            "Epoch 114: train_loss=0.273570 train_acc=0.8521 val_loss=0.268719 val_acc=0.8824\n",
            "Epoch 115: train_loss=0.272426 train_acc=0.8810 val_loss=0.227614 val_acc=0.9118\n",
            "Epoch 116: train_loss=0.244553 train_acc=0.8907 val_loss=0.275218 val_acc=0.8824\n",
            "Epoch 117: train_loss=0.257366 train_acc=0.8810 val_loss=0.197679 val_acc=0.8529\n",
            "Epoch 118: train_loss=0.232921 train_acc=0.8842 val_loss=0.260141 val_acc=0.8824\n",
            "Epoch 119: train_loss=0.224004 train_acc=0.9035 val_loss=0.243968 val_acc=0.9118\n",
            "Epoch 120: train_loss=0.226274 train_acc=0.8971 val_loss=0.189620 val_acc=0.9118\n",
            "Epoch 121: train_loss=0.231071 train_acc=0.9003 val_loss=0.245860 val_acc=0.9412\n",
            "Epoch 122: train_loss=0.212541 train_acc=0.9132 val_loss=0.212050 val_acc=0.9118\n",
            "Epoch 123: train_loss=0.210814 train_acc=0.9100 val_loss=0.197298 val_acc=0.9118\n",
            "Epoch 124: train_loss=0.273430 train_acc=0.8650 val_loss=0.354440 val_acc=0.8529\n",
            "Epoch 125: train_loss=0.280328 train_acc=0.8457 val_loss=0.238206 val_acc=0.8235\n",
            "Epoch 126: train_loss=0.254775 train_acc=0.8842 val_loss=0.343816 val_acc=0.8529\n",
            "Epoch 127: train_loss=0.248110 train_acc=0.8714 val_loss=0.231351 val_acc=0.9118\n",
            "Epoch 128: train_loss=0.232487 train_acc=0.8842 val_loss=0.205259 val_acc=0.9118\n",
            "Epoch 129: train_loss=0.222694 train_acc=0.9035 val_loss=0.321875 val_acc=0.8235\n",
            "Epoch 130: train_loss=0.231454 train_acc=0.8746 val_loss=0.250287 val_acc=0.9118\n",
            "Epoch 131: train_loss=0.228634 train_acc=0.8810 val_loss=0.182538 val_acc=0.8824\n",
            "Epoch 132: train_loss=0.227634 train_acc=0.8746 val_loss=0.235243 val_acc=0.9118\n",
            "Epoch 133: train_loss=0.210097 train_acc=0.9068 val_loss=0.194740 val_acc=0.9118\n",
            "Epoch 134: train_loss=0.208067 train_acc=0.9035 val_loss=0.178595 val_acc=0.9412\n",
            "Epoch 135: train_loss=0.214794 train_acc=0.8971 val_loss=0.179017 val_acc=0.9118\n",
            "Epoch 136: train_loss=0.245214 train_acc=0.8746 val_loss=0.296793 val_acc=0.9118\n",
            "Epoch 137: train_loss=0.243607 train_acc=0.8746 val_loss=0.260846 val_acc=0.9118\n",
            "Epoch 138: train_loss=0.231829 train_acc=0.8714 val_loss=0.230259 val_acc=0.9412\n",
            "Epoch 139: train_loss=0.211649 train_acc=0.9068 val_loss=0.256136 val_acc=0.9118\n",
            "Epoch 140: train_loss=0.215077 train_acc=0.9003 val_loss=0.218021 val_acc=0.8529\n",
            "Epoch 141: train_loss=0.223287 train_acc=0.8778 val_loss=0.226489 val_acc=0.9412\n",
            "Epoch 142: train_loss=0.221190 train_acc=0.8971 val_loss=0.169635 val_acc=0.9412\n",
            "Epoch 143: train_loss=0.215731 train_acc=0.9035 val_loss=0.160613 val_acc=0.9412\n",
            "Epoch 144: train_loss=0.200751 train_acc=0.9132 val_loss=0.160721 val_acc=0.9412\n",
            "Epoch 145: train_loss=0.195407 train_acc=0.9068 val_loss=0.186901 val_acc=0.9118\n",
            "Epoch 146: train_loss=0.216238 train_acc=0.8939 val_loss=0.172687 val_acc=0.9412\n",
            "Epoch 147: train_loss=0.221839 train_acc=0.8682 val_loss=0.239763 val_acc=0.9412\n",
            "Epoch 148: train_loss=0.209835 train_acc=0.9068 val_loss=0.157399 val_acc=0.9412\n",
            "Epoch 149: train_loss=0.193125 train_acc=0.9100 val_loss=0.141092 val_acc=0.9412\n",
            "Epoch 150: train_loss=0.206795 train_acc=0.9035 val_loss=0.144074 val_acc=0.9118\n",
            "Epoch 151: train_loss=0.206083 train_acc=0.8939 val_loss=0.162731 val_acc=0.9412\n",
            "Epoch 152: train_loss=0.219576 train_acc=0.8971 val_loss=0.254486 val_acc=0.8824\n",
            "Epoch 153: train_loss=0.195606 train_acc=0.9228 val_loss=0.140197 val_acc=0.9412\n",
            "Epoch 154: train_loss=0.188360 train_acc=0.9132 val_loss=0.155253 val_acc=0.9118\n",
            "Epoch 155: train_loss=0.185528 train_acc=0.9325 val_loss=0.230064 val_acc=0.9412\n",
            "Epoch 156: train_loss=0.221169 train_acc=0.8810 val_loss=0.236777 val_acc=0.8824\n",
            "Epoch 157: train_loss=0.218880 train_acc=0.8810 val_loss=0.172306 val_acc=0.8824\n",
            "Epoch 158: train_loss=0.196097 train_acc=0.9100 val_loss=0.227491 val_acc=0.9412\n",
            "Epoch 159: train_loss=0.182422 train_acc=0.9196 val_loss=0.280979 val_acc=0.9118\n",
            "Epoch 160: train_loss=0.171734 train_acc=0.9260 val_loss=0.175245 val_acc=0.9118\n",
            "Epoch 161: train_loss=0.211022 train_acc=0.9003 val_loss=0.163819 val_acc=0.9118\n",
            "Epoch 162: train_loss=0.192181 train_acc=0.9068 val_loss=0.155764 val_acc=0.9118\n",
            "Epoch 163: train_loss=0.228677 train_acc=0.8842 val_loss=0.159390 val_acc=0.8529\n",
            "Epoch 164: train_loss=0.239722 train_acc=0.8842 val_loss=0.390057 val_acc=0.8824\n",
            "Epoch 165: train_loss=0.207562 train_acc=0.8971 val_loss=0.153812 val_acc=0.8824\n",
            "Epoch 166: train_loss=0.204029 train_acc=0.9164 val_loss=0.226917 val_acc=0.9412\n",
            "Epoch 167: train_loss=0.197133 train_acc=0.8971 val_loss=0.213186 val_acc=0.9706\n",
            "Epoch 168: train_loss=0.188814 train_acc=0.9325 val_loss=0.164803 val_acc=0.8824\n",
            "Epoch 169: train_loss=0.225629 train_acc=0.8939 val_loss=0.264369 val_acc=0.9412\n",
            "Epoch 170: train_loss=0.206810 train_acc=0.9068 val_loss=0.287613 val_acc=0.9118\n",
            "Epoch 171: train_loss=0.170597 train_acc=0.9260 val_loss=0.163807 val_acc=0.9412\n",
            "Epoch 172: train_loss=0.203979 train_acc=0.9100 val_loss=0.254257 val_acc=0.9412\n",
            "Epoch 173: train_loss=0.312797 train_acc=0.8617 val_loss=0.165827 val_acc=0.8529\n",
            "Epoch 174: train_loss=0.260135 train_acc=0.8585 val_loss=0.185996 val_acc=0.8529\n",
            "Epoch 175: train_loss=0.219212 train_acc=0.8971 val_loss=0.213120 val_acc=0.9118\n",
            "Epoch 176: train_loss=0.214789 train_acc=0.8971 val_loss=0.205908 val_acc=0.8529\n",
            "Epoch 177: train_loss=0.274722 train_acc=0.8650 val_loss=0.201984 val_acc=0.8235\n",
            "Epoch 178: train_loss=0.297520 train_acc=0.8071 val_loss=0.220535 val_acc=0.9118\n",
            "Epoch 179: train_loss=0.258190 train_acc=0.8810 val_loss=0.233520 val_acc=0.9118\n",
            "Epoch 180: train_loss=0.251578 train_acc=0.8842 val_loss=0.287403 val_acc=0.8529\n",
            "Epoch 181: train_loss=0.231821 train_acc=0.8810 val_loss=0.265787 val_acc=0.9118\n",
            "Epoch 182: train_loss=0.216849 train_acc=0.8778 val_loss=0.213649 val_acc=0.8824\n",
            "Epoch 183: train_loss=0.236035 train_acc=0.9132 val_loss=0.264929 val_acc=0.8529\n",
            "Epoch 184: train_loss=0.234685 train_acc=0.9068 val_loss=0.410620 val_acc=0.8529\n",
            "Epoch 185: train_loss=0.276841 train_acc=0.8553 val_loss=0.434664 val_acc=0.8824\n",
            "Epoch 186: train_loss=0.293991 train_acc=0.8392 val_loss=0.165604 val_acc=0.9412\n",
            "Epoch 187: train_loss=0.257526 train_acc=0.8842 val_loss=0.184518 val_acc=0.9412\n",
            "Epoch 188: train_loss=0.220403 train_acc=0.9068 val_loss=0.174396 val_acc=0.9118\n",
            "Epoch 189: train_loss=0.275296 train_acc=0.8650 val_loss=0.255087 val_acc=0.8529\n",
            "Epoch 190: train_loss=0.260908 train_acc=0.8489 val_loss=0.279070 val_acc=0.9118\n",
            "Epoch 191: train_loss=0.224058 train_acc=0.9100 val_loss=0.382000 val_acc=0.7647\n",
            "Epoch 192: train_loss=0.439923 train_acc=0.8103 val_loss=0.274386 val_acc=0.9118\n",
            "Epoch 193: train_loss=0.311477 train_acc=0.8392 val_loss=0.296610 val_acc=0.8235\n",
            "Epoch 194: train_loss=0.256285 train_acc=0.8714 val_loss=0.506864 val_acc=0.8529\n",
            "Epoch 195: train_loss=0.236739 train_acc=0.8778 val_loss=0.230807 val_acc=0.8824\n",
            "Epoch 196: train_loss=0.219034 train_acc=0.9068 val_loss=0.159632 val_acc=0.9412\n",
            "Epoch 197: train_loss=0.199463 train_acc=0.8939 val_loss=0.234389 val_acc=0.9706\n",
            "Epoch 198: train_loss=0.187106 train_acc=0.9068 val_loss=0.157460 val_acc=0.9118\n",
            "Epoch 199: train_loss=0.195905 train_acc=0.9132 val_loss=0.249804 val_acc=0.9706\n",
            "Train acc: 0.9357 | Test acc: 0.9000\n",
            "\n",
            "--- handwritten | LSTM | layers=2 | hidden=128 ---\n",
            "Epoch 000: train_loss=1.606839 train_acc=0.1672 val_loss=1.587773 val_acc=0.2059\n",
            "Epoch 001: train_loss=1.533068 train_acc=0.2122 val_loss=1.426800 val_acc=0.1765\n",
            "Epoch 002: train_loss=1.334442 train_acc=0.4405 val_loss=1.000115 val_acc=0.4412\n",
            "Epoch 003: train_loss=1.140340 train_acc=0.4695 val_loss=0.801433 val_acc=0.5588\n",
            "Epoch 004: train_loss=1.011856 train_acc=0.4887 val_loss=0.808786 val_acc=0.5294\n",
            "Epoch 005: train_loss=0.919558 train_acc=0.5113 val_loss=0.732409 val_acc=0.5588\n",
            "Epoch 006: train_loss=0.868802 train_acc=0.5305 val_loss=0.763356 val_acc=0.5294\n",
            "Epoch 007: train_loss=0.824207 train_acc=0.5659 val_loss=0.665020 val_acc=0.5294\n",
            "Epoch 008: train_loss=0.877034 train_acc=0.5241 val_loss=1.053061 val_acc=0.5000\n",
            "Epoch 009: train_loss=0.892179 train_acc=0.5595 val_loss=0.939640 val_acc=0.5000\n",
            "Epoch 010: train_loss=0.859493 train_acc=0.5498 val_loss=0.662391 val_acc=0.5588\n",
            "Epoch 011: train_loss=0.811402 train_acc=0.5305 val_loss=0.704282 val_acc=0.5294\n",
            "Epoch 012: train_loss=0.785231 train_acc=0.5627 val_loss=0.747232 val_acc=0.5588\n",
            "Epoch 013: train_loss=0.774754 train_acc=0.5820 val_loss=0.661240 val_acc=0.5588\n",
            "Epoch 014: train_loss=0.762551 train_acc=0.5723 val_loss=0.706535 val_acc=0.5882\n",
            "Epoch 015: train_loss=0.755162 train_acc=0.5531 val_loss=0.729153 val_acc=0.5882\n",
            "Epoch 016: train_loss=0.775855 train_acc=0.5916 val_loss=0.749997 val_acc=0.5000\n",
            "Epoch 017: train_loss=0.854412 train_acc=0.5531 val_loss=0.700787 val_acc=0.5294\n",
            "Epoch 018: train_loss=0.776897 train_acc=0.6013 val_loss=1.076744 val_acc=0.5588\n",
            "Epoch 019: train_loss=0.723919 train_acc=0.6334 val_loss=0.842796 val_acc=0.6176\n",
            "Epoch 020: train_loss=0.779672 train_acc=0.5916 val_loss=0.613178 val_acc=0.6176\n",
            "Epoch 021: train_loss=0.738827 train_acc=0.6270 val_loss=0.664320 val_acc=0.5882\n",
            "Epoch 022: train_loss=0.701895 train_acc=0.6463 val_loss=0.586082 val_acc=0.5882\n",
            "Epoch 023: train_loss=0.684244 train_acc=0.6527 val_loss=0.584778 val_acc=0.6765\n",
            "Epoch 024: train_loss=0.675501 train_acc=0.6399 val_loss=0.600734 val_acc=0.6176\n",
            "Epoch 025: train_loss=0.669767 train_acc=0.6463 val_loss=0.573624 val_acc=0.6471\n",
            "Epoch 026: train_loss=0.675369 train_acc=0.6334 val_loss=0.755223 val_acc=0.6471\n",
            "Epoch 027: train_loss=0.669475 train_acc=0.6656 val_loss=0.568311 val_acc=0.7059\n",
            "Epoch 028: train_loss=0.623006 train_acc=0.7010 val_loss=0.589623 val_acc=0.6765\n",
            "Epoch 029: train_loss=0.578438 train_acc=0.7331 val_loss=0.437165 val_acc=0.7941\n",
            "Epoch 030: train_loss=0.491528 train_acc=0.7846 val_loss=0.463852 val_acc=0.8235\n",
            "Epoch 031: train_loss=0.568515 train_acc=0.7556 val_loss=0.318798 val_acc=0.8529\n",
            "Epoch 032: train_loss=0.478249 train_acc=0.7524 val_loss=0.401994 val_acc=0.8824\n",
            "Epoch 033: train_loss=0.423852 train_acc=0.8103 val_loss=0.463040 val_acc=0.8824\n",
            "Epoch 034: train_loss=0.553771 train_acc=0.7717 val_loss=0.491083 val_acc=0.6176\n",
            "Epoch 035: train_loss=0.575559 train_acc=0.7395 val_loss=0.588482 val_acc=0.6765\n",
            "Epoch 036: train_loss=0.482156 train_acc=0.7717 val_loss=0.392795 val_acc=0.7647\n",
            "Epoch 037: train_loss=0.438798 train_acc=0.7717 val_loss=0.349886 val_acc=0.8235\n",
            "Epoch 038: train_loss=0.385428 train_acc=0.7942 val_loss=0.352927 val_acc=0.8529\n",
            "Epoch 039: train_loss=0.347859 train_acc=0.8199 val_loss=0.351656 val_acc=0.7941\n",
            "Epoch 040: train_loss=0.341027 train_acc=0.8553 val_loss=0.344513 val_acc=0.8235\n",
            "Epoch 041: train_loss=0.316794 train_acc=0.8553 val_loss=0.276572 val_acc=0.8824\n",
            "Epoch 042: train_loss=0.319197 train_acc=0.8457 val_loss=0.395891 val_acc=0.7647\n",
            "Epoch 043: train_loss=0.348069 train_acc=0.8167 val_loss=0.346777 val_acc=0.7941\n",
            "Epoch 044: train_loss=0.343734 train_acc=0.8328 val_loss=0.339894 val_acc=0.8824\n",
            "Epoch 045: train_loss=0.294456 train_acc=0.8650 val_loss=0.324730 val_acc=0.8529\n",
            "Epoch 046: train_loss=0.286869 train_acc=0.8489 val_loss=0.267548 val_acc=0.8824\n",
            "Epoch 047: train_loss=0.282479 train_acc=0.8392 val_loss=0.253456 val_acc=0.8824\n",
            "Epoch 048: train_loss=0.275244 train_acc=0.8457 val_loss=0.252027 val_acc=0.9118\n",
            "Epoch 049: train_loss=0.263938 train_acc=0.8617 val_loss=0.299437 val_acc=0.9118\n",
            "Epoch 050: train_loss=0.254403 train_acc=0.8746 val_loss=0.275759 val_acc=0.9118\n",
            "Epoch 051: train_loss=0.247328 train_acc=0.8778 val_loss=0.290224 val_acc=0.9118\n",
            "Epoch 052: train_loss=0.240661 train_acc=0.8842 val_loss=0.214276 val_acc=0.9118\n",
            "Epoch 053: train_loss=0.281759 train_acc=0.8714 val_loss=0.356070 val_acc=0.7059\n",
            "Epoch 054: train_loss=0.343791 train_acc=0.8424 val_loss=0.227247 val_acc=0.7941\n",
            "Epoch 055: train_loss=0.292749 train_acc=0.8199 val_loss=0.252220 val_acc=0.8824\n",
            "Epoch 056: train_loss=0.305487 train_acc=0.8328 val_loss=0.246569 val_acc=0.8824\n",
            "Epoch 057: train_loss=0.268368 train_acc=0.8746 val_loss=0.336265 val_acc=0.8529\n",
            "Epoch 058: train_loss=0.255387 train_acc=0.8875 val_loss=0.213774 val_acc=0.9118\n",
            "Epoch 059: train_loss=0.283222 train_acc=0.8585 val_loss=0.253613 val_acc=0.8235\n",
            "Epoch 060: train_loss=0.333272 train_acc=0.8489 val_loss=0.363229 val_acc=0.7647\n",
            "Epoch 061: train_loss=0.441593 train_acc=0.7974 val_loss=0.278012 val_acc=0.8824\n",
            "Epoch 062: train_loss=0.374021 train_acc=0.8135 val_loss=0.322992 val_acc=0.8235\n",
            "Epoch 063: train_loss=0.303556 train_acc=0.8521 val_loss=0.285518 val_acc=0.9118\n",
            "Epoch 064: train_loss=0.276053 train_acc=0.8617 val_loss=0.268984 val_acc=0.9118\n",
            "Epoch 065: train_loss=0.257393 train_acc=0.8714 val_loss=0.305505 val_acc=0.8824\n",
            "Epoch 066: train_loss=0.244713 train_acc=0.8939 val_loss=0.280909 val_acc=0.9118\n",
            "Epoch 067: train_loss=0.231822 train_acc=0.8907 val_loss=0.262057 val_acc=0.9412\n",
            "Epoch 068: train_loss=0.221225 train_acc=0.8875 val_loss=0.317403 val_acc=0.9118\n",
            "Epoch 069: train_loss=0.231887 train_acc=0.8971 val_loss=0.332400 val_acc=0.8824\n",
            "Epoch 070: train_loss=0.243594 train_acc=0.8907 val_loss=0.272210 val_acc=0.9118\n",
            "Epoch 071: train_loss=0.225914 train_acc=0.8810 val_loss=0.298923 val_acc=0.9118\n",
            "Epoch 072: train_loss=0.227531 train_acc=0.8842 val_loss=0.204322 val_acc=0.9118\n",
            "Epoch 073: train_loss=0.257575 train_acc=0.8489 val_loss=0.317499 val_acc=0.8824\n",
            "Epoch 074: train_loss=0.287190 train_acc=0.8650 val_loss=0.441495 val_acc=0.8529\n",
            "Epoch 075: train_loss=0.253033 train_acc=0.8489 val_loss=0.275520 val_acc=0.7941\n",
            "Epoch 076: train_loss=0.265282 train_acc=0.8650 val_loss=0.450397 val_acc=0.8824\n",
            "Epoch 077: train_loss=0.257251 train_acc=0.8617 val_loss=0.263812 val_acc=0.8824\n",
            "Epoch 078: train_loss=0.247616 train_acc=0.8746 val_loss=0.294172 val_acc=0.9118\n",
            "Epoch 079: train_loss=0.233914 train_acc=0.8907 val_loss=0.287017 val_acc=0.9118\n",
            "Epoch 080: train_loss=0.241047 train_acc=0.8810 val_loss=0.302449 val_acc=0.9118\n",
            "Epoch 081: train_loss=0.210554 train_acc=0.8907 val_loss=0.275523 val_acc=0.9412\n",
            "Epoch 082: train_loss=0.227261 train_acc=0.9003 val_loss=0.389472 val_acc=0.8529\n",
            "Epoch 083: train_loss=0.237661 train_acc=0.8842 val_loss=0.193569 val_acc=0.8824\n",
            "Epoch 084: train_loss=0.225797 train_acc=0.8842 val_loss=0.420736 val_acc=0.9118\n",
            "Epoch 085: train_loss=0.234860 train_acc=0.8939 val_loss=0.202101 val_acc=0.9118\n",
            "Epoch 086: train_loss=0.292406 train_acc=0.8232 val_loss=0.264742 val_acc=0.9118\n",
            "Epoch 087: train_loss=0.258982 train_acc=0.8650 val_loss=0.416984 val_acc=0.8824\n",
            "Epoch 088: train_loss=0.433314 train_acc=0.8232 val_loss=0.340447 val_acc=0.7647\n",
            "Epoch 089: train_loss=0.455018 train_acc=0.7588 val_loss=1.125519 val_acc=0.5882\n",
            "Epoch 090: train_loss=0.444461 train_acc=0.7717 val_loss=0.296333 val_acc=0.8235\n",
            "Epoch 091: train_loss=0.339958 train_acc=0.8135 val_loss=0.294097 val_acc=0.8824\n",
            "Epoch 092: train_loss=0.301326 train_acc=0.8457 val_loss=0.216201 val_acc=0.9412\n",
            "Epoch 093: train_loss=0.255597 train_acc=0.8553 val_loss=0.191377 val_acc=0.9412\n",
            "Epoch 094: train_loss=0.232021 train_acc=0.8778 val_loss=0.178504 val_acc=0.9412\n",
            "Epoch 095: train_loss=0.229628 train_acc=0.8810 val_loss=0.488114 val_acc=0.9118\n",
            "Epoch 096: train_loss=0.245158 train_acc=0.8682 val_loss=0.168738 val_acc=0.9118\n",
            "Epoch 097: train_loss=0.296699 train_acc=0.8746 val_loss=0.304856 val_acc=0.9118\n",
            "Epoch 098: train_loss=0.324373 train_acc=0.8232 val_loss=0.221911 val_acc=0.8824\n",
            "Epoch 099: train_loss=0.276301 train_acc=0.8585 val_loss=0.266130 val_acc=0.9118\n",
            "Epoch 100: train_loss=0.293524 train_acc=0.8553 val_loss=0.257864 val_acc=0.8824\n",
            "Epoch 101: train_loss=0.257393 train_acc=0.8746 val_loss=0.300221 val_acc=0.9118\n",
            "Epoch 102: train_loss=0.225742 train_acc=0.8939 val_loss=0.285681 val_acc=0.9118\n",
            "Converged at epoch 103 (train_loss change 7.15e-05 < 0.0001)\n",
            "Train acc: 0.8907 | Test acc: 0.8400\n",
            "\n",
            "--- handwritten | GRU | layers=1 | hidden=64 ---\n",
            "Epoch 000: train_loss=1.605709 train_acc=0.1897 val_loss=1.600321 val_acc=0.1471\n",
            "Epoch 001: train_loss=1.574323 train_acc=0.3730 val_loss=1.575187 val_acc=0.5294\n",
            "Epoch 002: train_loss=1.539035 train_acc=0.4437 val_loss=1.549552 val_acc=0.4706\n",
            "Epoch 003: train_loss=1.482610 train_acc=0.3376 val_loss=1.438022 val_acc=0.4118\n",
            "Epoch 004: train_loss=1.354324 train_acc=0.3923 val_loss=1.182890 val_acc=0.3824\n",
            "Epoch 005: train_loss=1.254357 train_acc=0.3859 val_loss=1.080503 val_acc=0.4412\n",
            "Epoch 006: train_loss=1.175467 train_acc=0.4180 val_loss=0.944463 val_acc=0.4412\n",
            "Epoch 007: train_loss=1.089627 train_acc=0.4180 val_loss=0.878116 val_acc=0.4412\n",
            "Epoch 008: train_loss=1.034604 train_acc=0.4341 val_loss=0.818077 val_acc=0.4118\n",
            "Epoch 009: train_loss=0.988458 train_acc=0.4373 val_loss=0.776271 val_acc=0.5588\n",
            "Epoch 010: train_loss=0.961283 train_acc=0.4920 val_loss=0.850642 val_acc=0.5294\n",
            "Epoch 011: train_loss=0.931718 train_acc=0.5113 val_loss=0.807178 val_acc=0.5294\n",
            "Epoch 012: train_loss=0.899282 train_acc=0.5209 val_loss=0.777060 val_acc=0.5588\n",
            "Epoch 013: train_loss=0.882740 train_acc=0.5145 val_loss=0.750936 val_acc=0.5588\n",
            "Epoch 014: train_loss=0.860665 train_acc=0.5402 val_loss=0.725536 val_acc=0.5588\n",
            "Epoch 015: train_loss=0.844477 train_acc=0.5659 val_loss=0.722910 val_acc=0.5588\n",
            "Epoch 016: train_loss=0.819301 train_acc=0.5466 val_loss=0.733609 val_acc=0.5588\n",
            "Epoch 017: train_loss=0.808371 train_acc=0.6045 val_loss=0.728120 val_acc=0.5882\n",
            "Epoch 018: train_loss=0.787894 train_acc=0.5916 val_loss=0.680919 val_acc=0.6176\n",
            "Epoch 019: train_loss=0.782783 train_acc=0.5981 val_loss=0.677879 val_acc=0.6471\n",
            "Epoch 020: train_loss=0.818409 train_acc=0.5884 val_loss=0.717895 val_acc=0.5588\n",
            "Epoch 021: train_loss=0.806563 train_acc=0.5723 val_loss=0.657156 val_acc=0.5882\n",
            "Epoch 022: train_loss=0.790173 train_acc=0.6302 val_loss=0.723653 val_acc=0.4706\n",
            "Epoch 023: train_loss=0.787568 train_acc=0.6141 val_loss=0.686529 val_acc=0.6471\n",
            "Epoch 024: train_loss=0.793528 train_acc=0.6206 val_loss=0.723496 val_acc=0.5588\n",
            "Epoch 025: train_loss=0.781134 train_acc=0.6045 val_loss=0.667848 val_acc=0.5588\n",
            "Epoch 026: train_loss=0.781866 train_acc=0.5949 val_loss=0.711480 val_acc=0.6471\n",
            "Epoch 027: train_loss=0.778923 train_acc=0.6334 val_loss=0.687806 val_acc=0.6176\n",
            "Epoch 028: train_loss=0.759797 train_acc=0.6206 val_loss=0.670057 val_acc=0.6176\n",
            "Epoch 029: train_loss=0.761152 train_acc=0.6077 val_loss=0.679467 val_acc=0.6471\n",
            "Epoch 030: train_loss=0.770273 train_acc=0.6206 val_loss=0.633034 val_acc=0.6176\n",
            "Epoch 031: train_loss=0.792504 train_acc=0.6270 val_loss=0.705598 val_acc=0.5882\n",
            "Epoch 032: train_loss=0.764803 train_acc=0.6013 val_loss=0.668127 val_acc=0.6176\n",
            "Epoch 033: train_loss=0.762205 train_acc=0.6367 val_loss=0.670627 val_acc=0.5294\n",
            "Epoch 034: train_loss=0.755244 train_acc=0.6206 val_loss=0.659179 val_acc=0.5882\n",
            "Epoch 035: train_loss=0.751067 train_acc=0.6077 val_loss=0.653183 val_acc=0.5882\n",
            "Epoch 036: train_loss=0.738957 train_acc=0.6367 val_loss=0.633519 val_acc=0.6176\n",
            "Epoch 037: train_loss=0.731832 train_acc=0.6431 val_loss=0.634365 val_acc=0.6176\n",
            "Epoch 038: train_loss=0.731212 train_acc=0.6334 val_loss=0.657574 val_acc=0.7059\n",
            "Epoch 039: train_loss=0.748417 train_acc=0.6206 val_loss=0.654332 val_acc=0.6471\n",
            "Epoch 040: train_loss=0.756756 train_acc=0.6109 val_loss=0.674202 val_acc=0.5882\n",
            "Epoch 041: train_loss=0.738247 train_acc=0.6206 val_loss=0.598335 val_acc=0.6471\n",
            "Epoch 042: train_loss=0.727633 train_acc=0.6527 val_loss=0.619170 val_acc=0.6471\n",
            "Epoch 043: train_loss=0.711176 train_acc=0.6785 val_loss=0.613504 val_acc=0.7647\n",
            "Epoch 044: train_loss=0.699295 train_acc=0.6913 val_loss=0.629025 val_acc=0.6765\n",
            "Epoch 045: train_loss=0.704238 train_acc=0.6785 val_loss=0.612603 val_acc=0.6765\n",
            "Epoch 046: train_loss=0.668922 train_acc=0.6785 val_loss=0.554018 val_acc=0.7059\n",
            "Epoch 047: train_loss=0.667051 train_acc=0.6913 val_loss=0.566623 val_acc=0.8235\n",
            "Epoch 048: train_loss=0.617201 train_acc=0.7460 val_loss=0.554295 val_acc=0.8235\n",
            "Epoch 049: train_loss=0.630066 train_acc=0.7299 val_loss=0.592364 val_acc=0.6765\n",
            "Epoch 050: train_loss=0.663252 train_acc=0.7267 val_loss=0.530275 val_acc=0.8235\n",
            "Epoch 051: train_loss=0.558494 train_acc=0.7331 val_loss=0.491812 val_acc=0.7353\n",
            "Epoch 052: train_loss=0.579291 train_acc=0.7492 val_loss=0.525342 val_acc=0.8529\n",
            "Epoch 053: train_loss=0.519418 train_acc=0.7749 val_loss=0.482103 val_acc=0.8824\n",
            "Epoch 054: train_loss=0.490991 train_acc=0.7942 val_loss=0.402316 val_acc=0.7941\n",
            "Epoch 055: train_loss=0.452804 train_acc=0.8135 val_loss=0.433602 val_acc=0.8235\n",
            "Epoch 056: train_loss=0.438641 train_acc=0.7974 val_loss=0.412652 val_acc=0.8824\n",
            "Epoch 057: train_loss=0.422794 train_acc=0.8006 val_loss=0.414574 val_acc=0.8235\n",
            "Epoch 058: train_loss=0.464369 train_acc=0.7588 val_loss=0.445767 val_acc=0.8235\n",
            "Epoch 059: train_loss=0.407288 train_acc=0.8167 val_loss=0.369197 val_acc=0.7941\n",
            "Epoch 060: train_loss=0.412813 train_acc=0.8039 val_loss=0.353067 val_acc=0.9118\n",
            "Epoch 061: train_loss=0.424838 train_acc=0.8103 val_loss=0.422082 val_acc=0.8529\n",
            "Epoch 062: train_loss=0.407553 train_acc=0.8199 val_loss=0.377447 val_acc=0.8235\n",
            "Epoch 063: train_loss=0.398601 train_acc=0.7910 val_loss=0.363108 val_acc=0.8824\n",
            "Epoch 064: train_loss=0.373254 train_acc=0.7942 val_loss=0.367482 val_acc=0.8235\n",
            "Epoch 065: train_loss=0.368799 train_acc=0.8103 val_loss=0.369307 val_acc=0.8529\n",
            "Epoch 066: train_loss=0.364819 train_acc=0.8232 val_loss=0.380740 val_acc=0.8529\n",
            "Epoch 067: train_loss=0.394318 train_acc=0.8039 val_loss=0.385264 val_acc=0.8529\n",
            "Epoch 068: train_loss=0.472458 train_acc=0.7910 val_loss=0.542048 val_acc=0.7941\n",
            "Epoch 069: train_loss=0.401982 train_acc=0.8199 val_loss=0.347596 val_acc=0.8529\n",
            "Epoch 070: train_loss=0.391166 train_acc=0.7910 val_loss=0.341192 val_acc=0.7647\n",
            "Epoch 071: train_loss=0.393152 train_acc=0.7910 val_loss=0.381814 val_acc=0.8529\n",
            "Epoch 072: train_loss=0.357907 train_acc=0.8199 val_loss=0.388321 val_acc=0.8824\n",
            "Epoch 073: train_loss=0.350528 train_acc=0.8232 val_loss=0.428503 val_acc=0.8235\n",
            "Epoch 074: train_loss=0.379288 train_acc=0.8199 val_loss=0.359376 val_acc=0.8824\n",
            "Epoch 075: train_loss=0.351715 train_acc=0.8071 val_loss=0.353534 val_acc=0.8824\n",
            "Epoch 076: train_loss=0.345327 train_acc=0.8135 val_loss=0.337756 val_acc=0.8824\n",
            "Epoch 077: train_loss=0.329648 train_acc=0.8296 val_loss=0.352614 val_acc=0.8824\n",
            "Epoch 078: train_loss=0.327522 train_acc=0.8424 val_loss=0.347750 val_acc=0.8824\n",
            "Epoch 079: train_loss=0.318745 train_acc=0.8296 val_loss=0.355617 val_acc=0.8824\n",
            "Epoch 080: train_loss=0.344784 train_acc=0.8167 val_loss=0.333277 val_acc=0.8235\n",
            "Epoch 081: train_loss=0.316414 train_acc=0.8328 val_loss=0.356672 val_acc=0.9118\n",
            "Epoch 082: train_loss=0.329670 train_acc=0.8328 val_loss=0.382408 val_acc=0.8529\n",
            "Epoch 083: train_loss=0.319925 train_acc=0.8199 val_loss=0.423812 val_acc=0.7941\n",
            "Epoch 084: train_loss=0.335845 train_acc=0.8232 val_loss=0.335878 val_acc=0.8824\n",
            "Epoch 085: train_loss=0.318781 train_acc=0.8457 val_loss=0.350555 val_acc=0.8824\n",
            "Epoch 086: train_loss=0.302321 train_acc=0.8457 val_loss=0.371424 val_acc=0.8235\n",
            "Epoch 087: train_loss=0.302604 train_acc=0.8489 val_loss=0.335789 val_acc=0.9118\n",
            "Epoch 088: train_loss=0.307250 train_acc=0.8360 val_loss=0.350641 val_acc=0.8824\n",
            "Epoch 089: train_loss=0.293988 train_acc=0.8360 val_loss=0.324345 val_acc=0.8529\n",
            "Epoch 090: train_loss=0.297348 train_acc=0.8392 val_loss=0.345379 val_acc=0.8529\n",
            "Epoch 091: train_loss=0.282008 train_acc=0.8392 val_loss=0.335396 val_acc=0.8529\n",
            "Epoch 092: train_loss=0.275900 train_acc=0.8489 val_loss=0.338253 val_acc=0.8824\n",
            "Epoch 093: train_loss=0.280725 train_acc=0.8585 val_loss=0.323156 val_acc=0.8529\n",
            "Epoch 094: train_loss=0.279222 train_acc=0.8489 val_loss=0.375261 val_acc=0.8824\n",
            "Epoch 095: train_loss=0.271267 train_acc=0.8489 val_loss=0.385239 val_acc=0.8529\n",
            "Epoch 096: train_loss=0.255667 train_acc=0.8650 val_loss=0.323932 val_acc=0.8529\n",
            "Epoch 097: train_loss=0.266808 train_acc=0.8842 val_loss=0.353602 val_acc=0.8235\n",
            "Epoch 098: train_loss=0.257235 train_acc=0.8650 val_loss=0.407113 val_acc=0.8529\n",
            "Epoch 099: train_loss=0.320378 train_acc=0.8682 val_loss=0.358421 val_acc=0.8529\n",
            "Epoch 100: train_loss=0.416816 train_acc=0.8071 val_loss=0.288748 val_acc=0.9118\n",
            "Epoch 101: train_loss=0.334610 train_acc=0.8167 val_loss=0.310344 val_acc=0.8824\n",
            "Epoch 102: train_loss=0.316052 train_acc=0.8296 val_loss=0.350829 val_acc=0.9118\n",
            "Epoch 103: train_loss=0.318443 train_acc=0.8521 val_loss=0.340457 val_acc=0.8529\n",
            "Epoch 104: train_loss=0.291738 train_acc=0.8746 val_loss=0.354083 val_acc=0.8235\n",
            "Epoch 105: train_loss=0.247594 train_acc=0.8746 val_loss=0.343363 val_acc=0.8824\n",
            "Epoch 106: train_loss=0.241785 train_acc=0.9068 val_loss=0.361807 val_acc=0.8824\n",
            "Epoch 107: train_loss=0.234513 train_acc=0.8971 val_loss=0.338906 val_acc=0.8824\n",
            "Epoch 108: train_loss=0.223453 train_acc=0.9164 val_loss=0.344389 val_acc=0.8824\n",
            "Epoch 109: train_loss=0.220697 train_acc=0.9132 val_loss=0.327750 val_acc=0.8824\n",
            "Epoch 110: train_loss=0.223286 train_acc=0.9132 val_loss=0.336899 val_acc=0.8824\n",
            "Epoch 111: train_loss=0.219960 train_acc=0.9003 val_loss=0.277772 val_acc=0.8529\n",
            "Epoch 112: train_loss=0.223828 train_acc=0.8939 val_loss=0.337570 val_acc=0.9412\n",
            "Epoch 113: train_loss=0.201451 train_acc=0.9164 val_loss=0.380516 val_acc=0.9118\n",
            "Epoch 114: train_loss=0.199616 train_acc=0.9068 val_loss=0.369399 val_acc=0.9412\n",
            "Epoch 115: train_loss=0.198636 train_acc=0.9068 val_loss=0.359559 val_acc=0.9118\n",
            "Epoch 116: train_loss=0.196542 train_acc=0.9228 val_loss=0.295016 val_acc=0.9118\n",
            "Epoch 117: train_loss=0.183368 train_acc=0.9132 val_loss=0.273667 val_acc=0.9412\n",
            "Epoch 118: train_loss=0.192709 train_acc=0.9228 val_loss=0.585719 val_acc=0.8824\n",
            "Epoch 119: train_loss=0.189746 train_acc=0.9196 val_loss=0.228601 val_acc=0.9118\n",
            "Epoch 120: train_loss=0.187516 train_acc=0.9068 val_loss=0.301835 val_acc=0.9118\n",
            "Epoch 121: train_loss=0.216632 train_acc=0.8875 val_loss=0.350719 val_acc=0.9118\n",
            "Epoch 122: train_loss=0.190775 train_acc=0.9196 val_loss=0.419118 val_acc=0.9118\n",
            "Epoch 123: train_loss=0.163097 train_acc=0.9293 val_loss=0.435599 val_acc=0.9118\n",
            "Epoch 124: train_loss=0.166063 train_acc=0.9357 val_loss=0.365836 val_acc=0.8824\n",
            "Epoch 125: train_loss=0.161654 train_acc=0.9196 val_loss=0.414672 val_acc=0.9118\n",
            "Epoch 126: train_loss=0.156284 train_acc=0.9486 val_loss=0.640544 val_acc=0.8824\n",
            "Epoch 127: train_loss=0.182604 train_acc=0.9035 val_loss=0.476008 val_acc=0.9412\n",
            "Epoch 128: train_loss=0.131619 train_acc=0.9486 val_loss=0.196417 val_acc=0.9412\n",
            "Epoch 129: train_loss=0.142484 train_acc=0.9486 val_loss=0.272432 val_acc=0.9118\n",
            "Epoch 130: train_loss=0.135388 train_acc=0.9614 val_loss=0.414871 val_acc=0.9118\n",
            "Epoch 131: train_loss=0.133054 train_acc=0.9421 val_loss=0.592833 val_acc=0.9412\n",
            "Epoch 132: train_loss=0.143277 train_acc=0.9486 val_loss=0.458142 val_acc=0.9118\n",
            "Epoch 133: train_loss=0.115307 train_acc=0.9614 val_loss=0.304768 val_acc=0.9412\n",
            "Epoch 134: train_loss=0.103470 train_acc=0.9678 val_loss=0.381240 val_acc=0.9412\n",
            "Epoch 135: train_loss=0.104630 train_acc=0.9614 val_loss=0.360148 val_acc=0.9118\n",
            "Epoch 136: train_loss=0.117556 train_acc=0.9582 val_loss=0.127697 val_acc=0.9118\n",
            "Epoch 137: train_loss=0.253133 train_acc=0.9164 val_loss=0.283202 val_acc=0.9706\n",
            "Epoch 138: train_loss=0.278782 train_acc=0.9196 val_loss=0.353035 val_acc=0.9706\n",
            "Epoch 139: train_loss=0.153451 train_acc=0.9293 val_loss=0.160697 val_acc=0.9412\n",
            "Epoch 140: train_loss=0.137686 train_acc=0.9582 val_loss=0.114564 val_acc=0.9706\n",
            "Epoch 141: train_loss=0.107536 train_acc=0.9678 val_loss=0.214541 val_acc=0.9706\n",
            "Epoch 142: train_loss=0.114527 train_acc=0.9614 val_loss=0.122728 val_acc=0.9706\n",
            "Epoch 143: train_loss=0.089038 train_acc=0.9711 val_loss=0.476145 val_acc=0.9412\n",
            "Epoch 144: train_loss=0.085239 train_acc=0.9582 val_loss=0.445607 val_acc=0.9706\n",
            "Epoch 145: train_loss=0.096862 train_acc=0.9711 val_loss=0.400831 val_acc=0.9118\n",
            "Epoch 146: train_loss=0.086725 train_acc=0.9646 val_loss=0.417225 val_acc=0.9706\n",
            "Epoch 147: train_loss=0.116931 train_acc=0.9518 val_loss=0.076940 val_acc=0.9412\n",
            "Epoch 148: train_loss=0.210203 train_acc=0.9196 val_loss=0.344780 val_acc=0.9706\n",
            "Epoch 149: train_loss=0.122511 train_acc=0.9421 val_loss=0.451161 val_acc=0.9706\n",
            "Epoch 150: train_loss=0.103962 train_acc=0.9486 val_loss=0.321476 val_acc=0.9706\n",
            "Epoch 151: train_loss=0.085098 train_acc=0.9711 val_loss=0.472463 val_acc=0.9706\n",
            "Epoch 152: train_loss=0.073899 train_acc=0.9743 val_loss=0.478006 val_acc=0.9706\n",
            "Epoch 153: train_loss=0.071874 train_acc=0.9807 val_loss=0.095393 val_acc=1.0000\n",
            "Epoch 154: train_loss=0.063883 train_acc=0.9775 val_loss=0.143452 val_acc=1.0000\n",
            "Epoch 155: train_loss=0.074748 train_acc=0.9678 val_loss=0.077387 val_acc=1.0000\n",
            "Epoch 156: train_loss=0.049805 train_acc=0.9807 val_loss=0.087056 val_acc=1.0000\n",
            "Epoch 157: train_loss=0.055780 train_acc=0.9807 val_loss=0.101030 val_acc=1.0000\n",
            "Converged at epoch 158 (train_loss change 7.42e-06 < 0.0001)\n",
            "Train acc: 0.9711 | Test acc: 0.9700\n",
            "\n",
            "--- handwritten | GRU | layers=1 | hidden=128 ---\n",
            "Epoch 000: train_loss=1.600425 train_acc=0.1736 val_loss=1.585333 val_acc=0.2059\n",
            "Epoch 001: train_loss=1.559548 train_acc=0.2379 val_loss=1.552115 val_acc=0.2941\n",
            "Epoch 002: train_loss=1.438437 train_acc=0.3183 val_loss=1.304561 val_acc=0.4412\n",
            "Epoch 003: train_loss=1.246556 train_acc=0.5370 val_loss=0.973207 val_acc=0.4412\n",
            "Epoch 004: train_loss=1.080449 train_acc=0.4437 val_loss=0.775010 val_acc=0.5882\n",
            "Epoch 005: train_loss=0.958857 train_acc=0.5048 val_loss=0.756220 val_acc=0.5588\n",
            "Epoch 006: train_loss=0.911069 train_acc=0.5241 val_loss=0.792000 val_acc=0.5000\n",
            "Epoch 007: train_loss=0.876641 train_acc=0.5466 val_loss=0.770114 val_acc=0.5294\n",
            "Epoch 008: train_loss=0.840774 train_acc=0.5563 val_loss=0.656115 val_acc=0.5588\n",
            "Epoch 009: train_loss=0.830541 train_acc=0.5209 val_loss=0.730070 val_acc=0.5294\n",
            "Epoch 010: train_loss=0.807303 train_acc=0.5145 val_loss=0.659329 val_acc=0.5882\n",
            "Epoch 011: train_loss=0.796013 train_acc=0.5949 val_loss=0.693358 val_acc=0.5882\n",
            "Epoch 012: train_loss=0.775423 train_acc=0.5531 val_loss=0.668328 val_acc=0.6176\n",
            "Epoch 013: train_loss=0.805359 train_acc=0.5691 val_loss=0.653053 val_acc=0.5588\n",
            "Epoch 014: train_loss=0.820562 train_acc=0.5659 val_loss=0.699661 val_acc=0.5588\n",
            "Epoch 015: train_loss=0.783856 train_acc=0.5723 val_loss=0.672772 val_acc=0.5588\n",
            "Epoch 016: train_loss=0.776895 train_acc=0.5756 val_loss=0.645231 val_acc=0.5882\n",
            "Epoch 017: train_loss=0.754651 train_acc=0.5852 val_loss=0.655611 val_acc=0.5294\n",
            "Epoch 018: train_loss=0.756069 train_acc=0.6270 val_loss=0.618401 val_acc=0.5588\n",
            "Epoch 019: train_loss=0.737696 train_acc=0.5981 val_loss=0.620109 val_acc=0.5882\n",
            "Epoch 020: train_loss=0.741108 train_acc=0.6463 val_loss=0.658397 val_acc=0.6176\n",
            "Epoch 021: train_loss=0.738529 train_acc=0.6495 val_loss=0.705945 val_acc=0.6176\n",
            "Epoch 022: train_loss=0.723396 train_acc=0.6785 val_loss=0.577220 val_acc=0.5882\n",
            "Epoch 023: train_loss=0.713645 train_acc=0.6399 val_loss=0.726131 val_acc=0.6176\n",
            "Epoch 024: train_loss=0.713132 train_acc=0.6270 val_loss=0.656713 val_acc=0.7941\n",
            "Epoch 025: train_loss=0.709562 train_acc=0.6817 val_loss=0.691988 val_acc=0.5882\n",
            "Epoch 026: train_loss=0.680673 train_acc=0.7010 val_loss=0.550006 val_acc=0.7059\n",
            "Epoch 027: train_loss=0.676900 train_acc=0.6559 val_loss=0.593100 val_acc=0.7647\n",
            "Epoch 028: train_loss=0.618487 train_acc=0.7492 val_loss=0.495211 val_acc=0.7647\n",
            "Epoch 029: train_loss=0.644321 train_acc=0.6977 val_loss=1.045964 val_acc=0.5882\n",
            "Epoch 030: train_loss=0.667507 train_acc=0.6624 val_loss=0.547108 val_acc=0.6471\n",
            "Epoch 031: train_loss=0.674266 train_acc=0.6849 val_loss=0.618640 val_acc=0.6471\n",
            "Epoch 032: train_loss=0.606193 train_acc=0.7524 val_loss=0.542950 val_acc=0.6765\n",
            "Epoch 033: train_loss=0.519709 train_acc=0.7749 val_loss=0.435124 val_acc=0.7353\n",
            "Epoch 034: train_loss=0.480932 train_acc=0.7621 val_loss=0.491378 val_acc=0.7941\n",
            "Epoch 035: train_loss=0.439496 train_acc=0.7749 val_loss=0.426316 val_acc=0.7941\n",
            "Epoch 036: train_loss=0.444505 train_acc=0.7942 val_loss=0.496363 val_acc=0.7353\n",
            "Epoch 037: train_loss=0.431838 train_acc=0.7878 val_loss=0.421696 val_acc=0.8529\n",
            "Epoch 038: train_loss=0.379344 train_acc=0.7974 val_loss=0.390487 val_acc=0.8235\n",
            "Epoch 039: train_loss=0.347880 train_acc=0.8167 val_loss=0.396356 val_acc=0.8824\n",
            "Epoch 040: train_loss=0.340622 train_acc=0.8264 val_loss=0.319275 val_acc=0.8529\n",
            "Epoch 041: train_loss=0.334884 train_acc=0.8232 val_loss=0.369307 val_acc=0.8235\n",
            "Epoch 042: train_loss=0.335322 train_acc=0.8167 val_loss=0.352786 val_acc=0.8529\n",
            "Epoch 043: train_loss=0.457752 train_acc=0.7878 val_loss=1.605470 val_acc=0.6471\n",
            "Converged at epoch 44 (train_loss change 3.76e-05 < 0.0001)\n",
            "Train acc: 0.7814 | Test acc: 0.8400\n",
            "\n",
            "--- handwritten | GRU | layers=2 | hidden=64 ---\n",
            "Epoch 000: train_loss=1.597412 train_acc=0.3119 val_loss=1.548718 val_acc=0.2647\n",
            "Epoch 001: train_loss=1.553725 train_acc=0.2315 val_loss=1.530966 val_acc=0.2059\n",
            "Epoch 002: train_loss=1.453367 train_acc=0.3826 val_loss=1.341209 val_acc=0.5000\n",
            "Epoch 003: train_loss=1.180927 train_acc=0.5016 val_loss=0.886957 val_acc=0.4412\n",
            "Epoch 004: train_loss=1.071252 train_acc=0.4373 val_loss=0.812066 val_acc=0.4706\n",
            "Epoch 005: train_loss=0.976040 train_acc=0.4791 val_loss=0.870817 val_acc=0.5000\n",
            "Epoch 006: train_loss=0.924214 train_acc=0.5338 val_loss=0.806010 val_acc=0.5882\n",
            "Epoch 007: train_loss=0.889112 train_acc=0.4920 val_loss=0.784882 val_acc=0.4706\n",
            "Epoch 008: train_loss=0.846166 train_acc=0.5531 val_loss=0.715268 val_acc=0.5588\n",
            "Epoch 009: train_loss=0.814531 train_acc=0.5370 val_loss=0.737778 val_acc=0.5294\n",
            "Epoch 010: train_loss=0.793832 train_acc=0.5627 val_loss=0.680954 val_acc=0.5000\n",
            "Converged at epoch 11 (train_loss change 6.99e-05 < 0.0001)\n",
            "Train acc: 0.6077 | Test acc: 0.6300\n",
            "\n",
            "--- handwritten | GRU | layers=2 | hidden=128 ---\n",
            "Epoch 000: train_loss=1.590923 train_acc=0.2026 val_loss=1.545134 val_acc=0.2059\n",
            "Epoch 001: train_loss=1.466162 train_acc=0.2830 val_loss=1.247741 val_acc=0.5588\n",
            "Epoch 002: train_loss=1.118226 train_acc=0.4695 val_loss=0.765444 val_acc=0.4412\n",
            "Epoch 003: train_loss=0.950107 train_acc=0.5080 val_loss=0.766008 val_acc=0.5294\n",
            "Epoch 004: train_loss=0.875782 train_acc=0.5627 val_loss=0.703326 val_acc=0.5588\n",
            "Epoch 005: train_loss=0.803967 train_acc=0.5177 val_loss=0.670709 val_acc=0.5882\n",
            "Epoch 006: train_loss=0.790292 train_acc=0.5498 val_loss=0.676096 val_acc=0.4118\n",
            "Epoch 007: train_loss=0.818882 train_acc=0.5016 val_loss=0.692051 val_acc=0.5588\n",
            "Epoch 008: train_loss=0.788597 train_acc=0.5531 val_loss=0.662007 val_acc=0.5588\n",
            "Epoch 009: train_loss=0.768563 train_acc=0.6302 val_loss=0.695355 val_acc=0.5882\n",
            "Epoch 010: train_loss=0.737351 train_acc=0.6302 val_loss=0.688027 val_acc=0.5588\n",
            "Epoch 011: train_loss=0.743045 train_acc=0.6045 val_loss=0.625379 val_acc=0.6471\n",
            "Epoch 012: train_loss=0.703905 train_acc=0.6559 val_loss=0.649212 val_acc=0.6176\n",
            "Epoch 013: train_loss=0.697373 train_acc=0.6624 val_loss=0.641867 val_acc=0.6471\n",
            "Epoch 014: train_loss=0.678643 train_acc=0.6752 val_loss=0.569099 val_acc=0.7353\n",
            "Epoch 015: train_loss=0.655404 train_acc=0.6913 val_loss=0.545904 val_acc=0.7647\n",
            "Epoch 016: train_loss=0.589804 train_acc=0.7170 val_loss=0.458882 val_acc=0.8235\n",
            "Epoch 017: train_loss=0.561476 train_acc=0.7267 val_loss=0.463320 val_acc=0.6765\n",
            "Epoch 018: train_loss=0.504374 train_acc=0.7588 val_loss=0.370928 val_acc=0.7647\n",
            "Epoch 019: train_loss=0.467535 train_acc=0.7653 val_loss=0.387288 val_acc=0.7647\n",
            "Epoch 020: train_loss=0.463713 train_acc=0.7395 val_loss=0.379042 val_acc=0.7941\n",
            "Epoch 021: train_loss=0.476481 train_acc=0.7685 val_loss=0.431314 val_acc=0.7353\n",
            "Epoch 022: train_loss=0.463003 train_acc=0.7524 val_loss=0.437250 val_acc=0.7059\n",
            "Epoch 023: train_loss=0.421971 train_acc=0.7685 val_loss=0.384762 val_acc=0.8824\n",
            "Epoch 024: train_loss=0.418644 train_acc=0.7814 val_loss=0.349650 val_acc=0.8235\n",
            "Epoch 025: train_loss=0.409577 train_acc=0.7846 val_loss=0.402171 val_acc=0.7647\n",
            "Epoch 026: train_loss=0.379884 train_acc=0.8103 val_loss=0.340584 val_acc=0.8235\n",
            "Epoch 027: train_loss=0.386739 train_acc=0.8006 val_loss=0.325252 val_acc=0.7941\n",
            "Epoch 028: train_loss=0.333479 train_acc=0.8264 val_loss=0.337746 val_acc=0.7647\n",
            "Epoch 029: train_loss=0.337791 train_acc=0.8167 val_loss=0.338642 val_acc=0.8235\n",
            "Epoch 030: train_loss=0.341575 train_acc=0.8071 val_loss=0.367412 val_acc=0.8235\n",
            "Epoch 031: train_loss=0.326865 train_acc=0.8006 val_loss=0.371551 val_acc=0.7059\n",
            "Epoch 032: train_loss=0.346847 train_acc=0.8360 val_loss=0.325346 val_acc=0.8529\n",
            "Epoch 033: train_loss=0.304091 train_acc=0.8424 val_loss=0.328330 val_acc=0.7941\n",
            "Epoch 034: train_loss=0.303427 train_acc=0.8650 val_loss=0.292407 val_acc=0.7647\n",
            "Epoch 035: train_loss=0.331853 train_acc=0.7685 val_loss=0.320783 val_acc=0.8529\n",
            "Epoch 036: train_loss=0.315342 train_acc=0.8135 val_loss=0.297831 val_acc=0.8235\n",
            "Epoch 037: train_loss=0.291201 train_acc=0.8489 val_loss=0.292797 val_acc=0.8529\n",
            "Epoch 038: train_loss=0.276423 train_acc=0.8714 val_loss=0.282721 val_acc=0.8529\n",
            "Epoch 039: train_loss=0.272105 train_acc=0.8650 val_loss=0.305133 val_acc=0.8824\n",
            "Epoch 040: train_loss=0.285143 train_acc=0.8746 val_loss=0.254635 val_acc=0.7941\n",
            "Epoch 041: train_loss=0.275536 train_acc=0.8682 val_loss=0.294517 val_acc=0.8824\n",
            "Epoch 042: train_loss=0.277722 train_acc=0.8650 val_loss=0.277369 val_acc=0.8529\n",
            "Epoch 043: train_loss=0.254045 train_acc=0.8650 val_loss=0.279715 val_acc=0.8235\n",
            "Epoch 044: train_loss=0.249094 train_acc=0.8650 val_loss=0.256795 val_acc=0.8235\n",
            "Epoch 045: train_loss=0.261787 train_acc=0.8778 val_loss=0.247993 val_acc=0.8529\n",
            "Epoch 046: train_loss=0.270817 train_acc=0.8489 val_loss=0.285017 val_acc=0.8824\n",
            "Epoch 047: train_loss=0.259105 train_acc=0.8682 val_loss=0.263481 val_acc=0.7941\n",
            "Epoch 048: train_loss=0.263116 train_acc=0.8714 val_loss=0.284938 val_acc=0.8529\n",
            "Epoch 049: train_loss=0.265234 train_acc=0.8810 val_loss=0.256108 val_acc=0.7353\n",
            "Epoch 050: train_loss=0.307833 train_acc=0.8199 val_loss=0.276043 val_acc=0.8824\n",
            "Epoch 051: train_loss=0.250858 train_acc=0.8650 val_loss=0.325717 val_acc=0.8235\n",
            "Epoch 052: train_loss=0.247331 train_acc=0.8585 val_loss=0.271843 val_acc=0.7647\n",
            "Epoch 053: train_loss=0.237281 train_acc=0.8682 val_loss=0.274440 val_acc=0.8824\n",
            "Epoch 054: train_loss=0.238157 train_acc=0.8875 val_loss=0.278839 val_acc=0.8529\n",
            "Epoch 055: train_loss=0.235007 train_acc=0.8842 val_loss=0.258943 val_acc=0.8529\n",
            "Epoch 056: train_loss=0.246778 train_acc=0.8746 val_loss=0.265251 val_acc=0.8824\n",
            "Epoch 057: train_loss=0.240349 train_acc=0.8907 val_loss=0.236306 val_acc=0.8824\n",
            "Epoch 058: train_loss=0.254449 train_acc=0.8778 val_loss=0.214954 val_acc=0.8824\n",
            "Epoch 059: train_loss=0.286678 train_acc=0.8714 val_loss=0.216950 val_acc=0.7941\n",
            "Epoch 060: train_loss=0.239609 train_acc=0.8778 val_loss=0.238735 val_acc=0.8529\n",
            "Epoch 061: train_loss=0.228797 train_acc=0.8875 val_loss=0.213918 val_acc=0.8529\n",
            "Epoch 062: train_loss=0.231049 train_acc=0.9003 val_loss=0.404633 val_acc=0.8529\n",
            "Epoch 063: train_loss=0.288365 train_acc=0.8296 val_loss=0.211876 val_acc=0.8235\n",
            "Epoch 064: train_loss=0.237527 train_acc=0.8778 val_loss=0.270052 val_acc=0.8824\n",
            "Epoch 065: train_loss=0.286230 train_acc=0.8489 val_loss=0.337300 val_acc=0.8529\n",
            "Epoch 066: train_loss=0.255417 train_acc=0.8650 val_loss=0.254893 val_acc=0.8235\n",
            "Epoch 067: train_loss=0.259039 train_acc=0.8714 val_loss=0.285545 val_acc=0.8824\n",
            "Epoch 068: train_loss=0.253581 train_acc=0.8650 val_loss=0.270223 val_acc=0.8824\n",
            "Epoch 069: train_loss=0.240454 train_acc=0.8907 val_loss=0.233480 val_acc=0.8529\n",
            "Epoch 070: train_loss=0.234565 train_acc=0.8842 val_loss=0.265976 val_acc=0.8824\n",
            "Epoch 071: train_loss=0.246954 train_acc=0.8810 val_loss=0.206015 val_acc=0.8824\n",
            "Epoch 072: train_loss=0.237908 train_acc=0.8746 val_loss=0.249732 val_acc=0.8824\n",
            "Epoch 073: train_loss=0.227732 train_acc=0.8746 val_loss=0.243128 val_acc=0.9118\n",
            "Epoch 074: train_loss=0.216335 train_acc=0.8971 val_loss=0.323128 val_acc=0.8529\n",
            "Epoch 075: train_loss=0.223617 train_acc=0.9035 val_loss=0.248419 val_acc=0.8824\n",
            "Epoch 076: train_loss=0.198623 train_acc=0.9132 val_loss=0.186603 val_acc=0.8824\n",
            "Epoch 077: train_loss=0.182443 train_acc=0.9421 val_loss=0.185614 val_acc=0.8235\n",
            "Epoch 078: train_loss=0.235691 train_acc=0.8907 val_loss=0.351292 val_acc=0.9118\n",
            "Epoch 079: train_loss=0.178850 train_acc=0.9293 val_loss=0.330199 val_acc=0.9118\n",
            "Epoch 080: train_loss=0.176706 train_acc=0.9325 val_loss=0.132483 val_acc=0.8824\n",
            "Epoch 081: train_loss=0.152687 train_acc=0.9260 val_loss=0.193972 val_acc=0.9118\n",
            "Epoch 082: train_loss=0.149236 train_acc=0.9421 val_loss=0.481965 val_acc=0.9118\n",
            "Epoch 083: train_loss=0.156884 train_acc=0.9325 val_loss=0.141703 val_acc=0.9118\n",
            "Epoch 084: train_loss=0.178773 train_acc=0.9228 val_loss=0.213802 val_acc=0.8529\n",
            "Epoch 085: train_loss=0.145802 train_acc=0.9421 val_loss=0.617296 val_acc=0.9118\n",
            "Epoch 086: train_loss=0.170674 train_acc=0.9293 val_loss=0.383336 val_acc=0.9118\n",
            "Epoch 087: train_loss=0.183892 train_acc=0.9164 val_loss=0.110381 val_acc=0.9118\n",
            "Epoch 088: train_loss=0.109341 train_acc=0.9678 val_loss=0.118331 val_acc=0.9412\n",
            "Epoch 089: train_loss=0.105333 train_acc=0.9614 val_loss=0.083223 val_acc=0.9118\n",
            "Epoch 090: train_loss=0.132192 train_acc=0.9550 val_loss=0.789447 val_acc=0.8529\n",
            "Epoch 091: train_loss=0.137862 train_acc=0.9421 val_loss=0.341787 val_acc=0.9118\n",
            "Epoch 092: train_loss=0.106036 train_acc=0.9550 val_loss=0.096904 val_acc=0.9412\n",
            "Epoch 093: train_loss=0.072289 train_acc=0.9807 val_loss=0.428746 val_acc=0.9118\n",
            "Epoch 094: train_loss=0.059421 train_acc=0.9807 val_loss=0.381501 val_acc=0.9412\n",
            "Epoch 095: train_loss=0.072970 train_acc=0.9678 val_loss=0.389381 val_acc=0.9118\n",
            "Epoch 096: train_loss=0.092796 train_acc=0.9646 val_loss=0.649205 val_acc=0.9118\n",
            "Epoch 097: train_loss=0.089413 train_acc=0.9614 val_loss=1.055330 val_acc=0.8824\n",
            "Epoch 098: train_loss=0.111087 train_acc=0.9550 val_loss=0.492007 val_acc=0.9412\n",
            "Epoch 099: train_loss=0.051634 train_acc=0.9775 val_loss=0.107539 val_acc=0.9412\n",
            "Epoch 100: train_loss=0.093332 train_acc=0.9711 val_loss=0.201503 val_acc=0.9412\n",
            "Epoch 101: train_loss=0.034501 train_acc=0.9936 val_loss=0.106660 val_acc=0.9706\n",
            "Epoch 102: train_loss=0.048623 train_acc=0.9904 val_loss=0.107917 val_acc=0.8824\n",
            "Epoch 103: train_loss=0.078062 train_acc=0.9678 val_loss=0.033153 val_acc=0.9706\n",
            "Epoch 104: train_loss=0.059171 train_acc=0.9807 val_loss=0.040317 val_acc=0.9706\n",
            "Epoch 105: train_loss=0.051489 train_acc=0.9904 val_loss=0.056279 val_acc=0.9706\n",
            "Epoch 106: train_loss=0.077022 train_acc=0.9711 val_loss=0.768955 val_acc=0.8824\n",
            "Epoch 107: train_loss=0.042380 train_acc=0.9871 val_loss=0.079029 val_acc=0.9412\n",
            "Epoch 108: train_loss=0.047215 train_acc=0.9839 val_loss=0.070621 val_acc=0.9706\n",
            "Epoch 109: train_loss=0.018145 train_acc=0.9968 val_loss=0.112612 val_acc=0.9412\n",
            "Epoch 110: train_loss=0.016184 train_acc=0.9968 val_loss=0.192657 val_acc=0.9118\n",
            "Epoch 111: train_loss=0.012265 train_acc=1.0000 val_loss=0.065426 val_acc=0.9412\n",
            "Epoch 112: train_loss=0.011007 train_acc=1.0000 val_loss=0.849185 val_acc=0.9118\n",
            "Epoch 113: train_loss=0.040023 train_acc=0.9871 val_loss=0.114651 val_acc=0.9706\n",
            "Epoch 114: train_loss=0.063183 train_acc=0.9775 val_loss=0.049769 val_acc=0.9706\n",
            "Epoch 115: train_loss=0.016145 train_acc=1.0000 val_loss=0.067922 val_acc=0.9706\n",
            "Epoch 116: train_loss=0.013065 train_acc=0.9968 val_loss=0.063509 val_acc=0.9706\n",
            "Epoch 117: train_loss=0.012407 train_acc=1.0000 val_loss=0.090626 val_acc=0.9706\n",
            "Epoch 118: train_loss=0.008018 train_acc=1.0000 val_loss=0.061430 val_acc=0.9706\n",
            "Epoch 119: train_loss=0.006113 train_acc=1.0000 val_loss=0.078738 val_acc=0.9706\n",
            "Epoch 120: train_loss=0.005177 train_acc=1.0000 val_loss=0.073688 val_acc=0.9706\n",
            "Epoch 121: train_loss=0.004301 train_acc=1.0000 val_loss=0.071079 val_acc=0.9706\n",
            "Epoch 122: train_loss=0.004016 train_acc=1.0000 val_loss=0.075036 val_acc=0.9706\n",
            "Epoch 123: train_loss=0.003679 train_acc=1.0000 val_loss=0.078135 val_acc=0.9706\n",
            "Epoch 124: train_loss=0.003383 train_acc=1.0000 val_loss=0.078234 val_acc=0.9706\n",
            "Epoch 125: train_loss=0.003155 train_acc=1.0000 val_loss=0.078735 val_acc=0.9706\n",
            "Epoch 126: train_loss=0.003018 train_acc=1.0000 val_loss=0.079805 val_acc=0.9706\n",
            "Epoch 127: train_loss=0.002846 train_acc=1.0000 val_loss=0.079719 val_acc=0.9706\n",
            "Epoch 128: train_loss=0.002713 train_acc=1.0000 val_loss=0.080352 val_acc=0.9706\n",
            "Epoch 129: train_loss=0.002596 train_acc=1.0000 val_loss=0.082080 val_acc=0.9706\n",
            "Converged at epoch 130 (train_loss change 6.76e-05 < 0.0001)\n",
            "Train acc: 1.0000 | Test acc: 0.9900\n",
            "\n",
            "Saved results table to /content/results/handwritten_results_table.csv\n",
            "    dataset model  layers  hidden  train_acc  test_acc\n",
            "handwritten   RNN       1      64   0.598071      0.62\n",
            "handwritten   RNN       1     128   0.655949      0.65\n",
            "handwritten   RNN       2      64   0.713826      0.63\n",
            "handwritten   RNN       2     128   0.691318      0.64\n",
            "handwritten  LSTM       1      64   0.858521      0.86\n",
            "handwritten  LSTM       1     128   0.897106      0.88\n",
            "handwritten  LSTM       2      64   0.935691      0.90\n",
            "handwritten  LSTM       2     128   0.890675      0.84\n",
            "handwritten   GRU       1      64   0.971061      0.97\n",
            "handwritten   GRU       1     128   0.781350      0.84\n",
            "handwritten   GRU       2      64   0.607717      0.63\n",
            "handwritten   GRU       2     128   1.000000      0.99\n",
            "\n",
            "Processing MFCC (CV) dataset...\n",
            "Loaded 290 train and 72 test samples for class 'ba' (0 bad files).\n",
            "Loaded 197 train and 49 test samples for class 'bhA' (0 bad files).\n",
            "Loaded 383 train and 96 test samples for class 'ka' (0 bad files).\n",
            "Loaded 186 train and 47 test samples for class 'ni' (0 bad files).\n",
            "Loaded 408 train and 102 test samples for class 'tA' (0 bad files).\n",
            "Total MFCC samples: train=1464 test=366 classes=5 input_dim=39\n",
            "\n",
            "=== Running on dataset: mfcc_cv ===\n",
            "\n",
            "--- mfcc_cv | RNN | layers=1 | hidden=64 ---\n",
            "Epoch 000: train_loss=1.478896 train_acc=0.3528 val_loss=1.310833 val_acc=0.4521\n",
            "Epoch 001: train_loss=1.224928 train_acc=0.5046 val_loss=1.188116 val_acc=0.5342\n",
            "Epoch 002: train_loss=1.147802 train_acc=0.5470 val_loss=1.107271 val_acc=0.6027\n",
            "Epoch 003: train_loss=1.080189 train_acc=0.6100 val_loss=1.044814 val_acc=0.6575\n",
            "Epoch 004: train_loss=1.027978 train_acc=0.6305 val_loss=1.011909 val_acc=0.6301\n",
            "Epoch 005: train_loss=0.963638 train_acc=0.6533 val_loss=0.964539 val_acc=0.6712\n",
            "Epoch 006: train_loss=0.906126 train_acc=0.6753 val_loss=0.884161 val_acc=0.6986\n",
            "Epoch 007: train_loss=0.847130 train_acc=0.6958 val_loss=0.835507 val_acc=0.6918\n",
            "Epoch 008: train_loss=0.792649 train_acc=0.7079 val_loss=0.812851 val_acc=0.6986\n",
            "Epoch 009: train_loss=0.750829 train_acc=0.7299 val_loss=0.811963 val_acc=0.6918\n",
            "Epoch 010: train_loss=0.702247 train_acc=0.7489 val_loss=0.816449 val_acc=0.7123\n",
            "Epoch 011: train_loss=0.680230 train_acc=0.7481 val_loss=0.849480 val_acc=0.6781\n",
            "Epoch 012: train_loss=0.623626 train_acc=0.7762 val_loss=0.785584 val_acc=0.6918\n",
            "Epoch 013: train_loss=0.610515 train_acc=0.7944 val_loss=0.784401 val_acc=0.7055\n",
            "Epoch 014: train_loss=0.611773 train_acc=0.7785 val_loss=0.800761 val_acc=0.6712\n",
            "Epoch 015: train_loss=0.539082 train_acc=0.8194 val_loss=0.769787 val_acc=0.6986\n",
            "Epoch 016: train_loss=0.496208 train_acc=0.8323 val_loss=0.773267 val_acc=0.7055\n",
            "Epoch 017: train_loss=0.495362 train_acc=0.8293 val_loss=0.758509 val_acc=0.7260\n",
            "Epoch 018: train_loss=0.488353 train_acc=0.8338 val_loss=0.693436 val_acc=0.7534\n",
            "Epoch 019: train_loss=0.453799 train_acc=0.8513 val_loss=0.750830 val_acc=0.7466\n",
            "Epoch 020: train_loss=0.563452 train_acc=0.7974 val_loss=0.806395 val_acc=0.7192\n",
            "Epoch 021: train_loss=0.495904 train_acc=0.8270 val_loss=0.717635 val_acc=0.7397\n",
            "Epoch 022: train_loss=0.425645 train_acc=0.8627 val_loss=0.753741 val_acc=0.7466\n",
            "Epoch 023: train_loss=0.388386 train_acc=0.8718 val_loss=0.712922 val_acc=0.7466\n",
            "Epoch 024: train_loss=0.359757 train_acc=0.8885 val_loss=0.698545 val_acc=0.7534\n",
            "Epoch 025: train_loss=0.355345 train_acc=0.8885 val_loss=0.732566 val_acc=0.7466\n",
            "Epoch 026: train_loss=0.328190 train_acc=0.8976 val_loss=0.703133 val_acc=0.7466\n",
            "Epoch 027: train_loss=0.304731 train_acc=0.9044 val_loss=0.725520 val_acc=0.7466\n",
            "Epoch 028: train_loss=0.287353 train_acc=0.9241 val_loss=0.703623 val_acc=0.7603\n",
            "Epoch 029: train_loss=0.294395 train_acc=0.9052 val_loss=0.729071 val_acc=0.7671\n",
            "Epoch 030: train_loss=0.287905 train_acc=0.9120 val_loss=0.791187 val_acc=0.7192\n",
            "Epoch 031: train_loss=0.279310 train_acc=0.9158 val_loss=0.760178 val_acc=0.7466\n",
            "Epoch 032: train_loss=0.249956 train_acc=0.9256 val_loss=0.724418 val_acc=0.7671\n",
            "Epoch 033: train_loss=0.222558 train_acc=0.9393 val_loss=0.760607 val_acc=0.7603\n",
            "Epoch 034: train_loss=0.241331 train_acc=0.9279 val_loss=0.777848 val_acc=0.7466\n",
            "Epoch 035: train_loss=0.224652 train_acc=0.9310 val_loss=0.763050 val_acc=0.7671\n",
            "Epoch 036: train_loss=0.221018 train_acc=0.9408 val_loss=0.807324 val_acc=0.7260\n",
            "Epoch 037: train_loss=0.201802 train_acc=0.9439 val_loss=0.784831 val_acc=0.7740\n",
            "Epoch 038: train_loss=0.208233 train_acc=0.9363 val_loss=0.717428 val_acc=0.7603\n",
            "Epoch 039: train_loss=0.187841 train_acc=0.9461 val_loss=0.839673 val_acc=0.7329\n",
            "Epoch 040: train_loss=0.208167 train_acc=0.9370 val_loss=0.897262 val_acc=0.7123\n",
            "Epoch 041: train_loss=0.191660 train_acc=0.9401 val_loss=0.786367 val_acc=0.7534\n",
            "Epoch 042: train_loss=0.167691 train_acc=0.9484 val_loss=0.833780 val_acc=0.7740\n",
            "Epoch 043: train_loss=0.157256 train_acc=0.9575 val_loss=0.831761 val_acc=0.7397\n",
            "Epoch 044: train_loss=0.186434 train_acc=0.9446 val_loss=0.840640 val_acc=0.7397\n",
            "Epoch 045: train_loss=0.222114 train_acc=0.9279 val_loss=0.781339 val_acc=0.7740\n",
            "Epoch 046: train_loss=0.179216 train_acc=0.9461 val_loss=0.783536 val_acc=0.7740\n",
            "Epoch 047: train_loss=0.126260 train_acc=0.9750 val_loss=0.800017 val_acc=0.7877\n",
            "Epoch 048: train_loss=0.117073 train_acc=0.9742 val_loss=0.842226 val_acc=0.7945\n",
            "Epoch 049: train_loss=0.116381 train_acc=0.9757 val_loss=0.841640 val_acc=0.7671\n",
            "Epoch 050: train_loss=0.115101 train_acc=0.9719 val_loss=0.831882 val_acc=0.7808\n",
            "Epoch 051: train_loss=0.100602 train_acc=0.9833 val_loss=0.884486 val_acc=0.7740\n",
            "Epoch 052: train_loss=0.088597 train_acc=0.9810 val_loss=0.874776 val_acc=0.7671\n",
            "Epoch 053: train_loss=0.092484 train_acc=0.9803 val_loss=0.786620 val_acc=0.7877\n",
            "Epoch 054: train_loss=0.081084 train_acc=0.9841 val_loss=0.825757 val_acc=0.7808\n",
            "Epoch 055: train_loss=0.074758 train_acc=0.9863 val_loss=0.847724 val_acc=0.7740\n",
            "Epoch 056: train_loss=0.068096 train_acc=0.9901 val_loss=0.860713 val_acc=0.7808\n",
            "Epoch 057: train_loss=0.064391 train_acc=0.9886 val_loss=0.876766 val_acc=0.7808\n",
            "Epoch 058: train_loss=0.060492 train_acc=0.9909 val_loss=0.833525 val_acc=0.7603\n",
            "Epoch 059: train_loss=0.130031 train_acc=0.9659 val_loss=1.249947 val_acc=0.6644\n",
            "Epoch 060: train_loss=0.350485 train_acc=0.8869 val_loss=0.790065 val_acc=0.7740\n",
            "Epoch 061: train_loss=0.236050 train_acc=0.9287 val_loss=0.794355 val_acc=0.7671\n",
            "Epoch 062: train_loss=0.125635 train_acc=0.9613 val_loss=0.770798 val_acc=0.7603\n",
            "Epoch 063: train_loss=0.134995 train_acc=0.9605 val_loss=0.701448 val_acc=0.7877\n",
            "Epoch 064: train_loss=0.079248 train_acc=0.9856 val_loss=0.777806 val_acc=0.7740\n",
            "Epoch 065: train_loss=0.063555 train_acc=0.9886 val_loss=0.877158 val_acc=0.7740\n",
            "Epoch 066: train_loss=0.063035 train_acc=0.9909 val_loss=0.830793 val_acc=0.7671\n",
            "Epoch 067: train_loss=0.053661 train_acc=0.9924 val_loss=0.864998 val_acc=0.7671\n",
            "Epoch 068: train_loss=0.048117 train_acc=0.9947 val_loss=0.869333 val_acc=0.7671\n",
            "Epoch 069: train_loss=0.044355 train_acc=0.9947 val_loss=0.896363 val_acc=0.7671\n",
            "Epoch 070: train_loss=0.041816 train_acc=0.9954 val_loss=0.890672 val_acc=0.7671\n",
            "Epoch 071: train_loss=0.039825 train_acc=0.9962 val_loss=0.902508 val_acc=0.7740\n",
            "Epoch 072: train_loss=0.038733 train_acc=0.9962 val_loss=0.901559 val_acc=0.7808\n",
            "Epoch 073: train_loss=0.036887 train_acc=0.9954 val_loss=0.943625 val_acc=0.7808\n",
            "Epoch 074: train_loss=0.040194 train_acc=0.9947 val_loss=0.959814 val_acc=0.7740\n",
            "Epoch 075: train_loss=0.379773 train_acc=0.9090 val_loss=1.083014 val_acc=0.7260\n",
            "Epoch 076: train_loss=0.421293 train_acc=0.8589 val_loss=0.809872 val_acc=0.7603\n",
            "Epoch 077: train_loss=0.286786 train_acc=0.9036 val_loss=0.866286 val_acc=0.7671\n",
            "Epoch 078: train_loss=0.163513 train_acc=0.9461 val_loss=0.724685 val_acc=0.7740\n",
            "Epoch 079: train_loss=0.109097 train_acc=0.9727 val_loss=0.880875 val_acc=0.7603\n",
            "Epoch 080: train_loss=0.081832 train_acc=0.9856 val_loss=0.825047 val_acc=0.7603\n",
            "Epoch 081: train_loss=0.062897 train_acc=0.9909 val_loss=0.805020 val_acc=0.7808\n",
            "Epoch 082: train_loss=0.051374 train_acc=0.9939 val_loss=0.843486 val_acc=0.7877\n",
            "Epoch 083: train_loss=0.047018 train_acc=0.9932 val_loss=0.909552 val_acc=0.7808\n",
            "Epoch 084: train_loss=0.042437 train_acc=0.9962 val_loss=0.914255 val_acc=0.7877\n",
            "Epoch 085: train_loss=0.038975 train_acc=0.9962 val_loss=0.888239 val_acc=0.7877\n",
            "Epoch 086: train_loss=0.035933 train_acc=0.9970 val_loss=0.916478 val_acc=0.7808\n",
            "Epoch 087: train_loss=0.033643 train_acc=0.9977 val_loss=0.896522 val_acc=0.7877\n",
            "Epoch 088: train_loss=0.034635 train_acc=0.9977 val_loss=0.866262 val_acc=0.7945\n",
            "Epoch 089: train_loss=0.105671 train_acc=0.9727 val_loss=0.954392 val_acc=0.7123\n",
            "Epoch 090: train_loss=0.139114 train_acc=0.9636 val_loss=1.187938 val_acc=0.7329\n",
            "Epoch 091: train_loss=0.095013 train_acc=0.9727 val_loss=0.826288 val_acc=0.7808\n",
            "Epoch 092: train_loss=0.050914 train_acc=0.9947 val_loss=0.892809 val_acc=0.7671\n",
            "Epoch 093: train_loss=0.043182 train_acc=0.9939 val_loss=0.912960 val_acc=0.7740\n",
            "Epoch 094: train_loss=0.044614 train_acc=0.9939 val_loss=0.802339 val_acc=0.7945\n",
            "Epoch 095: train_loss=0.033562 train_acc=0.9962 val_loss=0.875785 val_acc=0.7877\n",
            "Epoch 096: train_loss=0.028027 train_acc=0.9985 val_loss=0.864539 val_acc=0.7945\n",
            "Epoch 097: train_loss=0.025170 train_acc=0.9985 val_loss=0.874245 val_acc=0.8014\n",
            "Epoch 098: train_loss=0.023723 train_acc=0.9985 val_loss=0.944362 val_acc=0.7877\n",
            "Epoch 099: train_loss=0.022615 train_acc=0.9985 val_loss=0.940712 val_acc=0.7877\n",
            "Epoch 100: train_loss=0.021503 train_acc=0.9985 val_loss=0.919231 val_acc=0.7945\n",
            "Epoch 101: train_loss=0.020612 train_acc=0.9985 val_loss=0.958975 val_acc=0.7945\n",
            "Converged at epoch 102 (train_loss change 7.81e-05 < 0.0001)\n",
            "Train acc: 0.9985 | Test acc: 0.7514\n",
            "\n",
            "--- mfcc_cv | RNN | layers=1 | hidden=128 ---\n",
            "Epoch 000: train_loss=1.327313 train_acc=0.4143 val_loss=1.136438 val_acc=0.5685\n",
            "Epoch 001: train_loss=1.127082 train_acc=0.5379 val_loss=1.037693 val_acc=0.6370\n",
            "Epoch 002: train_loss=1.020186 train_acc=0.6146 val_loss=0.917847 val_acc=0.6438\n",
            "Epoch 003: train_loss=0.881759 train_acc=0.6669 val_loss=0.833136 val_acc=0.6781\n",
            "Epoch 004: train_loss=0.797629 train_acc=0.6958 val_loss=0.742212 val_acc=0.7329\n",
            "Epoch 005: train_loss=0.711203 train_acc=0.7352 val_loss=0.767393 val_acc=0.6918\n",
            "Epoch 006: train_loss=0.639889 train_acc=0.7511 val_loss=0.654719 val_acc=0.7534\n",
            "Epoch 007: train_loss=0.614331 train_acc=0.7754 val_loss=0.779319 val_acc=0.7123\n",
            "Epoch 008: train_loss=0.582973 train_acc=0.7883 val_loss=0.675368 val_acc=0.7329\n",
            "Epoch 009: train_loss=0.556458 train_acc=0.7997 val_loss=0.655090 val_acc=0.7123\n",
            "Epoch 010: train_loss=0.551726 train_acc=0.7906 val_loss=0.609479 val_acc=0.7123\n",
            "Epoch 011: train_loss=0.468240 train_acc=0.8331 val_loss=0.591429 val_acc=0.7945\n",
            "Epoch 012: train_loss=0.439632 train_acc=0.8558 val_loss=0.542173 val_acc=0.8151\n",
            "Epoch 013: train_loss=0.385265 train_acc=0.8794 val_loss=0.544126 val_acc=0.8014\n",
            "Epoch 014: train_loss=0.401501 train_acc=0.8505 val_loss=0.621013 val_acc=0.7671\n",
            "Epoch 015: train_loss=0.363386 train_acc=0.8710 val_loss=0.531835 val_acc=0.8151\n",
            "Epoch 016: train_loss=0.312026 train_acc=0.8968 val_loss=0.535786 val_acc=0.8151\n",
            "Epoch 017: train_loss=0.286052 train_acc=0.9052 val_loss=0.662754 val_acc=0.7671\n",
            "Epoch 018: train_loss=0.293309 train_acc=0.8961 val_loss=0.517093 val_acc=0.8699\n",
            "Epoch 019: train_loss=0.260759 train_acc=0.9188 val_loss=0.462185 val_acc=0.8356\n",
            "Epoch 020: train_loss=0.200107 train_acc=0.9347 val_loss=0.553294 val_acc=0.8151\n",
            "Epoch 021: train_loss=0.232058 train_acc=0.9219 val_loss=0.525230 val_acc=0.8288\n",
            "Epoch 022: train_loss=0.209058 train_acc=0.9332 val_loss=0.617099 val_acc=0.8151\n",
            "Epoch 023: train_loss=0.229724 train_acc=0.9234 val_loss=0.455963 val_acc=0.8630\n",
            "Epoch 024: train_loss=0.187398 train_acc=0.9423 val_loss=0.528027 val_acc=0.8562\n",
            "Epoch 025: train_loss=0.165391 train_acc=0.9530 val_loss=0.437398 val_acc=0.8699\n",
            "Epoch 026: train_loss=0.113472 train_acc=0.9742 val_loss=0.470582 val_acc=0.8493\n",
            "Epoch 027: train_loss=0.167287 train_acc=0.9636 val_loss=0.631589 val_acc=0.8288\n",
            "Epoch 028: train_loss=0.409661 train_acc=0.8703 val_loss=0.533678 val_acc=0.8219\n",
            "Epoch 029: train_loss=0.207688 train_acc=0.9234 val_loss=0.538439 val_acc=0.8219\n",
            "Epoch 030: train_loss=0.165323 train_acc=0.9514 val_loss=0.588318 val_acc=0.7808\n",
            "Epoch 031: train_loss=0.118318 train_acc=0.9659 val_loss=0.507103 val_acc=0.8699\n",
            "Epoch 032: train_loss=0.080716 train_acc=0.9841 val_loss=0.525177 val_acc=0.8630\n",
            "Epoch 033: train_loss=0.072357 train_acc=0.9841 val_loss=0.519140 val_acc=0.8562\n",
            "Epoch 034: train_loss=0.059020 train_acc=0.9909 val_loss=0.524343 val_acc=0.8425\n",
            "Epoch 035: train_loss=0.050761 train_acc=0.9917 val_loss=0.518360 val_acc=0.8493\n",
            "Epoch 036: train_loss=0.090845 train_acc=0.9765 val_loss=0.613167 val_acc=0.8562\n",
            "Epoch 037: train_loss=0.344672 train_acc=0.8885 val_loss=0.844380 val_acc=0.7740\n",
            "Epoch 038: train_loss=0.227843 train_acc=0.9219 val_loss=0.624125 val_acc=0.8356\n",
            "Epoch 039: train_loss=0.327125 train_acc=0.8892 val_loss=0.506530 val_acc=0.8356\n",
            "Epoch 040: train_loss=0.197518 train_acc=0.9264 val_loss=0.650046 val_acc=0.8288\n",
            "Epoch 041: train_loss=0.097014 train_acc=0.9780 val_loss=0.628273 val_acc=0.8219\n",
            "Epoch 042: train_loss=0.384050 train_acc=0.8832 val_loss=0.773192 val_acc=0.7466\n",
            "Epoch 043: train_loss=0.246055 train_acc=0.9173 val_loss=0.716062 val_acc=0.7603\n",
            "Epoch 044: train_loss=0.136277 train_acc=0.9636 val_loss=0.674927 val_acc=0.8014\n",
            "Epoch 045: train_loss=0.083459 train_acc=0.9810 val_loss=0.641094 val_acc=0.8219\n",
            "Epoch 046: train_loss=0.062639 train_acc=0.9863 val_loss=0.620864 val_acc=0.8425\n",
            "Epoch 047: train_loss=0.047750 train_acc=0.9939 val_loss=0.598836 val_acc=0.8493\n",
            "Epoch 048: train_loss=0.041350 train_acc=0.9954 val_loss=0.633464 val_acc=0.8288\n",
            "Epoch 049: train_loss=0.051153 train_acc=0.9909 val_loss=0.593206 val_acc=0.8425\n",
            "Epoch 050: train_loss=0.045423 train_acc=0.9932 val_loss=0.667090 val_acc=0.8356\n",
            "Epoch 051: train_loss=0.060801 train_acc=0.9856 val_loss=0.588432 val_acc=0.8425\n",
            "Epoch 052: train_loss=0.038870 train_acc=0.9939 val_loss=0.627770 val_acc=0.8151\n",
            "Epoch 053: train_loss=0.031905 train_acc=0.9970 val_loss=0.661142 val_acc=0.8493\n",
            "Epoch 054: train_loss=0.029105 train_acc=0.9962 val_loss=0.592825 val_acc=0.8493\n",
            "Epoch 055: train_loss=0.022085 train_acc=0.9977 val_loss=0.579230 val_acc=0.8699\n",
            "Epoch 056: train_loss=0.018040 train_acc=0.9985 val_loss=0.594696 val_acc=0.8699\n",
            "Epoch 057: train_loss=0.016713 train_acc=0.9985 val_loss=0.580368 val_acc=0.8767\n",
            "Epoch 058: train_loss=0.014662 train_acc=0.9992 val_loss=0.612888 val_acc=0.8630\n",
            "Epoch 059: train_loss=0.014291 train_acc=1.0000 val_loss=0.593682 val_acc=0.8836\n",
            "Epoch 060: train_loss=0.012668 train_acc=1.0000 val_loss=0.613910 val_acc=0.8836\n",
            "Epoch 061: train_loss=0.011569 train_acc=1.0000 val_loss=0.614816 val_acc=0.8767\n",
            "Epoch 062: train_loss=0.010829 train_acc=1.0000 val_loss=0.618043 val_acc=0.8767\n",
            "Converged at epoch 63 (train_loss change 6.67e-05 < 0.0001)\n",
            "Train acc: 1.0000 | Test acc: 0.7923\n",
            "\n",
            "--- mfcc_cv | RNN | layers=2 | hidden=64 ---\n",
            "Epoch 000: train_loss=1.351495 train_acc=0.4112 val_loss=1.160439 val_acc=0.5548\n",
            "Epoch 001: train_loss=1.120528 train_acc=0.5357 val_loss=0.999837 val_acc=0.6164\n",
            "Epoch 002: train_loss=0.949500 train_acc=0.6358 val_loss=0.848631 val_acc=0.6781\n",
            "Epoch 003: train_loss=0.855767 train_acc=0.6737 val_loss=0.796870 val_acc=0.7123\n",
            "Epoch 004: train_loss=0.783360 train_acc=0.6995 val_loss=0.719623 val_acc=0.7192\n",
            "Epoch 005: train_loss=0.699659 train_acc=0.7519 val_loss=0.739770 val_acc=0.7329\n",
            "Epoch 006: train_loss=0.656541 train_acc=0.7754 val_loss=0.703529 val_acc=0.6918\n",
            "Epoch 007: train_loss=0.589064 train_acc=0.7929 val_loss=0.624759 val_acc=0.7466\n",
            "Epoch 008: train_loss=0.580817 train_acc=0.7936 val_loss=0.711545 val_acc=0.7397\n",
            "Epoch 009: train_loss=0.504183 train_acc=0.8278 val_loss=0.650928 val_acc=0.7603\n",
            "Epoch 010: train_loss=0.430207 train_acc=0.8566 val_loss=0.735724 val_acc=0.7192\n",
            "Epoch 011: train_loss=0.453106 train_acc=0.8490 val_loss=0.636518 val_acc=0.7945\n",
            "Epoch 012: train_loss=0.423631 train_acc=0.8657 val_loss=0.817895 val_acc=0.7397\n",
            "Epoch 013: train_loss=0.463601 train_acc=0.8498 val_loss=0.618801 val_acc=0.7945\n",
            "Epoch 014: train_loss=0.356288 train_acc=0.8892 val_loss=0.678215 val_acc=0.7671\n",
            "Epoch 015: train_loss=0.325822 train_acc=0.9029 val_loss=0.585184 val_acc=0.8151\n",
            "Epoch 016: train_loss=0.318701 train_acc=0.8998 val_loss=0.597681 val_acc=0.8151\n",
            "Epoch 017: train_loss=0.275360 train_acc=0.9173 val_loss=0.584425 val_acc=0.8219\n",
            "Epoch 018: train_loss=0.252005 train_acc=0.9264 val_loss=0.683748 val_acc=0.7808\n",
            "Epoch 019: train_loss=0.253803 train_acc=0.9211 val_loss=0.728183 val_acc=0.7671\n",
            "Epoch 020: train_loss=0.230501 train_acc=0.9310 val_loss=0.616849 val_acc=0.8288\n",
            "Epoch 021: train_loss=0.204041 train_acc=0.9461 val_loss=0.662993 val_acc=0.8014\n",
            "Epoch 022: train_loss=0.231111 train_acc=0.9355 val_loss=0.621799 val_acc=0.8014\n",
            "Epoch 023: train_loss=0.439145 train_acc=0.8498 val_loss=0.714779 val_acc=0.7877\n",
            "Epoch 024: train_loss=0.264156 train_acc=0.9097 val_loss=0.616129 val_acc=0.8219\n",
            "Epoch 025: train_loss=0.206936 train_acc=0.9317 val_loss=0.636194 val_acc=0.8014\n",
            "Epoch 026: train_loss=0.165468 train_acc=0.9461 val_loss=0.770399 val_acc=0.7671\n",
            "Epoch 027: train_loss=0.142804 train_acc=0.9590 val_loss=0.568123 val_acc=0.8493\n",
            "Epoch 028: train_loss=0.151569 train_acc=0.9575 val_loss=0.640135 val_acc=0.7877\n",
            "Epoch 029: train_loss=0.174543 train_acc=0.9393 val_loss=0.779540 val_acc=0.8014\n",
            "Epoch 030: train_loss=0.159547 train_acc=0.9537 val_loss=0.659149 val_acc=0.8151\n",
            "Epoch 031: train_loss=0.195473 train_acc=0.9363 val_loss=0.609116 val_acc=0.8014\n",
            "Epoch 032: train_loss=0.187493 train_acc=0.9355 val_loss=0.696843 val_acc=0.8356\n",
            "Epoch 033: train_loss=0.207796 train_acc=0.9363 val_loss=0.652291 val_acc=0.8151\n",
            "Epoch 034: train_loss=0.206126 train_acc=0.9279 val_loss=0.814029 val_acc=0.7740\n",
            "Epoch 035: train_loss=0.225652 train_acc=0.9317 val_loss=0.665021 val_acc=0.7877\n",
            "Epoch 036: train_loss=0.253433 train_acc=0.9158 val_loss=0.755621 val_acc=0.7671\n",
            "Epoch 037: train_loss=0.218581 train_acc=0.9272 val_loss=0.611631 val_acc=0.8151\n",
            "Epoch 038: train_loss=0.137358 train_acc=0.9621 val_loss=0.702894 val_acc=0.8082\n",
            "Epoch 039: train_loss=0.101460 train_acc=0.9674 val_loss=0.704926 val_acc=0.7945\n",
            "Epoch 040: train_loss=0.079208 train_acc=0.9788 val_loss=0.699837 val_acc=0.8151\n",
            "Epoch 041: train_loss=0.050882 train_acc=0.9909 val_loss=0.729816 val_acc=0.8082\n",
            "Epoch 042: train_loss=0.049460 train_acc=0.9871 val_loss=0.724104 val_acc=0.8082\n",
            "Epoch 043: train_loss=0.094636 train_acc=0.9788 val_loss=0.907590 val_acc=0.7466\n",
            "Epoch 044: train_loss=0.104031 train_acc=0.9666 val_loss=0.799339 val_acc=0.7945\n",
            "Epoch 045: train_loss=0.065598 train_acc=0.9803 val_loss=0.903284 val_acc=0.8151\n",
            "Epoch 046: train_loss=0.096072 train_acc=0.9651 val_loss=0.752166 val_acc=0.8356\n",
            "Epoch 047: train_loss=0.058770 train_acc=0.9833 val_loss=0.744641 val_acc=0.8288\n",
            "Epoch 048: train_loss=0.053319 train_acc=0.9825 val_loss=0.699599 val_acc=0.8356\n",
            "Epoch 049: train_loss=0.031427 train_acc=0.9924 val_loss=0.796087 val_acc=0.8288\n",
            "Epoch 050: train_loss=0.027116 train_acc=0.9954 val_loss=0.774460 val_acc=0.8219\n",
            "Epoch 051: train_loss=0.057337 train_acc=0.9833 val_loss=0.759772 val_acc=0.8082\n",
            "Epoch 052: train_loss=0.045628 train_acc=0.9871 val_loss=0.739105 val_acc=0.8288\n",
            "Epoch 053: train_loss=0.035720 train_acc=0.9901 val_loss=0.848076 val_acc=0.8082\n",
            "Epoch 054: train_loss=0.061742 train_acc=0.9818 val_loss=1.022629 val_acc=0.7740\n",
            "Epoch 055: train_loss=0.141490 train_acc=0.9552 val_loss=0.720975 val_acc=0.8082\n",
            "Epoch 056: train_loss=0.208806 train_acc=0.9446 val_loss=1.036147 val_acc=0.7740\n",
            "Epoch 057: train_loss=0.423510 train_acc=0.8695 val_loss=0.857385 val_acc=0.7603\n",
            "Epoch 058: train_loss=0.175861 train_acc=0.9416 val_loss=0.800559 val_acc=0.8014\n",
            "Epoch 059: train_loss=0.104621 train_acc=0.9613 val_loss=0.766661 val_acc=0.8082\n",
            "Epoch 060: train_loss=0.181717 train_acc=0.9401 val_loss=0.811627 val_acc=0.7740\n",
            "Epoch 061: train_loss=0.078877 train_acc=0.9712 val_loss=0.874155 val_acc=0.8082\n",
            "Epoch 062: train_loss=0.118704 train_acc=0.9681 val_loss=0.875067 val_acc=0.7808\n",
            "Epoch 063: train_loss=0.075911 train_acc=0.9795 val_loss=0.883949 val_acc=0.7945\n",
            "Epoch 064: train_loss=0.054464 train_acc=0.9848 val_loss=0.843961 val_acc=0.8288\n",
            "Epoch 065: train_loss=0.036803 train_acc=0.9924 val_loss=0.957524 val_acc=0.7808\n",
            "Epoch 066: train_loss=0.049740 train_acc=0.9871 val_loss=0.951958 val_acc=0.7808\n",
            "Epoch 067: train_loss=0.063716 train_acc=0.9795 val_loss=0.882998 val_acc=0.7877\n",
            "Epoch 068: train_loss=0.023440 train_acc=0.9954 val_loss=0.824480 val_acc=0.8219\n",
            "Epoch 069: train_loss=0.015081 train_acc=0.9977 val_loss=0.869863 val_acc=0.8219\n",
            "Epoch 070: train_loss=0.011823 train_acc=0.9977 val_loss=0.869260 val_acc=0.8219\n",
            "Epoch 071: train_loss=0.009887 train_acc=0.9992 val_loss=0.873925 val_acc=0.8151\n",
            "Epoch 072: train_loss=0.012608 train_acc=0.9992 val_loss=0.885927 val_acc=0.8219\n",
            "Epoch 073: train_loss=0.007947 train_acc=1.0000 val_loss=0.889159 val_acc=0.8288\n",
            "Epoch 074: train_loss=0.007081 train_acc=1.0000 val_loss=0.891764 val_acc=0.8288\n",
            "Epoch 075: train_loss=0.006392 train_acc=1.0000 val_loss=0.904893 val_acc=0.8151\n",
            "Epoch 076: train_loss=0.005899 train_acc=1.0000 val_loss=0.913899 val_acc=0.8151\n",
            "Epoch 077: train_loss=0.005555 train_acc=1.0000 val_loss=0.952691 val_acc=0.8151\n",
            "Epoch 078: train_loss=0.005296 train_acc=1.0000 val_loss=0.952977 val_acc=0.8151\n",
            "Epoch 079: train_loss=0.004862 train_acc=1.0000 val_loss=0.997820 val_acc=0.8151\n",
            "Epoch 080: train_loss=0.004541 train_acc=1.0000 val_loss=0.996177 val_acc=0.8151\n",
            "Epoch 081: train_loss=0.004280 train_acc=1.0000 val_loss=0.990452 val_acc=0.8151\n",
            "Epoch 082: train_loss=0.004024 train_acc=1.0000 val_loss=0.989043 val_acc=0.8151\n",
            "Epoch 083: train_loss=0.003885 train_acc=1.0000 val_loss=1.009645 val_acc=0.8219\n",
            "Epoch 084: train_loss=0.003779 train_acc=1.0000 val_loss=1.019868 val_acc=0.8151\n",
            "Epoch 085: train_loss=0.003446 train_acc=1.0000 val_loss=1.025413 val_acc=0.8082\n",
            "Epoch 086: train_loss=0.003274 train_acc=1.0000 val_loss=1.030718 val_acc=0.8082\n",
            "Converged at epoch 87 (train_loss change 5.61e-05 < 0.0001)\n",
            "Train acc: 1.0000 | Test acc: 0.8197\n",
            "\n",
            "--- mfcc_cv | RNN | layers=2 | hidden=128 ---\n",
            "Epoch 000: train_loss=1.284268 train_acc=0.4393 val_loss=1.126944 val_acc=0.5274\n",
            "Epoch 001: train_loss=1.030150 train_acc=0.5721 val_loss=0.867485 val_acc=0.6849\n",
            "Epoch 002: train_loss=0.875145 train_acc=0.6616 val_loss=0.918875 val_acc=0.6164\n",
            "Epoch 003: train_loss=0.831010 train_acc=0.6722 val_loss=0.722276 val_acc=0.6644\n",
            "Epoch 004: train_loss=0.732900 train_acc=0.7231 val_loss=0.664965 val_acc=0.7329\n",
            "Epoch 005: train_loss=0.725381 train_acc=0.7170 val_loss=0.692177 val_acc=0.7466\n",
            "Epoch 006: train_loss=0.653933 train_acc=0.7686 val_loss=0.597869 val_acc=0.7671\n",
            "Epoch 007: train_loss=0.628533 train_acc=0.7701 val_loss=0.749402 val_acc=0.7192\n",
            "Epoch 008: train_loss=0.604383 train_acc=0.7822 val_loss=0.759731 val_acc=0.7397\n",
            "Epoch 009: train_loss=0.553833 train_acc=0.8050 val_loss=1.042914 val_acc=0.5959\n",
            "Epoch 010: train_loss=0.615794 train_acc=0.7777 val_loss=0.677853 val_acc=0.7397\n",
            "Epoch 011: train_loss=0.530105 train_acc=0.8065 val_loss=0.609956 val_acc=0.7808\n",
            "Epoch 012: train_loss=0.456778 train_acc=0.8414 val_loss=0.769117 val_acc=0.7466\n",
            "Epoch 013: train_loss=0.506437 train_acc=0.8217 val_loss=0.677594 val_acc=0.7808\n",
            "Epoch 014: train_loss=0.394037 train_acc=0.8649 val_loss=0.552941 val_acc=0.7945\n",
            "Epoch 015: train_loss=0.329611 train_acc=0.8907 val_loss=0.609284 val_acc=0.7877\n",
            "Epoch 016: train_loss=0.318671 train_acc=0.8938 val_loss=0.594717 val_acc=0.8151\n",
            "Epoch 017: train_loss=0.302850 train_acc=0.8998 val_loss=0.892671 val_acc=0.6712\n",
            "Epoch 018: train_loss=0.381598 train_acc=0.8695 val_loss=0.562684 val_acc=0.7740\n",
            "Epoch 019: train_loss=0.361701 train_acc=0.8907 val_loss=0.640208 val_acc=0.7740\n",
            "Epoch 020: train_loss=0.256722 train_acc=0.9173 val_loss=0.544212 val_acc=0.8082\n",
            "Epoch 021: train_loss=0.193231 train_acc=0.9416 val_loss=0.587019 val_acc=0.8219\n",
            "Epoch 022: train_loss=0.168567 train_acc=0.9461 val_loss=0.535677 val_acc=0.8288\n",
            "Epoch 023: train_loss=0.148676 train_acc=0.9522 val_loss=0.561023 val_acc=0.8151\n",
            "Epoch 024: train_loss=0.233929 train_acc=0.9241 val_loss=0.826307 val_acc=0.7466\n",
            "Epoch 025: train_loss=0.394096 train_acc=0.8619 val_loss=0.682117 val_acc=0.8014\n",
            "Epoch 026: train_loss=0.223717 train_acc=0.9310 val_loss=0.634263 val_acc=0.8014\n",
            "Epoch 027: train_loss=0.175374 train_acc=0.9454 val_loss=0.598482 val_acc=0.8219\n",
            "Epoch 028: train_loss=0.259665 train_acc=0.9173 val_loss=0.913786 val_acc=0.7123\n",
            "Epoch 029: train_loss=0.247113 train_acc=0.9188 val_loss=0.553625 val_acc=0.8288\n",
            "Epoch 030: train_loss=0.117216 train_acc=0.9659 val_loss=0.619140 val_acc=0.8288\n",
            "Epoch 031: train_loss=0.130746 train_acc=0.9621 val_loss=0.730910 val_acc=0.7877\n",
            "Epoch 032: train_loss=0.127963 train_acc=0.9575 val_loss=0.747422 val_acc=0.8356\n",
            "Epoch 033: train_loss=0.118516 train_acc=0.9621 val_loss=0.637003 val_acc=0.8356\n",
            "Epoch 034: train_loss=0.066378 train_acc=0.9810 val_loss=0.620222 val_acc=0.8288\n",
            "Epoch 035: train_loss=0.038436 train_acc=0.9932 val_loss=0.595020 val_acc=0.8425\n",
            "Epoch 036: train_loss=0.027369 train_acc=0.9962 val_loss=0.586200 val_acc=0.8630\n",
            "Epoch 037: train_loss=0.021881 train_acc=0.9977 val_loss=0.621589 val_acc=0.8562\n",
            "Epoch 038: train_loss=0.018526 train_acc=0.9977 val_loss=0.608288 val_acc=0.8630\n",
            "Epoch 039: train_loss=0.015727 train_acc=0.9977 val_loss=0.653214 val_acc=0.8562\n",
            "Epoch 040: train_loss=0.014047 train_acc=0.9977 val_loss=0.699166 val_acc=0.8425\n",
            "Epoch 041: train_loss=0.015990 train_acc=0.9985 val_loss=0.666304 val_acc=0.8562\n",
            "Epoch 042: train_loss=0.013263 train_acc=0.9977 val_loss=0.768746 val_acc=0.8425\n",
            "Epoch 043: train_loss=0.187776 train_acc=0.9454 val_loss=0.910560 val_acc=0.7808\n",
            "Epoch 044: train_loss=0.151672 train_acc=0.9484 val_loss=0.632117 val_acc=0.8425\n",
            "Epoch 045: train_loss=0.139553 train_acc=0.9583 val_loss=0.595292 val_acc=0.8425\n",
            "Epoch 046: train_loss=0.105221 train_acc=0.9689 val_loss=1.161438 val_acc=0.7260\n",
            "Epoch 047: train_loss=0.144062 train_acc=0.9613 val_loss=0.644980 val_acc=0.8151\n",
            "Epoch 048: train_loss=0.125338 train_acc=0.9590 val_loss=0.782256 val_acc=0.8082\n",
            "Epoch 049: train_loss=0.068957 train_acc=0.9795 val_loss=0.648730 val_acc=0.8219\n",
            "Epoch 050: train_loss=0.027370 train_acc=0.9954 val_loss=0.699885 val_acc=0.8630\n",
            "Epoch 051: train_loss=0.013803 train_acc=0.9992 val_loss=0.680191 val_acc=0.8699\n",
            "Epoch 052: train_loss=0.009784 train_acc=0.9992 val_loss=0.674665 val_acc=0.8699\n",
            "Epoch 053: train_loss=0.007629 train_acc=0.9992 val_loss=0.696222 val_acc=0.8699\n",
            "Epoch 054: train_loss=0.006588 train_acc=1.0000 val_loss=0.710558 val_acc=0.8699\n",
            "Epoch 055: train_loss=0.005935 train_acc=1.0000 val_loss=0.729547 val_acc=0.8699\n",
            "Epoch 056: train_loss=0.005073 train_acc=1.0000 val_loss=0.718780 val_acc=0.8699\n",
            "Epoch 057: train_loss=0.004553 train_acc=1.0000 val_loss=0.735379 val_acc=0.8630\n",
            "Epoch 058: train_loss=0.004120 train_acc=1.0000 val_loss=0.755082 val_acc=0.8630\n",
            "Epoch 059: train_loss=0.003708 train_acc=1.0000 val_loss=0.766010 val_acc=0.8493\n",
            "Converged at epoch 60 (train_loss change 2.04e-05 < 0.0001)\n",
            "Train acc: 1.0000 | Test acc: 0.8142\n",
            "\n",
            "--- mfcc_cv | LSTM | layers=1 | hidden=64 ---\n",
            "Epoch 000: train_loss=1.420016 train_acc=0.4105 val_loss=1.225627 val_acc=0.5274\n",
            "Epoch 001: train_loss=1.059837 train_acc=0.6313 val_loss=0.871268 val_acc=0.7740\n",
            "Epoch 002: train_loss=0.731908 train_acc=0.7944 val_loss=0.597235 val_acc=0.8699\n",
            "Epoch 003: train_loss=0.525113 train_acc=0.8566 val_loss=0.494042 val_acc=0.8836\n",
            "Epoch 004: train_loss=0.399105 train_acc=0.9044 val_loss=0.385839 val_acc=0.8767\n",
            "Epoch 005: train_loss=0.314326 train_acc=0.9294 val_loss=0.355931 val_acc=0.8973\n",
            "Epoch 006: train_loss=0.248865 train_acc=0.9439 val_loss=0.316700 val_acc=0.8904\n",
            "Epoch 007: train_loss=0.196988 train_acc=0.9537 val_loss=0.292999 val_acc=0.9247\n",
            "Epoch 008: train_loss=0.165883 train_acc=0.9575 val_loss=0.271623 val_acc=0.9178\n",
            "Epoch 009: train_loss=0.131077 train_acc=0.9697 val_loss=0.259486 val_acc=0.9247\n",
            "Epoch 010: train_loss=0.108225 train_acc=0.9757 val_loss=0.246543 val_acc=0.9315\n",
            "Epoch 011: train_loss=0.090954 train_acc=0.9795 val_loss=0.227031 val_acc=0.9315\n",
            "Epoch 012: train_loss=0.082246 train_acc=0.9833 val_loss=0.215359 val_acc=0.9452\n",
            "Epoch 013: train_loss=0.064632 train_acc=0.9909 val_loss=0.215599 val_acc=0.9384\n",
            "Epoch 014: train_loss=0.049840 train_acc=0.9924 val_loss=0.216189 val_acc=0.9384\n",
            "Epoch 015: train_loss=0.042159 train_acc=0.9939 val_loss=0.213143 val_acc=0.9452\n",
            "Epoch 016: train_loss=0.035925 train_acc=0.9947 val_loss=0.205272 val_acc=0.9521\n",
            "Epoch 017: train_loss=0.031212 train_acc=0.9954 val_loss=0.214445 val_acc=0.9521\n",
            "Epoch 018: train_loss=0.027028 train_acc=0.9962 val_loss=0.224128 val_acc=0.9452\n",
            "Epoch 019: train_loss=0.023523 train_acc=0.9970 val_loss=0.212597 val_acc=0.9452\n",
            "Epoch 020: train_loss=0.020194 train_acc=0.9970 val_loss=0.213747 val_acc=0.9521\n",
            "Epoch 021: train_loss=0.018097 train_acc=0.9970 val_loss=0.219878 val_acc=0.9452\n",
            "Epoch 022: train_loss=0.015466 train_acc=0.9977 val_loss=0.229486 val_acc=0.9315\n",
            "Epoch 023: train_loss=0.014985 train_acc=0.9977 val_loss=0.222214 val_acc=0.9521\n",
            "Epoch 024: train_loss=0.012419 train_acc=0.9985 val_loss=0.225819 val_acc=0.9521\n",
            "Epoch 025: train_loss=0.011753 train_acc=0.9985 val_loss=0.227269 val_acc=0.9384\n",
            "Epoch 026: train_loss=0.010217 train_acc=0.9985 val_loss=0.234258 val_acc=0.9384\n",
            "Epoch 027: train_loss=0.009140 train_acc=0.9985 val_loss=0.234168 val_acc=0.9384\n",
            "Epoch 028: train_loss=0.008824 train_acc=0.9985 val_loss=0.246618 val_acc=0.9315\n",
            "Epoch 029: train_loss=0.007562 train_acc=0.9985 val_loss=0.242256 val_acc=0.9384\n",
            "Epoch 030: train_loss=0.006742 train_acc=0.9985 val_loss=0.246588 val_acc=0.9384\n",
            "Epoch 031: train_loss=0.005896 train_acc=1.0000 val_loss=0.252765 val_acc=0.9384\n",
            "Epoch 032: train_loss=0.005514 train_acc=1.0000 val_loss=0.252461 val_acc=0.9384\n",
            "Epoch 033: train_loss=0.006335 train_acc=1.0000 val_loss=0.258793 val_acc=0.9178\n",
            "Epoch 034: train_loss=0.011344 train_acc=0.9985 val_loss=0.501502 val_acc=0.8562\n",
            "Epoch 035: train_loss=0.184822 train_acc=0.9355 val_loss=0.325523 val_acc=0.9110\n",
            "Epoch 036: train_loss=0.124043 train_acc=0.9568 val_loss=0.330695 val_acc=0.8973\n",
            "Epoch 037: train_loss=0.044427 train_acc=0.9901 val_loss=0.294675 val_acc=0.9110\n",
            "Epoch 038: train_loss=0.021637 train_acc=0.9977 val_loss=0.308245 val_acc=0.9110\n",
            "Epoch 039: train_loss=0.014895 train_acc=0.9985 val_loss=0.320144 val_acc=0.9247\n",
            "Epoch 040: train_loss=0.008950 train_acc=1.0000 val_loss=0.314312 val_acc=0.9315\n",
            "Epoch 041: train_loss=0.007176 train_acc=1.0000 val_loss=0.320446 val_acc=0.9247\n",
            "Epoch 042: train_loss=0.006132 train_acc=1.0000 val_loss=0.322318 val_acc=0.9247\n",
            "Epoch 043: train_loss=0.005284 train_acc=1.0000 val_loss=0.319400 val_acc=0.9247\n",
            "Epoch 044: train_loss=0.004686 train_acc=1.0000 val_loss=0.325419 val_acc=0.9247\n",
            "Epoch 045: train_loss=0.004260 train_acc=1.0000 val_loss=0.326015 val_acc=0.9247\n",
            "Epoch 046: train_loss=0.003823 train_acc=1.0000 val_loss=0.327015 val_acc=0.9247\n",
            "Epoch 047: train_loss=0.003496 train_acc=1.0000 val_loss=0.327806 val_acc=0.9247\n",
            "Epoch 048: train_loss=0.003240 train_acc=1.0000 val_loss=0.329340 val_acc=0.9247\n",
            "Epoch 049: train_loss=0.002999 train_acc=1.0000 val_loss=0.329610 val_acc=0.9315\n",
            "Epoch 050: train_loss=0.002794 train_acc=1.0000 val_loss=0.331521 val_acc=0.9315\n",
            "Epoch 051: train_loss=0.002637 train_acc=1.0000 val_loss=0.334886 val_acc=0.9315\n",
            "Epoch 052: train_loss=0.002424 train_acc=1.0000 val_loss=0.337772 val_acc=0.9384\n",
            "Epoch 053: train_loss=0.002278 train_acc=1.0000 val_loss=0.337076 val_acc=0.9384\n",
            "Epoch 054: train_loss=0.002136 train_acc=1.0000 val_loss=0.338557 val_acc=0.9384\n",
            "Epoch 055: train_loss=0.002029 train_acc=1.0000 val_loss=0.340523 val_acc=0.9384\n",
            "Epoch 056: train_loss=0.001905 train_acc=1.0000 val_loss=0.341634 val_acc=0.9315\n",
            "Converged at epoch 57 (train_loss change 7.69e-05 < 0.0001)\n",
            "Train acc: 1.0000 | Test acc: 0.8798\n",
            "\n",
            "--- mfcc_cv | LSTM | layers=1 | hidden=128 ---\n",
            "Epoch 000: train_loss=1.259542 train_acc=0.4939 val_loss=0.930619 val_acc=0.6986\n",
            "Epoch 001: train_loss=0.741729 train_acc=0.7625 val_loss=0.480109 val_acc=0.8425\n",
            "Epoch 002: train_loss=0.450723 train_acc=0.8581 val_loss=0.343715 val_acc=0.8973\n",
            "Epoch 003: train_loss=0.293164 train_acc=0.9120 val_loss=0.310730 val_acc=0.8973\n",
            "Epoch 004: train_loss=0.194920 train_acc=0.9492 val_loss=0.257046 val_acc=0.9041\n",
            "Epoch 005: train_loss=0.138646 train_acc=0.9628 val_loss=0.241840 val_acc=0.9315\n",
            "Epoch 006: train_loss=0.096834 train_acc=0.9750 val_loss=0.208934 val_acc=0.9315\n",
            "Epoch 007: train_loss=0.080461 train_acc=0.9795 val_loss=0.217126 val_acc=0.9384\n",
            "Epoch 008: train_loss=0.055397 train_acc=0.9879 val_loss=0.223133 val_acc=0.9247\n",
            "Epoch 009: train_loss=0.042362 train_acc=0.9917 val_loss=0.191547 val_acc=0.9452\n",
            "Epoch 010: train_loss=0.048660 train_acc=0.9879 val_loss=0.269327 val_acc=0.9178\n",
            "Epoch 011: train_loss=0.054099 train_acc=0.9848 val_loss=0.250367 val_acc=0.9315\n",
            "Epoch 012: train_loss=0.033975 train_acc=0.9947 val_loss=0.162789 val_acc=0.9452\n",
            "Epoch 013: train_loss=0.018104 train_acc=0.9977 val_loss=0.169699 val_acc=0.9384\n",
            "Epoch 014: train_loss=0.011737 train_acc=0.9985 val_loss=0.183802 val_acc=0.9384\n",
            "Epoch 015: train_loss=0.008046 train_acc=0.9992 val_loss=0.177969 val_acc=0.9315\n",
            "Epoch 016: train_loss=0.006384 train_acc=1.0000 val_loss=0.186156 val_acc=0.9452\n",
            "Epoch 017: train_loss=0.005405 train_acc=1.0000 val_loss=0.185137 val_acc=0.9452\n",
            "Epoch 018: train_loss=0.004483 train_acc=1.0000 val_loss=0.192218 val_acc=0.9452\n",
            "Epoch 019: train_loss=0.003769 train_acc=1.0000 val_loss=0.186954 val_acc=0.9384\n",
            "Epoch 020: train_loss=0.003268 train_acc=1.0000 val_loss=0.185700 val_acc=0.9384\n",
            "Epoch 021: train_loss=0.002922 train_acc=1.0000 val_loss=0.182294 val_acc=0.9384\n",
            "Epoch 022: train_loss=0.002578 train_acc=1.0000 val_loss=0.188470 val_acc=0.9452\n",
            "Epoch 023: train_loss=0.002323 train_acc=1.0000 val_loss=0.183734 val_acc=0.9384\n",
            "Epoch 024: train_loss=0.002130 train_acc=1.0000 val_loss=0.178559 val_acc=0.9521\n",
            "Epoch 025: train_loss=0.001966 train_acc=1.0000 val_loss=0.188432 val_acc=0.9384\n",
            "Epoch 026: train_loss=0.001717 train_acc=1.0000 val_loss=0.191072 val_acc=0.9384\n",
            "Epoch 027: train_loss=0.001582 train_acc=1.0000 val_loss=0.190186 val_acc=0.9452\n",
            "Epoch 028: train_loss=0.001455 train_acc=1.0000 val_loss=0.189640 val_acc=0.9521\n",
            "Epoch 029: train_loss=0.001344 train_acc=1.0000 val_loss=0.187875 val_acc=0.9521\n",
            "Converged at epoch 30 (train_loss change 9.05e-05 < 0.0001)\n",
            "Train acc: 1.0000 | Test acc: 0.8934\n",
            "\n",
            "--- mfcc_cv | LSTM | layers=2 | hidden=64 ---\n",
            "Epoch 000: train_loss=1.323392 train_acc=0.4234 val_loss=0.953648 val_acc=0.6301\n",
            "Epoch 001: train_loss=0.757779 train_acc=0.7405 val_loss=0.512796 val_acc=0.8425\n",
            "Epoch 002: train_loss=0.451769 train_acc=0.8505 val_loss=0.473474 val_acc=0.8562\n",
            "Epoch 003: train_loss=0.341403 train_acc=0.8983 val_loss=0.365235 val_acc=0.9041\n",
            "Epoch 004: train_loss=0.262813 train_acc=0.9256 val_loss=0.278296 val_acc=0.9110\n",
            "Epoch 005: train_loss=0.191099 train_acc=0.9469 val_loss=0.279510 val_acc=0.9178\n",
            "Epoch 006: train_loss=0.155379 train_acc=0.9628 val_loss=0.256389 val_acc=0.9384\n",
            "Epoch 007: train_loss=0.151257 train_acc=0.9605 val_loss=0.268721 val_acc=0.9247\n",
            "Epoch 008: train_loss=0.117456 train_acc=0.9704 val_loss=0.375534 val_acc=0.8973\n",
            "Epoch 009: train_loss=0.091993 train_acc=0.9803 val_loss=0.341007 val_acc=0.9110\n",
            "Epoch 010: train_loss=0.104230 train_acc=0.9757 val_loss=0.338511 val_acc=0.9178\n",
            "Epoch 011: train_loss=0.067256 train_acc=0.9848 val_loss=0.395810 val_acc=0.9041\n",
            "Epoch 012: train_loss=0.083769 train_acc=0.9825 val_loss=0.330015 val_acc=0.9315\n",
            "Epoch 013: train_loss=0.070049 train_acc=0.9818 val_loss=0.297174 val_acc=0.9247\n",
            "Epoch 014: train_loss=0.073665 train_acc=0.9772 val_loss=0.307624 val_acc=0.9384\n",
            "Epoch 015: train_loss=0.067427 train_acc=0.9772 val_loss=0.380588 val_acc=0.9041\n",
            "Epoch 016: train_loss=0.031908 train_acc=0.9917 val_loss=0.428134 val_acc=0.9041\n",
            "Epoch 017: train_loss=0.040479 train_acc=0.9901 val_loss=0.396096 val_acc=0.9247\n",
            "Epoch 018: train_loss=0.022752 train_acc=0.9962 val_loss=0.383388 val_acc=0.9178\n",
            "Epoch 019: train_loss=0.020885 train_acc=0.9947 val_loss=0.400055 val_acc=0.9110\n",
            "Epoch 020: train_loss=0.013621 train_acc=0.9970 val_loss=0.355856 val_acc=0.9247\n",
            "Epoch 021: train_loss=0.009932 train_acc=0.9985 val_loss=0.388722 val_acc=0.9178\n",
            "Epoch 022: train_loss=0.007768 train_acc=0.9985 val_loss=0.378044 val_acc=0.9247\n",
            "Epoch 023: train_loss=0.005148 train_acc=0.9992 val_loss=0.410079 val_acc=0.9247\n",
            "Epoch 024: train_loss=0.004106 train_acc=1.0000 val_loss=0.418935 val_acc=0.9247\n",
            "Epoch 025: train_loss=0.003534 train_acc=1.0000 val_loss=0.417098 val_acc=0.9247\n",
            "Epoch 026: train_loss=0.002992 train_acc=1.0000 val_loss=0.424852 val_acc=0.9247\n",
            "Epoch 027: train_loss=0.002587 train_acc=1.0000 val_loss=0.427267 val_acc=0.9247\n",
            "Epoch 028: train_loss=0.002270 train_acc=1.0000 val_loss=0.431988 val_acc=0.9247\n",
            "Epoch 029: train_loss=0.002055 train_acc=1.0000 val_loss=0.436638 val_acc=0.9247\n",
            "Epoch 030: train_loss=0.001893 train_acc=1.0000 val_loss=0.442104 val_acc=0.9247\n",
            "Epoch 031: train_loss=0.001686 train_acc=1.0000 val_loss=0.448864 val_acc=0.9247\n",
            "Epoch 032: train_loss=0.001540 train_acc=1.0000 val_loss=0.447148 val_acc=0.9247\n",
            "Epoch 033: train_loss=0.001422 train_acc=1.0000 val_loss=0.453416 val_acc=0.9247\n",
            "Epoch 034: train_loss=0.001305 train_acc=1.0000 val_loss=0.454626 val_acc=0.9247\n",
            "Converged at epoch 35 (train_loss change 9.79e-05 < 0.0001)\n",
            "Train acc: 1.0000 | Test acc: 0.8989\n",
            "\n",
            "--- mfcc_cv | LSTM | layers=2 | hidden=128 ---\n",
            "Epoch 000: train_loss=1.161956 train_acc=0.5030 val_loss=0.668492 val_acc=0.7808\n",
            "Epoch 001: train_loss=0.587752 train_acc=0.7800 val_loss=0.377126 val_acc=0.8836\n",
            "Epoch 002: train_loss=0.358612 train_acc=0.8869 val_loss=0.285297 val_acc=0.9178\n",
            "Epoch 003: train_loss=0.258249 train_acc=0.9264 val_loss=0.273903 val_acc=0.9110\n",
            "Epoch 004: train_loss=0.204633 train_acc=0.9302 val_loss=0.331249 val_acc=0.9178\n",
            "Epoch 005: train_loss=0.125274 train_acc=0.9681 val_loss=0.424180 val_acc=0.8699\n",
            "Epoch 006: train_loss=0.115094 train_acc=0.9651 val_loss=0.275482 val_acc=0.9247\n",
            "Epoch 007: train_loss=0.067939 train_acc=0.9833 val_loss=0.255195 val_acc=0.9041\n",
            "Epoch 008: train_loss=0.077476 train_acc=0.9788 val_loss=0.200212 val_acc=0.9247\n",
            "Epoch 009: train_loss=0.060815 train_acc=0.9863 val_loss=0.230595 val_acc=0.9247\n",
            "Epoch 010: train_loss=0.046440 train_acc=0.9901 val_loss=0.211762 val_acc=0.9315\n",
            "Epoch 011: train_loss=0.029678 train_acc=0.9924 val_loss=0.168241 val_acc=0.9452\n",
            "Epoch 012: train_loss=0.024339 train_acc=0.9947 val_loss=0.225141 val_acc=0.9452\n",
            "Epoch 013: train_loss=0.089947 train_acc=0.9689 val_loss=0.249478 val_acc=0.9247\n",
            "Epoch 014: train_loss=0.070584 train_acc=0.9810 val_loss=0.231480 val_acc=0.9110\n",
            "Epoch 015: train_loss=0.033693 train_acc=0.9924 val_loss=0.155177 val_acc=0.9658\n",
            "Epoch 016: train_loss=0.016301 train_acc=0.9962 val_loss=0.217971 val_acc=0.9452\n",
            "Epoch 017: train_loss=0.011364 train_acc=0.9977 val_loss=0.170496 val_acc=0.9452\n",
            "Epoch 018: train_loss=0.007648 train_acc=0.9985 val_loss=0.134508 val_acc=0.9589\n",
            "Epoch 019: train_loss=0.011196 train_acc=0.9962 val_loss=0.133094 val_acc=0.9589\n",
            "Epoch 020: train_loss=0.004462 train_acc=0.9992 val_loss=0.178243 val_acc=0.9521\n",
            "Epoch 021: train_loss=0.002170 train_acc=1.0000 val_loss=0.148836 val_acc=0.9589\n",
            "Epoch 022: train_loss=0.001483 train_acc=1.0000 val_loss=0.157447 val_acc=0.9521\n",
            "Epoch 023: train_loss=0.001283 train_acc=1.0000 val_loss=0.165386 val_acc=0.9521\n",
            "Epoch 024: train_loss=0.001042 train_acc=1.0000 val_loss=0.175016 val_acc=0.9521\n",
            "Epoch 025: train_loss=0.000914 train_acc=1.0000 val_loss=0.180686 val_acc=0.9452\n",
            "Epoch 026: train_loss=0.000807 train_acc=1.0000 val_loss=0.189700 val_acc=0.9452\n",
            "Converged at epoch 27 (train_loss change 6.12e-05 < 0.0001)\n",
            "Train acc: 1.0000 | Test acc: 0.9071\n",
            "\n",
            "--- mfcc_cv | GRU | layers=1 | hidden=64 ---\n",
            "Epoch 000: train_loss=1.384137 train_acc=0.4135 val_loss=1.114200 val_acc=0.6370\n",
            "Epoch 001: train_loss=0.959104 train_acc=0.7223 val_loss=0.821841 val_acc=0.7877\n",
            "Epoch 002: train_loss=0.714366 train_acc=0.8118 val_loss=0.596043 val_acc=0.8288\n",
            "Epoch 003: train_loss=0.529015 train_acc=0.8574 val_loss=0.452534 val_acc=0.9041\n",
            "Epoch 004: train_loss=0.403950 train_acc=0.8900 val_loss=0.361816 val_acc=0.9041\n",
            "Epoch 005: train_loss=0.310939 train_acc=0.9127 val_loss=0.315549 val_acc=0.9110\n",
            "Epoch 006: train_loss=0.238783 train_acc=0.9378 val_loss=0.295214 val_acc=0.9178\n",
            "Epoch 007: train_loss=0.200771 train_acc=0.9537 val_loss=0.253296 val_acc=0.9178\n",
            "Epoch 008: train_loss=0.153359 train_acc=0.9605 val_loss=0.232198 val_acc=0.9178\n",
            "Epoch 009: train_loss=0.119755 train_acc=0.9772 val_loss=0.220909 val_acc=0.9247\n",
            "Epoch 010: train_loss=0.104090 train_acc=0.9772 val_loss=0.199499 val_acc=0.9452\n",
            "Epoch 011: train_loss=0.084519 train_acc=0.9803 val_loss=0.227410 val_acc=0.9315\n",
            "Epoch 012: train_loss=0.073854 train_acc=0.9871 val_loss=0.205993 val_acc=0.9384\n",
            "Epoch 013: train_loss=0.065424 train_acc=0.9871 val_loss=0.200277 val_acc=0.9384\n",
            "Epoch 014: train_loss=0.052017 train_acc=0.9917 val_loss=0.223861 val_acc=0.9110\n",
            "Epoch 015: train_loss=0.039497 train_acc=0.9932 val_loss=0.230169 val_acc=0.9178\n",
            "Epoch 016: train_loss=0.032263 train_acc=0.9947 val_loss=0.214169 val_acc=0.9247\n",
            "Epoch 017: train_loss=0.025710 train_acc=0.9947 val_loss=0.193514 val_acc=0.9384\n",
            "Epoch 018: train_loss=0.021739 train_acc=0.9954 val_loss=0.207621 val_acc=0.9384\n",
            "Epoch 019: train_loss=0.032911 train_acc=0.9909 val_loss=0.264947 val_acc=0.9178\n",
            "Epoch 020: train_loss=0.088612 train_acc=0.9727 val_loss=0.455510 val_acc=0.8562\n",
            "Epoch 021: train_loss=0.047387 train_acc=0.9901 val_loss=0.257949 val_acc=0.9384\n",
            "Epoch 022: train_loss=0.023028 train_acc=0.9970 val_loss=0.216375 val_acc=0.9452\n",
            "Epoch 023: train_loss=0.015407 train_acc=0.9985 val_loss=0.187721 val_acc=0.9658\n",
            "Epoch 024: train_loss=0.011966 train_acc=0.9992 val_loss=0.183423 val_acc=0.9726\n",
            "Epoch 025: train_loss=0.010028 train_acc=1.0000 val_loss=0.179033 val_acc=0.9726\n",
            "Epoch 026: train_loss=0.008600 train_acc=1.0000 val_loss=0.180586 val_acc=0.9726\n",
            "Epoch 027: train_loss=0.007387 train_acc=1.0000 val_loss=0.183328 val_acc=0.9726\n",
            "Epoch 028: train_loss=0.006594 train_acc=1.0000 val_loss=0.186971 val_acc=0.9452\n",
            "Epoch 029: train_loss=0.005835 train_acc=1.0000 val_loss=0.194368 val_acc=0.9452\n",
            "Epoch 030: train_loss=0.005261 train_acc=1.0000 val_loss=0.205948 val_acc=0.9452\n",
            "Epoch 031: train_loss=0.004702 train_acc=1.0000 val_loss=0.210505 val_acc=0.9452\n",
            "Epoch 032: train_loss=0.004232 train_acc=1.0000 val_loss=0.215011 val_acc=0.9452\n",
            "Epoch 033: train_loss=0.004088 train_acc=1.0000 val_loss=0.218202 val_acc=0.9452\n",
            "Epoch 034: train_loss=0.003521 train_acc=1.0000 val_loss=0.225710 val_acc=0.9384\n",
            "Epoch 035: train_loss=0.003300 train_acc=1.0000 val_loss=0.226703 val_acc=0.9384\n",
            "Epoch 036: train_loss=0.003169 train_acc=1.0000 val_loss=0.232821 val_acc=0.9384\n",
            "Epoch 037: train_loss=0.002787 train_acc=1.0000 val_loss=0.236320 val_acc=0.9384\n",
            "Epoch 038: train_loss=0.002658 train_acc=1.0000 val_loss=0.238723 val_acc=0.9384\n",
            "Epoch 039: train_loss=0.002444 train_acc=1.0000 val_loss=0.240154 val_acc=0.9384\n",
            "Converged at epoch 40 (train_loss change 7.48e-05 < 0.0001)\n",
            "Train acc: 1.0000 | Test acc: 0.8880\n",
            "\n",
            "--- mfcc_cv | GRU | layers=1 | hidden=128 ---\n",
            "Epoch 000: train_loss=1.146989 train_acc=0.5311 val_loss=0.845680 val_acc=0.7534\n",
            "Epoch 001: train_loss=0.684414 train_acc=0.7853 val_loss=0.534943 val_acc=0.8219\n",
            "Epoch 002: train_loss=0.429153 train_acc=0.8703 val_loss=0.329964 val_acc=0.9178\n",
            "Epoch 003: train_loss=0.298969 train_acc=0.9127 val_loss=0.273828 val_acc=0.9178\n",
            "Epoch 004: train_loss=0.213871 train_acc=0.9393 val_loss=0.237781 val_acc=0.9178\n",
            "Epoch 005: train_loss=0.168747 train_acc=0.9545 val_loss=0.197810 val_acc=0.9658\n",
            "Epoch 006: train_loss=0.164248 train_acc=0.9514 val_loss=0.211593 val_acc=0.9247\n",
            "Epoch 007: train_loss=0.093206 train_acc=0.9765 val_loss=0.174283 val_acc=0.9452\n",
            "Epoch 008: train_loss=0.063183 train_acc=0.9863 val_loss=0.197448 val_acc=0.9384\n",
            "Epoch 009: train_loss=0.045061 train_acc=0.9886 val_loss=0.165752 val_acc=0.9589\n",
            "Epoch 010: train_loss=0.032340 train_acc=0.9932 val_loss=0.138361 val_acc=0.9521\n",
            "Epoch 011: train_loss=0.022857 train_acc=0.9962 val_loss=0.196136 val_acc=0.9315\n",
            "Epoch 012: train_loss=0.023787 train_acc=0.9954 val_loss=0.259739 val_acc=0.9315\n",
            "Epoch 013: train_loss=0.027886 train_acc=0.9939 val_loss=0.202867 val_acc=0.9452\n",
            "Epoch 014: train_loss=0.015293 train_acc=0.9970 val_loss=0.207042 val_acc=0.9521\n",
            "Epoch 015: train_loss=0.008619 train_acc=1.0000 val_loss=0.155335 val_acc=0.9521\n",
            "Epoch 016: train_loss=0.005940 train_acc=1.0000 val_loss=0.186575 val_acc=0.9521\n",
            "Epoch 017: train_loss=0.004821 train_acc=1.0000 val_loss=0.174788 val_acc=0.9521\n",
            "Epoch 018: train_loss=0.004005 train_acc=1.0000 val_loss=0.173178 val_acc=0.9521\n",
            "Epoch 019: train_loss=0.003461 train_acc=1.0000 val_loss=0.179172 val_acc=0.9521\n",
            "Epoch 020: train_loss=0.002876 train_acc=1.0000 val_loss=0.188660 val_acc=0.9521\n",
            "Epoch 021: train_loss=0.002515 train_acc=1.0000 val_loss=0.186881 val_acc=0.9521\n",
            "Epoch 022: train_loss=0.002210 train_acc=1.0000 val_loss=0.193154 val_acc=0.9521\n",
            "Epoch 023: train_loss=0.002028 train_acc=1.0000 val_loss=0.197080 val_acc=0.9521\n",
            "Epoch 024: train_loss=0.001801 train_acc=1.0000 val_loss=0.192431 val_acc=0.9521\n",
            "Epoch 025: train_loss=0.001639 train_acc=1.0000 val_loss=0.200367 val_acc=0.9521\n",
            "Epoch 026: train_loss=0.001512 train_acc=1.0000 val_loss=0.201518 val_acc=0.9521\n",
            "Epoch 027: train_loss=0.001362 train_acc=1.0000 val_loss=0.207143 val_acc=0.9521\n",
            "Converged at epoch 28 (train_loss change 8.58e-05 < 0.0001)\n",
            "Train acc: 1.0000 | Test acc: 0.9016\n",
            "\n",
            "--- mfcc_cv | GRU | layers=2 | hidden=64 ---\n",
            "Epoch 000: train_loss=1.256609 train_acc=0.4552 val_loss=0.903379 val_acc=0.6986\n",
            "Epoch 001: train_loss=0.769582 train_acc=0.7564 val_loss=0.565070 val_acc=0.8425\n",
            "Epoch 002: train_loss=0.505647 train_acc=0.8308 val_loss=0.443910 val_acc=0.8288\n",
            "Epoch 003: train_loss=0.331819 train_acc=0.9006 val_loss=0.361497 val_acc=0.8767\n",
            "Epoch 004: train_loss=0.262027 train_acc=0.9234 val_loss=0.352316 val_acc=0.8904\n",
            "Epoch 005: train_loss=0.177994 train_acc=0.9446 val_loss=0.288474 val_acc=0.9247\n",
            "Epoch 006: train_loss=0.126427 train_acc=0.9636 val_loss=0.286322 val_acc=0.9110\n",
            "Epoch 007: train_loss=0.121482 train_acc=0.9674 val_loss=0.281344 val_acc=0.9247\n",
            "Epoch 008: train_loss=0.095800 train_acc=0.9765 val_loss=0.244178 val_acc=0.9315\n",
            "Epoch 009: train_loss=0.067768 train_acc=0.9848 val_loss=0.247085 val_acc=0.9247\n",
            "Epoch 010: train_loss=0.055013 train_acc=0.9894 val_loss=0.290640 val_acc=0.9178\n",
            "Epoch 011: train_loss=0.047336 train_acc=0.9879 val_loss=0.334447 val_acc=0.9110\n",
            "Epoch 012: train_loss=0.048693 train_acc=0.9879 val_loss=0.300745 val_acc=0.9247\n",
            "Epoch 013: train_loss=0.038239 train_acc=0.9924 val_loss=0.258846 val_acc=0.9178\n",
            "Epoch 014: train_loss=0.021538 train_acc=0.9977 val_loss=0.251364 val_acc=0.9247\n",
            "Epoch 015: train_loss=0.016128 train_acc=0.9985 val_loss=0.264596 val_acc=0.9315\n",
            "Epoch 016: train_loss=0.013702 train_acc=0.9985 val_loss=0.275868 val_acc=0.9315\n",
            "Epoch 017: train_loss=0.012150 train_acc=0.9985 val_loss=0.280081 val_acc=0.9247\n",
            "Epoch 018: train_loss=0.010277 train_acc=0.9985 val_loss=0.277317 val_acc=0.9315\n",
            "Epoch 019: train_loss=0.008772 train_acc=0.9985 val_loss=0.289284 val_acc=0.9315\n",
            "Epoch 020: train_loss=0.006564 train_acc=0.9992 val_loss=0.295748 val_acc=0.9315\n",
            "Epoch 021: train_loss=0.005212 train_acc=0.9992 val_loss=0.301958 val_acc=0.9315\n",
            "Epoch 022: train_loss=0.004217 train_acc=0.9992 val_loss=0.315883 val_acc=0.9247\n",
            "Epoch 023: train_loss=0.003153 train_acc=1.0000 val_loss=0.314677 val_acc=0.9247\n",
            "Epoch 024: train_loss=0.002448 train_acc=1.0000 val_loss=0.324666 val_acc=0.9247\n",
            "Epoch 025: train_loss=0.002018 train_acc=1.0000 val_loss=0.333784 val_acc=0.9315\n",
            "Epoch 026: train_loss=0.001766 train_acc=1.0000 val_loss=0.342238 val_acc=0.9315\n",
            "Epoch 027: train_loss=0.001575 train_acc=1.0000 val_loss=0.348284 val_acc=0.9315\n",
            "Epoch 028: train_loss=0.001463 train_acc=1.0000 val_loss=0.358292 val_acc=0.9315\n",
            "Epoch 029: train_loss=0.001295 train_acc=1.0000 val_loss=0.371545 val_acc=0.9315\n",
            "Converged at epoch 30 (train_loss change 8.74e-05 < 0.0001)\n",
            "Train acc: 1.0000 | Test acc: 0.8852\n",
            "\n",
            "--- mfcc_cv | GRU | layers=2 | hidden=128 ---\n",
            "Epoch 000: train_loss=1.006039 train_acc=0.6343 val_loss=0.541306 val_acc=0.8356\n",
            "Epoch 001: train_loss=0.473313 train_acc=0.8346 val_loss=0.351497 val_acc=0.8904\n",
            "Epoch 002: train_loss=0.324929 train_acc=0.8945 val_loss=0.235199 val_acc=0.9384\n",
            "Epoch 003: train_loss=0.229184 train_acc=0.9249 val_loss=0.258301 val_acc=0.8973\n",
            "Epoch 004: train_loss=0.144422 train_acc=0.9590 val_loss=0.196460 val_acc=0.9178\n",
            "Epoch 005: train_loss=0.131740 train_acc=0.9545 val_loss=0.245883 val_acc=0.8973\n",
            "Epoch 006: train_loss=0.094043 train_acc=0.9727 val_loss=0.212168 val_acc=0.9178\n",
            "Epoch 007: train_loss=0.057351 train_acc=0.9841 val_loss=0.217682 val_acc=0.9452\n",
            "Epoch 008: train_loss=0.051579 train_acc=0.9848 val_loss=0.228919 val_acc=0.9178\n",
            "Epoch 009: train_loss=0.049666 train_acc=0.9833 val_loss=0.167755 val_acc=0.9452\n",
            "Epoch 010: train_loss=0.020876 train_acc=0.9954 val_loss=0.153650 val_acc=0.9589\n",
            "Epoch 011: train_loss=0.008430 train_acc=1.0000 val_loss=0.154627 val_acc=0.9726\n",
            "Epoch 012: train_loss=0.004042 train_acc=1.0000 val_loss=0.177803 val_acc=0.9658\n",
            "Epoch 013: train_loss=0.002315 train_acc=1.0000 val_loss=0.177253 val_acc=0.9726\n",
            "Epoch 014: train_loss=0.001788 train_acc=1.0000 val_loss=0.184322 val_acc=0.9658\n",
            "Epoch 015: train_loss=0.001482 train_acc=1.0000 val_loss=0.187649 val_acc=0.9726\n",
            "Epoch 016: train_loss=0.001275 train_acc=1.0000 val_loss=0.190477 val_acc=0.9726\n",
            "Epoch 017: train_loss=0.001111 train_acc=1.0000 val_loss=0.194500 val_acc=0.9726\n",
            "Epoch 018: train_loss=0.000982 train_acc=1.0000 val_loss=0.197818 val_acc=0.9726\n",
            "Converged at epoch 19 (train_loss change 9.52e-05 < 0.0001)\n",
            "Train acc: 1.0000 | Test acc: 0.9044\n",
            "\n",
            "Saved results table to /content/results/mfcc_cv_results_table.csv\n",
            "dataset model  layers  hidden  train_acc  test_acc\n",
            "mfcc_cv   RNN       1      64   0.998483  0.751366\n",
            "mfcc_cv   RNN       1     128   1.000000  0.792350\n",
            "mfcc_cv   RNN       2      64   1.000000  0.819672\n",
            "mfcc_cv   RNN       2     128   1.000000  0.814208\n",
            "mfcc_cv  LSTM       1      64   1.000000  0.879781\n",
            "mfcc_cv  LSTM       1     128   1.000000  0.893443\n",
            "mfcc_cv  LSTM       2      64   1.000000  0.898907\n",
            "mfcc_cv  LSTM       2     128   1.000000  0.907104\n",
            "mfcc_cv   GRU       1      64   1.000000  0.887978\n",
            "mfcc_cv   GRU       1     128   1.000000  0.901639\n",
            "mfcc_cv   GRU       2      64   1.000000  0.885246\n",
            "mfcc_cv   GRU       2     128   1.000000  0.904372\n",
            "\n",
            "All done. Results saved in /content/results\n",
            "\n",
            "Summary for handwritten:\n",
            "    dataset model  layers  hidden  train_acc  test_acc\n",
            "handwritten   RNN       1      64   0.598071      0.62\n",
            "handwritten   RNN       1     128   0.655949      0.65\n",
            "handwritten   RNN       2      64   0.713826      0.63\n",
            "handwritten   RNN       2     128   0.691318      0.64\n",
            "handwritten  LSTM       1      64   0.858521      0.86\n",
            "handwritten  LSTM       1     128   0.897106      0.88\n",
            "handwritten  LSTM       2      64   0.935691      0.90\n",
            "handwritten  LSTM       2     128   0.890675      0.84\n",
            "handwritten   GRU       1      64   0.971061      0.97\n",
            "handwritten   GRU       1     128   0.781350      0.84\n",
            "handwritten   GRU       2      64   0.607717      0.63\n",
            "handwritten   GRU       2     128   1.000000      0.99\n",
            "\n",
            "Summary for mfcc_cv:\n",
            "dataset model  layers  hidden  train_acc  test_acc\n",
            "mfcc_cv   RNN       1      64   0.998483  0.751366\n",
            "mfcc_cv   RNN       1     128   1.000000  0.792350\n",
            "mfcc_cv   RNN       2      64   1.000000  0.819672\n",
            "mfcc_cv   RNN       2     128   1.000000  0.814208\n",
            "mfcc_cv  LSTM       1      64   1.000000  0.879781\n",
            "mfcc_cv  LSTM       1     128   1.000000  0.893443\n",
            "mfcc_cv  LSTM       2      64   1.000000  0.898907\n",
            "mfcc_cv  LSTM       2     128   1.000000  0.907104\n",
            "mfcc_cv   GRU       1      64   1.000000  0.887978\n",
            "mfcc_cv   GRU       1     128   1.000000  0.901639\n",
            "mfcc_cv   GRU       2      64   1.000000  0.885246\n",
            "mfcc_cv   GRU       2     128   1.000000  0.904372\n"
          ]
        }
      ],
      "source": [
        "# Monolithic Colab script: RNN / LSTM / GRU comparison for both datasets\n",
        "# Paste and run in Colab after uploading Data_Assignment4.zip to /content\n",
        "\n",
        "import os, zipfile, math, itertools, sys\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "# ----------------- Settings -----------------\n",
        "ZIP_PATH = \"/content/Data_Assignment4.zip\"\n",
        "EXTRACT_TO = \"/content/assignment4_data\"\n",
        "RESULTS_DIR = \"/content/results\"\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Hyperparameter grid (small  adjust if you want more exploration)\n",
        "MODEL_TYPES = [\"RNN\", \"LSTM\", \"GRU\"]\n",
        "NUM_LAYERS_LIST = [1, 2]\n",
        "HIDDEN_SIZES = [64, 128]\n",
        "LR = 1e-3\n",
        "BATCH_SIZE = 32\n",
        "MAX_EPOCHS = 200\n",
        "CONV_THRESH = 1e-4\n",
        "\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "# ----------------- Helpers: unzip and inspect -----------------\n",
        "def extract_zip(zip_path, extract_to=EXTRACT_TO):\n",
        "    if not os.path.exists(zip_path):\n",
        "        raise FileNotFoundError(f\"{zip_path} not found. Upload Data_Assignment4.zip to /content.\")\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
        "        z.extractall(extract_to)\n",
        "    print(\"Extracted\", zip_path, \"->\", extract_to)\n",
        "    return extract_to\n",
        "\n",
        "# ----------------- Data parsing -----------------\n",
        "def read_handwritten_txt(path):\n",
        "    # returns np.array(shape=(n_points,2)) or None\n",
        "    try:\n",
        "        vals = [float(x) for x in open(path, \"r\").read().split()]\n",
        "        if len(vals) < 3:\n",
        "            return None\n",
        "        n = int(vals[0])\n",
        "        coords = np.array(vals[1:])\n",
        "        # ensure even count\n",
        "        if coords.size < 2:\n",
        "            return None\n",
        "        coords = coords[: (coords.size // 2) * 2 ]\n",
        "        coords = coords.reshape(-1, 2).astype(np.float32)\n",
        "        # normalize per-sample to 0-1 for x and y\n",
        "        if coords.shape[0] > 0:\n",
        "            x = coords[:,0]; y = coords[:,1]\n",
        "            if x.max() > x.min(): x = (x - x.min()) / (x.max() - x.min())\n",
        "            else: x = np.zeros_like(x)\n",
        "            if y.max() > y.min(): y = (y - y.min()) / (y.max() - y.min())\n",
        "            else: y = np.zeros_like(y)\n",
        "            coords = np.stack([x,y], axis=1)\n",
        "        return coords\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def read_mfcc_file(path):\n",
        "    try:\n",
        "        arr = np.loadtxt(path)\n",
        "        if arr.ndim == 1:\n",
        "            arr = arr.reshape(-1, arr.shape[0]) if arr.size>0 else None\n",
        "        return arr.astype(np.float32) if arr is not None else None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def find_subfolder_case_insensitive(root, target_name):\n",
        "    # returns path to first folder name matching target_name case-insensitive, else None\n",
        "    for entry in os.listdir(root):\n",
        "        p = os.path.join(root, entry)\n",
        "        if os.path.isdir(p) and entry.lower() == target_name.lower():\n",
        "            return p\n",
        "    return None\n",
        "\n",
        "def collect_handwritten_class_samples(class_root):\n",
        "    # class_root may contain nested subfolders like class/class/train and class/class/dev\n",
        "    train_samples = []\n",
        "    test_samples = []\n",
        "    bad = 0\n",
        "    # find any subdirs named 'train' and 'dev' (case-insensitive) anywhere under class_root\n",
        "    for dirpath, dirs, files in os.walk(class_root):\n",
        "        for fname in files:\n",
        "            if not fname.lower().endswith(\".txt\"):\n",
        "                continue\n",
        "            full = os.path.join(dirpath, fname)\n",
        "            # determine if file is under a train or dev/test folder by path\n",
        "            lower = dirpath.lower()\n",
        "            if os.sep + \"train\" + os.sep in os.sep + lower + os.sep or lower.endswith(os.sep + \"train\") or lower.endswith(\"train\"):\n",
        "                sample = read_handwritten_txt(full)\n",
        "                if sample is not None:\n",
        "                    train_samples.append(sample)\n",
        "                else:\n",
        "                    bad += 1\n",
        "            elif os.sep + \"dev\" + os.sep in os.sep + lower + os.sep or lower.endswith(os.sep + \"dev\") or lower.endswith(\"dev\") or \"dev\" in os.path.basename(dirpath).lower():\n",
        "                sample = read_handwritten_txt(full)\n",
        "                if sample is not None:\n",
        "                    test_samples.append(sample)\n",
        "                else:\n",
        "                    bad += 1\n",
        "            else:\n",
        "                # if we don't know, treat as train by default\n",
        "                sample = read_handwritten_txt(full)\n",
        "                if sample is not None:\n",
        "                    train_samples.append(sample)\n",
        "                else:\n",
        "                    bad += 1\n",
        "    return train_samples, test_samples, bad\n",
        "\n",
        "def collect_mfcc_class_samples(class_root):\n",
        "    # expects Train/Train and Test/Test or Train/ and Test/ under nested folder\n",
        "    train_samples = []\n",
        "    test_samples = []\n",
        "    bad = 0\n",
        "    for dirpath, dirs, files in os.walk(class_root):\n",
        "        for fname in files:\n",
        "            if not (fname.lower().endswith(\".mfcc\") or fname.lower().endswith(\".txt\")):\n",
        "                continue\n",
        "            full = os.path.join(dirpath, fname)\n",
        "            lower = dirpath.lower()\n",
        "            if \"train\" in os.path.basename(lower) or os.sep + \"train\" + os.sep in os.sep + lower + os.sep:\n",
        "                arr = read_mfcc_file(full)\n",
        "                if arr is not None:\n",
        "                    train_samples.append(arr)\n",
        "                else:\n",
        "                    bad += 1\n",
        "            elif \"test\" in os.path.basename(lower) or os.sep + \"test\" + os.sep in os.sep + lower + os.sep:\n",
        "                arr = read_mfcc_file(full)\n",
        "                if arr is not None:\n",
        "                    test_samples.append(arr)\n",
        "                else:\n",
        "                    bad += 1\n",
        "            else:\n",
        "                # default: include in train\n",
        "                arr = read_mfcc_file(full)\n",
        "                if arr is not None:\n",
        "                    train_samples.append(arr)\n",
        "                else:\n",
        "                    bad += 1\n",
        "    return train_samples, test_samples, bad\n",
        "\n",
        "# ----------------- Build datasets -----------------\n",
        "def load_handwritten_dataset(root_handwriting):\n",
        "    classes = sorted([d for d in os.listdir(root_handwriting) if os.path.isdir(os.path.join(root_handwriting, d)) and not d.startswith(\"_\")])\n",
        "    all_train = []\n",
        "    all_test = []\n",
        "    labels_train = []\n",
        "    labels_test = []\n",
        "    class_map = {}\n",
        "    for idx, cname in enumerate(classes):\n",
        "        # class folder structure may be cname/cname/train\n",
        "        class_path = os.path.join(root_handwriting, cname)\n",
        "        # sometimes the actual folder is nested one level deeper (e.g., a/a/)\n",
        "        # try to detect nested folder with same name\n",
        "        nested_same = os.path.join(class_path, cname)\n",
        "        if os.path.isdir(nested_same):\n",
        "            use_root = nested_same\n",
        "        else:\n",
        "            use_root = class_path\n",
        "        train_s, test_s, bad = collect_handwritten_class_samples(use_root)\n",
        "        if len(train_s) + len(test_s) == 0:\n",
        "            continue\n",
        "        class_map[idx] = cname\n",
        "        all_train.extend(train_s)\n",
        "        labels_train.extend([idx]*len(train_s))\n",
        "        all_test.extend(test_s)\n",
        "        labels_test.extend([idx]*len(test_s))\n",
        "        print(f\"Loaded {len(train_s)} train and {len(test_s)} test samples for class '{cname}' ({bad} bad files).\")\n",
        "    return all_train, labels_train, all_test, labels_test, class_map\n",
        "\n",
        "def load_mfcc_dataset(root_cv):\n",
        "    classes = sorted([d for d in os.listdir(root_cv) if os.path.isdir(os.path.join(root_cv, d))])\n",
        "    all_train = []\n",
        "    all_test = []\n",
        "    labels_train = []\n",
        "    labels_test = []\n",
        "    class_map = {}\n",
        "    for idx, cname in enumerate(classes):\n",
        "        class_path = os.path.join(root_cv, cname)\n",
        "        nested_same = os.path.join(class_path, cname)\n",
        "        use_root = nested_same if os.path.isdir(nested_same) else class_path\n",
        "        train_s, test_s, bad = collect_mfcc_class_samples(use_root)\n",
        "        if len(train_s) + len(test_s) == 0:\n",
        "            continue\n",
        "        class_map[idx] = cname\n",
        "        all_train.extend(train_s)\n",
        "        labels_train.extend([idx]*len(train_s))\n",
        "        all_test.extend(test_s)\n",
        "        labels_test.extend([idx]*len(test_s))\n",
        "        print(f\"Loaded {len(train_s)} train and {len(test_s)} test samples for class '{cname}' ({bad} bad files).\")\n",
        "    return all_train, labels_train, all_test, labels_test, class_map\n",
        "\n",
        "# ----------------- PyTorch dataset/collate -----------------\n",
        "class SeqDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "    def __len__(self): return len(self.sequences)\n",
        "    def __getitem__(self, idx): return torch.from_numpy(self.sequences[idx]), torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "\n",
        "def collate_pad(batch):\n",
        "    # batch: list of (tensor_seq, label)\n",
        "    batch = sorted(batch, key=lambda x: x[0].shape[0], reverse=True)\n",
        "    seqs = [item[0] for item in batch]\n",
        "    labels = torch.stack([item[1] for item in batch])\n",
        "    lengths = torch.tensor([s.shape[0] for s in seqs], dtype=torch.long)\n",
        "    padded = pad_sequence(seqs, batch_first=True)  # (B, T, D)\n",
        "    return padded, labels, lengths\n",
        "\n",
        "# ----------------- Model -----------------\n",
        "class SequenceClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes, model_type=\"LSTM\"):\n",
        "        super().__init__()\n",
        "        model_type = model_type.upper()\n",
        "        self.model_type = model_type\n",
        "        if model_type == \"RNN\":\n",
        "            self.rnn = nn.RNN(input_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "        elif model_type == \"GRU\":\n",
        "            self.rnn = nn.GRU(input_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "        else:\n",
        "            self.rnn = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "    def forward(self, x, lengths):\n",
        "        # x: (B, T, D), lengths: (B,)\n",
        "        packed = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=True)\n",
        "        packed_out, hidden = self.rnn(packed)\n",
        "        if self.model_type == \"LSTM\":\n",
        "            h_n, c_n = hidden\n",
        "            last = h_n[-1]   # (B, H)\n",
        "        else:\n",
        "            # hidden is (num_layers, B, H)\n",
        "            last = hidden[-1]\n",
        "        # last: (B, H)\n",
        "        logits = self.fc(last)\n",
        "        return logits\n",
        "\n",
        "# ----------------- Training & evaluation -----------------\n",
        "def train_with_earlystop(model, train_loader, val_loader, device, lr=LR, max_epochs=MAX_EPOCHS, conv_thresh=CONV_THRESH):\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    prev_avg = None\n",
        "    history = {\"train_loss\":[], \"val_loss\":[], \"train_acc\":[], \"val_acc\":[]}\n",
        "    for epoch in range(max_epochs):\n",
        "        model.train()\n",
        "        losses = []\n",
        "        preds = []\n",
        "        trues = []\n",
        "        for xb, yb, lengths in train_loader:\n",
        "            xb, yb, lengths = xb.to(device), yb.to(device), lengths.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(xb, lengths)\n",
        "            loss = criterion(logits, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            losses.append(loss.item())\n",
        "            preds.extend(torch.argmax(logits, dim=1).detach().cpu().numpy().tolist())\n",
        "            trues.extend(yb.detach().cpu().numpy().tolist())\n",
        "        avg_train_loss = np.mean(losses) if losses else 0.0\n",
        "        train_acc = accuracy_score(trues, preds) if preds else 0.0\n",
        "\n",
        "        # validation\n",
        "        model.eval()\n",
        "        vlosses = []\n",
        "        vpreds = []\n",
        "        vtrues = []\n",
        "        with torch.no_grad():\n",
        "            for xb, yb, lengths in val_loader:\n",
        "                xb, yb, lengths = xb.to(device), yb.to(device), lengths.to(device)\n",
        "                logits = model(xb, lengths)\n",
        "                loss = criterion(logits, yb)\n",
        "                vlosses.append(loss.item())\n",
        "                vpreds.extend(torch.argmax(logits, dim=1).detach().cpu().numpy().tolist())\n",
        "                vtrues.extend(yb.detach().cpu().numpy().tolist())\n",
        "        avg_val_loss = np.mean(vlosses) if vlosses else 0.0\n",
        "        val_acc = accuracy_score(vtrues, vpreds) if vpreds else 0.0\n",
        "\n",
        "        history[\"train_loss\"].append(avg_train_loss)\n",
        "        history[\"val_loss\"].append(avg_val_loss)\n",
        "        history[\"train_acc\"].append(train_acc)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "        if prev_avg is not None and abs(prev_avg - avg_train_loss) < conv_thresh:\n",
        "            print(f\"Converged at epoch {epoch} (train_loss change {abs(prev_avg-avg_train_loss):.2e} < {conv_thresh})\")\n",
        "            break\n",
        "        prev_avg = avg_train_loss\n",
        "        print(f\"Epoch {epoch:03d}: train_loss={avg_train_loss:.6f} train_acc={train_acc:.4f} val_loss={avg_val_loss:.6f} val_acc={val_acc:.4f}\")\n",
        "    return model, history\n",
        "\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_trues = []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb, lengths in loader:\n",
        "            xb, yb, lengths = xb.to(device), yb.to(device), lengths.to(device)\n",
        "            logits = model(xb, lengths)\n",
        "            preds = torch.argmax(logits, dim=1).cpu().numpy().tolist()\n",
        "            all_preds.extend(preds)\n",
        "            all_trues.extend(yb.cpu().numpy().tolist())\n",
        "    acc = accuracy_score(all_trues, all_preds) if all_trues else 0.0\n",
        "    cm = confusion_matrix(all_trues, all_preds) if all_trues else None\n",
        "    return acc, cm, all_trues, all_preds\n",
        "\n",
        "# ----------------- Runner for a dataset -----------------\n",
        "def run_on_dataset(dataset_name, train_X, train_y, test_X, test_y, class_map, input_dim):\n",
        "    print(f\"\\n=== Running on dataset: {dataset_name} ===\")\n",
        "    # If there is no explicit test set for dev files, we will use provided test arrays.\n",
        "    # We also will split a small validation set from train for early stopping/val metrics if possible.\n",
        "    results_rows = []\n",
        "    # create PyTorch datasets\n",
        "    # Split train into train/val (90/10) if there are enough samples\n",
        "    n_total = len(train_X)\n",
        "    n_val = max(1, int(0.1 * n_total)) if n_total > 10 else 0\n",
        "    if n_val > 0:\n",
        "        # simple stratified-like split by class indices ensuring at least some samples per class if possible\n",
        "        # For simplicity, random split\n",
        "        idxs = np.arange(n_total)\n",
        "        np.random.shuffle(idxs)\n",
        "        val_idx = idxs[:n_val]\n",
        "        tr_idx = idxs[n_val:]\n",
        "        tr_X = [train_X[i] for i in tr_idx]\n",
        "        tr_y = [train_y[i] for i in tr_idx]\n",
        "        val_X = [train_X[i] for i in val_idx]\n",
        "        val_y = [train_y[i] for i in val_idx]\n",
        "    else:\n",
        "        tr_X, tr_y = train_X, train_y\n",
        "        val_X, val_y = [], []\n",
        "\n",
        "    # Create loaders\n",
        "    train_ds = SeqDataset(tr_X, tr_y)\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_pad)\n",
        "    val_loader = DataLoader(SeqDataset(val_X, val_y), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_pad) if len(val_X)>0 else DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_pad)\n",
        "    test_ds = SeqDataset(test_X, test_y)\n",
        "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_pad)\n",
        "\n",
        "    for model_type, num_layers, hidden_dim in itertools.product(MODEL_TYPES, NUM_LAYERS_LIST, HIDDEN_SIZES):\n",
        "        print(f\"\\n--- {dataset_name} | {model_type} | layers={num_layers} | hidden={hidden_dim} ---\")\n",
        "        num_classes = len(class_map)\n",
        "        model = SequenceClassifier(input_dim=input_dim, hidden_dim=hidden_dim, num_layers=num_layers, num_classes=num_classes, model_type=model_type)\n",
        "        model, history = train_with_earlystop(model, train_loader, val_loader, DEVICE, lr=LR, max_epochs=MAX_EPOCHS, conv_thresh=CONV_THRESH)\n",
        "        # Evaluate on train and test\n",
        "        train_acc, train_cm, _, _ = evaluate(model, train_loader, DEVICE)\n",
        "        test_acc, test_cm, trues, preds = evaluate(model, test_loader, DEVICE)\n",
        "\n",
        "        # Save loss plot\n",
        "        plt.figure(); plt.plot(history[\"train_loss\"], label=\"train_loss\"); plt.plot(history[\"val_loss\"], label=\"val_loss\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(f\"{dataset_name}-{model_type}-loss\")\n",
        "        plt.legend(); plt.tight_layout()\n",
        "        plt.savefig(os.path.join(RESULTS_DIR, f\"{dataset_name}_{model_type}_layers{num_layers}_h{hidden_dim}_loss.png\")); plt.close()\n",
        "\n",
        "        # Save confusion matrix plot for test\n",
        "        if test_cm is not None:\n",
        "            plt.figure(figsize=(6,6))\n",
        "            plt.imshow(test_cm, interpolation='nearest')\n",
        "            plt.title(f\"{dataset_name} {model_type} test confusion\")\n",
        "            plt.colorbar()\n",
        "            tick_marks = np.arange(len(class_map))\n",
        "            plt.xticks(tick_marks, [class_map[k] for k in range(len(class_map))], rotation=45)\n",
        "            plt.yticks(tick_marks, [class_map[k] for k in range(len(class_map))])\n",
        "            plt.ylabel(\"True\"); plt.xlabel(\"Pred\")\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(RESULTS_DIR, f\"{dataset_name}_{model_type}_layers{num_layers}_h{hidden_dim}_confusion.png\"))\n",
        "            plt.close()\n",
        "\n",
        "        results_rows.append({\n",
        "            \"dataset\": dataset_name,\n",
        "            \"model\": model_type,\n",
        "            \"layers\": num_layers,\n",
        "            \"hidden\": hidden_dim,\n",
        "            \"train_acc\": train_acc,\n",
        "            \"test_acc\": test_acc\n",
        "        })\n",
        "        print(f\"Train acc: {train_acc:.4f} | Test acc: {test_acc:.4f}\")\n",
        "\n",
        "    df = pd.DataFrame(results_rows)\n",
        "    # Save summary CSV\n",
        "    df.to_csv(os.path.join(RESULTS_DIR, f\"{dataset_name}_results_table.csv\"), index=False)\n",
        "    print(f\"\\nSaved results table to {os.path.join(RESULTS_DIR, f'{dataset_name}_results_table.csv')}\")\n",
        "    print(df.to_string(index=False))\n",
        "    return df\n",
        "\n",
        "# ----------------- Main -----------------\n",
        "def main():\n",
        "    extract_zip(ZIP_PATH, EXTRACT_TO)\n",
        "    # detect roots based on your described structure:\n",
        "    base = os.path.join(EXTRACT_TO, \"Data_Assignment4\", \"Data_Assignment4\")\n",
        "    if not os.path.isdir(base):\n",
        "        # fallback: take top-level under extract_to\n",
        "        base = EXTRACT_TO\n",
        "    handwriting_root = os.path.join(base, \"Hand_writing_Data\")\n",
        "    cv_root = os.path.join(base, \"CV_DATA\")\n",
        "    print(\"Detected handwritten_root:\", handwriting_root)\n",
        "    print(\"Detected mfcc_root:\", cv_root)\n",
        "\n",
        "    overall_tables = {}\n",
        "\n",
        "    # ---------- Handwritten dataset ----------\n",
        "    if os.path.isdir(handwriting_root):\n",
        "        print(\"\\nProcessing Handwritten dataset...\")\n",
        "        train_X, train_y, test_X, test_y, class_map = load_handwritten_dataset(handwriting_root)\n",
        "        if len(train_X) + len(test_X) == 0:\n",
        "            print(\"No handwritten samples found. Skipping.\")\n",
        "        else:\n",
        "            print(f\"Total handwriting samples: train={len(train_X)} test={len(test_X)} classes={len(class_map)}\")\n",
        "            # input_dim = 2 for stroke coordinates\n",
        "            df_hand = run_on_dataset(\"handwritten\", train_X, train_y, test_X, test_y, class_map, input_dim=2)\n",
        "            overall_tables[\"handwritten\"] = df_hand\n",
        "\n",
        "    # ---------- MFCC dataset ----------\n",
        "    if os.path.isdir(cv_root):\n",
        "        print(\"\\nProcessing MFCC (CV) dataset...\")\n",
        "        train_X, train_y, test_X, test_y, class_map_cv = load_mfcc_dataset(cv_root)\n",
        "        if len(train_X) + len(test_X) == 0:\n",
        "            print(\"No MFCC samples found. Skipping.\")\n",
        "        else:\n",
        "            # detect input dim from first sample\n",
        "            input_dim = train_X[0].shape[1] if len(train_X)>0 and train_X[0].ndim==2 else (test_X[0].shape[1] if len(test_X)>0 and test_X[0].ndim==2 else 39)\n",
        "            print(f\"Total MFCC samples: train={len(train_X)} test={len(test_X)} classes={len(class_map_cv)} input_dim={input_dim}\")\n",
        "            df_mfcc = run_on_dataset(\"mfcc_cv\", train_X, train_y, test_X, test_y, class_map_cv, input_dim=input_dim)\n",
        "            overall_tables[\"mfcc_cv\"] = df_mfcc\n",
        "\n",
        "    print(\"\\nAll done. Results saved in\", RESULTS_DIR)\n",
        "    # print summary tables\n",
        "    for k,v in overall_tables.items():\n",
        "        print(f\"\\nSummary for {k}:\")\n",
        "        print(v.to_string(index=False))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== Enhanced Visualization Cell =====================\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import os\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import torch\n",
        "\n",
        "RESULTS_DIR = \"/content/results\"\n",
        "PLOT_DIR = os.path.join(RESULTS_DIR, \"plots\")\n",
        "os.makedirs(PLOT_DIR, exist_ok=True)\n",
        "\n",
        "# Load saved results\n",
        "df_hand = pd.read_csv(os.path.join(RESULTS_DIR, \"handwritten_results_table.csv\"))\n",
        "df_mfcc = pd.read_csv(os.path.join(RESULTS_DIR, \"mfcc_cv_results_table.csv\"))\n",
        "\n",
        "# ================== (a) Combined Training Error vs Epochs ==================\n",
        "def plot_combined_training_error(dataset_name, results_dir=RESULTS_DIR):\n",
        "    \"\"\"\n",
        "    Creates a single plot showing average training error vs epochs\n",
        "    for all hyperparameter combinations in the dataset.\n",
        "    \"\"\"\n",
        "    pattern = f\"{dataset_name}_.*_loss_curve.csv\"\n",
        "    loss_files = [f for f in os.listdir(results_dir) if f.endswith(\"_loss_curve.csv\") and f.startswith(dataset_name)]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    if not loss_files:\n",
        "        print(f\"No loss curves found for {dataset_name}\")\n",
        "        return\n",
        "\n",
        "    for fname in loss_files:\n",
        "        path = os.path.join(results_dir, fname)\n",
        "        df_loss = pd.read_csv(path)\n",
        "        model_info = fname.replace(\".csv\", \"\").split(\"_\")\n",
        "        # example fname: handwritten_LSTM_layers2_h128_loss_curve.csv\n",
        "        try:\n",
        "            model_type = model_info[1]\n",
        "            layers = [s for s in model_info if \"layers\" in s][0].replace(\"layers\", \"\")\n",
        "            hidden = [s for s in model_info if \"h\" in s and \"loss\" not in s][0].replace(\"h\", \"\")\n",
        "        except:\n",
        "            model_type, layers, hidden = \"Unknown\", \"?\", \"?\"\n",
        "\n",
        "        plt.plot(df_loss[\"epoch\"], df_loss[\"train_loss\"],\n",
        "                 label=f\"{model_type} (L{layers}, H{hidden})\", linewidth=1.8)\n",
        "\n",
        "    plt.title(f\"{dataset_name.upper()}  Average Training Error vs Epochs (All Hyperparameters)\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Average Training Error (Loss)\")\n",
        "    plt.legend(fontsize=8, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    save_path = os.path.join(PLOT_DIR, f\"{dataset_name}_combined_training_error.png\")\n",
        "    plt.savefig(save_path, bbox_inches=\"tight\", dpi=150)\n",
        "    plt.close()\n",
        "    print(f\" Saved combined training error plot for {dataset_name} at {save_path}\")\n",
        "\n",
        "plot_combined_training_error(\"handwritten\")\n",
        "plot_combined_training_error(\"mfcc_cv\")\n",
        "\n",
        "# ================== (b) Training vs Test Accuracy Scatter ==================\n",
        "def plot_acc_comparison(df, name):\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.scatterplot(data=df, x=\"train_acc\", y=\"test_acc\", hue=\"model\", style=\"layers\", s=100)\n",
        "    plt.title(f\"{name}: Training vs Test Accuracy by Model\")\n",
        "    plt.xlabel(\"Training Accuracy\")\n",
        "    plt.ylabel(\"Test Accuracy\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(PLOT_DIR, f\"{name}_train_vs_test.png\"))\n",
        "    plt.close()\n",
        "\n",
        "plot_acc_comparison(df_hand, \"Handwriting\")\n",
        "plot_acc_comparison(df_mfcc, \"MFCC_CV\")\n",
        "\n",
        "# ================== (c) Confusion Matrix for Best Models ==================\n",
        "def best_model_confusion(df, dataset, results_dir=RESULTS_DIR):\n",
        "    best = df.iloc[df[\"test_acc\"].idxmax()]\n",
        "    model, layers, hidden = best[\"model\"], best[\"layers\"], best[\"hidden\"]\n",
        "    cm_path = os.path.join(results_dir, f\"{dataset}_{model}_layers{layers}_h{hidden}_confusion.png\")\n",
        "    if os.path.exists(cm_path):\n",
        "        img = plt.imread(cm_path)\n",
        "        plt.figure(figsize=(5,5))\n",
        "        plt.imshow(img)\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(f\"Best Confusion  {dataset} ({model}, L{layers}, H{hidden})\")\n",
        "        plt.tight_layout()\n",
        "        save_path = os.path.join(PLOT_DIR, f\"{dataset}_best_confusion_display.png\")\n",
        "        plt.savefig(save_path)\n",
        "        plt.close()\n",
        "        print(f\" Confusion matrix added for {dataset} best model: {save_path}\")\n",
        "    else:\n",
        "        print(\" Confusion matrix image not found for:\", dataset, model, layers, hidden)\n",
        "\n",
        "best_model_confusion(df_hand, \"handwritten\")\n",
        "best_model_confusion(df_mfcc, \"mfcc_cv\")\n",
        "\n",
        "# ================== (d) Normalized Samples from Handwriting Dataset ==================\n",
        "from random import sample\n",
        "from PIL import Image\n",
        "\n",
        "base = \"/content/assignment4_data/Data_Assignment4/Data_Assignment4/Hand_writing_Data\"\n",
        "out_dir = os.path.join(PLOT_DIR, \"handwriting_samples\")\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "def read_txt_as_img(txt_path):\n",
        "    vals = [float(x) for x in open(txt_path).read().split()]\n",
        "    coords = np.array(vals[1:]).reshape(-1,2)\n",
        "    x = (coords[:,0]-coords[:,0].min())/(coords[:,0].max()-coords[:,0].min()+1e-8)\n",
        "    y = (coords[:,1]-coords[:,1].min())/(coords[:,1].max()-coords[:,1].min()+1e-8)\n",
        "    return x, y\n",
        "\n",
        "classes = [d for d in os.listdir(base) if os.path.isdir(os.path.join(base, d))]\n",
        "for cname in classes:\n",
        "    cdir = os.path.join(base, cname)\n",
        "    txt_files = [os.path.join(dp, f) for dp, _, fs in os.walk(cdir) for f in fs if f.endswith(\".txt\")]\n",
        "    if len(txt_files) >= 5:\n",
        "        sel = sample(txt_files, 5)\n",
        "    else:\n",
        "        sel = txt_files\n",
        "    plt.figure(figsize=(10,2))\n",
        "    for i, path in enumerate(sel):\n",
        "        try:\n",
        "            x, y = read_txt_as_img(path)\n",
        "            plt.subplot(1,5,i+1)\n",
        "            plt.plot(x, y, '.-', color='blue')\n",
        "            plt.gca().invert_yaxis()\n",
        "            plt.axis(\"off\")\n",
        "        except Exception:\n",
        "            continue\n",
        "    plt.suptitle(f\"Normalized samples  {cname}\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(out_dir, f\"{cname}_samples.png\"))\n",
        "    plt.close()\n",
        "\n",
        "print(\"\\n All plots and confusion matrices saved in:\", PLOT_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "R7BUjzCnR-eP",
        "outputId": "be363620-9b29-410b-d03c-996006ed8be7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No loss curves found for handwritten\n",
            "No loss curves found for mfcc_cv\n",
            " Confusion matrix added for handwritten best model: /content/results/plots/handwritten_best_confusion_display.png\n",
            " Confusion matrix added for mfcc_cv best model: /content/results/plots/mfcc_cv_best_confusion_display.png\n",
            "\n",
            " All plots and confusion matrices saved in: /content/results/plots\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/results.zip /content/results\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhW5bIRKUIkY",
        "outputId": "17f893bc-336e-40fa-93d3-7463e5dccddd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: content/results/ (stored 0%)\n",
            "updating: content/results/handwritten_GRU_layers2_h64_loss.png (deflated 7%)\n",
            "updating: content/results/.ipynb_checkpoints/ (stored 0%)\n",
            "updating: content/results/mfcc_cv_LSTM_layers2_h64_confusion.png (deflated 17%)\n",
            "updating: content/results/mfcc_cv_LSTM_layers2_h128_loss.png (deflated 8%)\n",
            "updating: content/results/mfcc_cv_RNN_layers2_h64_loss.png (deflated 4%)\n",
            "updating: content/results/handwritten_results_table.csv (deflated 59%)\n",
            "updating: content/results/mfcc_cv_LSTM_layers1_h64_loss.png (deflated 7%)\n",
            "updating: content/results/handwritten_GRU_layers2_h128_confusion.png (deflated 17%)\n",
            "updating: content/results/mfcc_cv_LSTM_layers1_h64_confusion.png (deflated 18%)\n",
            "updating: content/results/handwritten_GRU_layers2_h64_confusion.png (deflated 17%)\n",
            "updating: content/results/mfcc_cv_GRU_layers2_h128_loss.png (deflated 8%)\n",
            "updating: content/results/mfcc_cv_LSTM_layers1_h128_loss.png (deflated 8%)\n",
            "updating: content/results/mfcc_cv_RNN_layers1_h128_confusion.png (deflated 17%)\n",
            "updating: content/results/mfcc_cv_results_table.csv (deflated 59%)\n",
            "updating: content/results/handwritten_samples.png (deflated 4%)\n",
            "updating: content/results/plots/ (stored 0%)\n",
            "updating: content/results/plots/mfcc_cv_best_confusion_display.png (deflated 10%)\n",
            "updating: content/results/plots/handwritten_best_confusion_display.png (deflated 11%)\n",
            "updating: content/results/plots/MFCC_CV_train_vs_test.png (deflated 15%)\n",
            "updating: content/results/plots/MFCC_CV_test_acc_bar.png (deflated 17%)\n",
            "updating: content/results/plots/Handwriting_train_vs_test.png (deflated 15%)\n",
            "updating: content/results/plots/Handwriting_test_acc_bar.png (deflated 19%)\n",
            "updating: content/results/plots/handwriting_samples/ (stored 0%)\n",
            "updating: content/results/plots/handwriting_samples/a_samples.png (deflated 3%)\n",
            "updating: content/results/plots/handwriting_samples/dA_samples.png (deflated 3%)\n",
            "updating: content/results/plots/handwriting_samples/chA_samples.png (deflated 3%)\n",
            "updating: content/results/plots/handwriting_samples/_Sample-Image-for-each-class_samples.png (deflated 21%)\n",
            "updating: content/results/plots/handwriting_samples/ai_samples.png (deflated 3%)\n",
            "updating: content/results/plots/handwriting_samples/bA_samples.png (deflated 3%)\n",
            "updating: content/results/plots/MFCC_CV_train_acc_bar.png (deflated 18%)\n",
            "updating: content/results/plots/Handwriting_train_acc_bar.png (deflated 19%)\n",
            "updating: content/results/mfcc_cv_RNN_layers2_h64_confusion.png (deflated 17%)\n",
            "updating: content/results/handwritten_RNN_layers2_h64_loss.png (deflated 5%)\n",
            "updating: content/results/handwritten_RNN_layers2_h128_loss.png (deflated 6%)\n",
            "updating: content/results/handwritten_LSTM_layers1_h64_loss.png (deflated 5%)\n",
            "updating: content/results/handwritten_LSTM_layers2_h128_loss.png (deflated 4%)\n",
            "updating: content/results/handwritten_RNN_layers1_h128_confusion.png (deflated 17%)\n",
            "updating: content/results/mfcc_cv_GRU_layers1_h128_loss.png (deflated 7%)\n",
            "updating: content/results/handwritten_GRU_layers2_h128_loss.png (deflated 4%)\n",
            "updating: content/results/mfcc_cv_GRU_layers1_h64_loss.png (deflated 7%)\n",
            "updating: content/results/handwritten_LSTM_layers2_h64_confusion.png (deflated 18%)\n",
            "updating: content/results/mfcc_cv_RNN_layers1_h64_loss.png (deflated 4%)\n",
            "updating: content/results/handwritten_GRU_layers1_h128_loss.png (deflated 4%)\n",
            "updating: content/results/handwritten_RNN_layers2_h128_confusion.png (deflated 17%)\n",
            "updating: content/results/mfcc_cv_GRU_layers1_h128_confusion.png (deflated 17%)\n",
            "updating: content/results/handwritten_GRU_layers1_h64_loss.png (deflated 5%)\n",
            "updating: content/results/handwritten_LSTM_layers2_h64_loss.png (deflated 5%)\n",
            "updating: content/results/handwritten_LSTM_layers1_h64_confusion.png (deflated 17%)\n",
            "updating: content/results/mfcc_cv_RNN_layers2_h128_confusion.png (deflated 17%)\n",
            "updating: content/results/handwritten_RNN_layers1_h128_loss.png (deflated 6%)\n",
            "updating: content/results/mfcc_cv_LSTM_layers2_h64_loss.png (deflated 7%)\n",
            "updating: content/results/handwritten_RNN_layers1_h64_loss.png (deflated 6%)\n",
            "updating: content/results/handwritten_LSTM_layers1_h128_loss.png (deflated 5%)\n",
            "updating: content/results/mfcc_cv_RNN_layers1_h64_confusion.png (deflated 17%)\n",
            "updating: content/results/handwritten_LSTM_layers2_h128_confusion.png (deflated 17%)\n",
            "updating: content/results/handwritten_LSTM_layers1_h128_confusion.png (deflated 17%)\n",
            "updating: content/results/handwritten_RNN_layers2_h64_confusion.png (deflated 17%)\n",
            "updating: content/results/handwritten_RNN_layers1_h64_confusion.png (deflated 17%)\n",
            "updating: content/results/handwritten_GRU_layers1_h128_confusion.png (deflated 17%)\n",
            "updating: content/results/mfcc_cv_LSTM_layers2_h128_confusion.png (deflated 18%)\n",
            "updating: content/results/mfcc_cv_GRU_layers2_h128_confusion.png (deflated 17%)\n",
            "updating: content/results/mfcc_cv_RNN_layers1_h128_loss.png (deflated 4%)\n",
            "updating: content/results/mfcc_cv_GRU_layers2_h64_confusion.png (deflated 17%)\n",
            "updating: content/results/handwritten_GRU_layers1_h64_confusion.png (deflated 17%)\n",
            "updating: content/results/mfcc_cv_GRU_layers2_h64_loss.png (deflated 8%)\n",
            "updating: content/results/mfcc_cv_RNN_layers2_h128_loss.png (deflated 4%)\n",
            "updating: content/results/mfcc_cv_GRU_layers1_h64_confusion.png (deflated 17%)\n",
            "updating: content/results/mfcc_cv_LSTM_layers1_h128_confusion.png (deflated 17%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T6Ptaw8PUMr3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}